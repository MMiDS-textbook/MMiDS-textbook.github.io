<!DOCTYPE html>
<html>
<head>
    <title>Self-Assessment Questions for Section 3.5</title>
    <style>
        .question {
            margin-bottom: 20px;
        }
        .options {
            list-style-type: none;
            padding: 0;
        }
        .options li {
            margin-bottom: 10px;
        }
        .feedback {
            margin-top: 10px;
            font-weight: bold;
        }
    </style>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <h1>Self-Assessment Questions for Section 3.5</h1>

    <p>Before moving on, you may wish to quickly test your knowledge by going over the following review questions.</p>

    <div id="quiz-container">
        <div class="question">
            <p>Q3.5.1. In gradient descent, the update rule for moving from \(x_t\) to \(x_{t+1}\) is given by:</p>
            <ul class="options">
                <li><label><input type="radio" name="q0" value="0"> a) \(x_{t+1} = x_t + \alpha_t \nabla f(x_t)\)</label></li>
                <li><label><input type="radio" name="q0" value="1"> b) \(x_{t+1} = x_t - \alpha_t \nabla f(x_t)\)</label></li>
                <li><label><input type="radio" name="q0" value="2"> c) \(x_{t+1} = x_t + \frac{\nabla f(x_t)}{\|\nabla f(x_t)\|}\)</label></li>
                <li><label><input type="radio" name="q0" value="3"> d) \(x_{t+1} = x_t - \frac{\nabla f(x_t)}{\|\nabla f(x_t)\|}\)</label></li>
            </ul>
            <div class="feedback" id="feedback0"></div>
        </div>

        <div class="question">
            <p>Q3.5.2. In the gradient descent update rule \(x_{t+1} = x_t - \alpha_t \nabla f(x_t)\), what does \(\alpha_t\) represent?</p>
            <ul class="options">
                <li><label><input type="radio" name="q1" value="0"> a) The gradient of \(f\) at \(x_t\)</label></li>
                <li><label><input type="radio" name="q1" value="1"> b) The step size or learning rate</label></li>
                <li><label><input type="radio" name="q1" value="2"> c) The direction of steepest ascent</label></li>
                <li><label><input type="radio" name="q1" value="3"> d) The Hessian matrix of \(f\) at \(x_t\)</label></li>
            </ul>
            <div class="feedback" id="feedback1"></div>
        </div>

        <div class="question">
            <p>Q3.5.3. A function \(f : \mathbb{R}^d \to \mathbb{R}\) is said to be \(L\)-smooth if:</p>
            <ul class="options">
                <li><label><input type="radio" name="q2" value="0"> a) \(\|\nabla f(x)\| \leq L\) for all \(x \in \mathbb{R}^d\)</label></li>
                <li><label><input type="radio" name="q2" value="1"> b) \(-LI_{d\times d} \preceq H_f(x) \preceq LI_{d\times d}\) for all \(x \in \mathbb{R}^d\)</label></li>
                <li><label><input type="radio" name="q2" value="2"> c) \(f(y) \leq f(x) + \nabla f(x)^T(y - x) + \frac{L}{2}\|y - x\|^2\) for all \(x, y \in \mathbb{R}^d\)</label></li>
                <li><label><input type="radio" name="q2" value="3"> d) Both b) and c)</label></li>
            </ul>
            <div class="feedback" id="feedback2"></div>
        </div>

        <div class="question">
            <p>Q3.5.4. What is the step size used in the convergence theorem for gradient descent in the smooth case?</p>
            <ul class="options">
                <li><label><input type="radio" name="q3" value="0"> a) \(\alpha_t = \frac{1}{L}\)</label></li>
                <li><label><input type="radio" name="q3" value="1"> b) \(\alpha_t = \frac{1}{t}\)</label></li>
                <li><label><input type="radio" name="q3" value="2"> c) \(\alpha_t = \frac{1}{L^2}\)</label></li>
                <li><label><input type="radio" name="q3" value="3"> d) \(\alpha_t = \frac{1}{\sqrt{t}}\)</label></li>
            </ul>
            <div class="feedback" id="feedback3"></div>
        </div>

        <div class="question">
            <p>Q3.5.5. Suppose \(f : \mathbb{R}^d \to \mathbb{R}\) is \(L\)-smooth and bounded from below. According to the "Convergence of Gradient Descent: Smooth Case" theorem, gradient descent with step size \(\alpha_t = \frac{1}{L}\) started from any \(x_0\) produces a sequence \(\{x_t\}\) such that</p>
            <ul class="options">
                <li><label><input type="radio" name="q4" value="0"> a) \(\lim_{t \to +\infty} f(x_t) = 0\)</label></li>
                <li><label><input type="radio" name="q4" value="1"> b) \(\lim_{t \to +\infty} \|\nabla f(x_t)\| = 0\)</label></li>
                <li><label><input type="radio" name="q4" value="2"> c) \(\min_{t=0,\ldots,S-1} \|\nabla f(x_t)\| \leq \sqrt{\frac{2L[f(x_0) - \bar{f}]}{S}}\) after \(S\) steps</label></li>
                <li><label><input type="radio" name="q4" value="3"> d) Both b) and c)</label></li>
            </ul>
            <div class="feedback" id="feedback4"></div>
        </div>

        <div class="question">
            <p>Q3.5.6. Suppose \(f : \mathbb{R}^d \to \mathbb{R}\) is \(L\)-smooth and \(m\)-strongly convex with a global minimizer at \(x^*\). According to the "Convergence of Gradient Descent: Strongly Convex Case" theorem, gradient descent with step size \(\alpha = \frac{1}{L}\) started from any \(x_0\) produces a sequence \(\{x_t\}\) such that after \(S\) steps:</p>
            <ul class="options">
                <li><label><input type="radio" name="q5" value="0"> a) \(f(x_S) - f(x^*) \leq \left(1 - \frac{m}{L}\right)^S[f(x_0) - f(x^*)]\)</label></li>
                <li><label><input type="radio" name="q5" value="1"> b) \(f(x_S) - f(x^*) \geq \left(1 - \frac{m}{L}\right)^S[f(x_0) - f(x^*)]\)</label></li>
                <li><label><input type="radio" name="q5" value="2"> c) \(f(x_S) - f(x^*) \leq \left(1 + \frac{m}{L}\right)^S[f(x_0) - f(x^*)]\)</label></li>
                <li><label><input type="radio" name="q5" value="3"> d) \(f(x_S) - f(x^*) \geq \left(1 + \frac{m}{L}\right)^S[f(x_0) - f(x^*)]\)</label></li>
            </ul>
            <div class="feedback" id="feedback5"></div>
        </div>

        <div class="question">
            <p>Q3.5.7. If a function \(f\) is \(m\)-strongly convex, what can we say about its global minimizer?</p>
            <ul class="options">
                <li><label><input type="radio" name="q6" value="0"> a) It may not exist.</label></li>
                <li><label><input type="radio" name="q6" value="1"> b) It exists and is unique.</label></li>
                <li><label><input type="radio" name="q6" value="2"> c) It exists but may not be unique.</label></li>
                <li><label><input type="radio" name="q6" value="3"> d) It always occurs at the origin.</label></li>
            </ul>
            <div class="feedback" id="feedback6"></div>
        </div>

        <div class="question">
            <p>Q3.5.8. What is the key property of strongly convex functions that allows us to establish a faster convergence rate for gradient descent compared to the smooth case?</p>
            <ul class="options">
                <li><label><input type="radio" name="q7" value="0"> a) Strong convexity implies smoothness.</label></li>
                <li><label><input type="radio" name="q7" value="1"> b) Strong convexity allows us to relate the function value at a point to the norm of the gradient at that point.</label></li>
                <li><label><input type="radio" name="q7" value="2"> c) Strong convexity guarantees the existence of a unique global minimum.</label></li>
                <li><label><input type="radio" name="q7" value="3"> d) Strong convexity ensures that the function is bounded from below.</label></li>
            </ul>
            <div class="feedback" id="feedback7"></div>
        </div>

        <div class="question">
            <p>Q3.5.9. What mathematical concept does the Descent Guarantee for Smooth Functions primarily rely on?</p>
            <ul class="options">
                <li><label><input type="radio" name="q8" value="0"> a) Taylor's Theorem</label></li>
                <li><label><input type="radio" name="q8" value="1"> b) Cauchy-Schwarz Inequality</label></li>
                <li><label><input type="radio" name="q8" value="2"> c) Mean Value Theorem</label></li>
                <li><label><input type="radio" name="q8" value="3"> d) Fundamental Theorem of Calculus</label></li>
            </ul>
            <div class="feedback" id="feedback8"></div>
        </div>
    </div>

    <script>
        document.querySelectorAll('.options').forEach((optionsList, index) => {
            const correctOptions = [1, 1, 3, 0, 3, 0, 1, 1, 0];

            optionsList.addEventListener('change', (event) => {
                const selectedOption = parseInt(event.target.value);
                const feedbackDiv = document.getElementById('feedback' + index);
                let feedbackText = '';

                if (selectedOption === correctOptions[index]) {
                    feedbackText = "Excellent! " + getCorrectFeedback(index);
                } else {
                    feedbackText = "Try again.";
                }

                feedbackDiv.innerHTML = feedbackText;
                feedbackDiv.style.color = selectedOption === correctOptions[index] ? 'green' : 'red';

                MathJax.typesetPromise([feedbackDiv]); 
            });
        });

        function getCorrectFeedback(index) {
            switch (index) {
                case 0:
                    return "The text defines the gradient descent update rule as \\(x_{t+1} = x_t - \\alpha_t \\nabla f(x_t)\\).";
                    
                case 1:
                    return "The text states 'At each iteration of gradient descent, we take a step in the direction of the negative of the gradient, that is, \\(x_{t+1} = x_t - \\alpha_t \\nabla f(x_t), t = 0, 1, 2, \\ldots\\) for a sequence of step sizes \\(\\alpha_t > 0\\).'";

                case 2:
                    return "The text provides both the definition in terms of the Hessian matrix (option b) and the equivalent characterization in terms of the quadratic bound (option c).";

                case 3:
                    return "The text specifies the step size as \\(\\alpha_t = \\alpha := \\frac{1}{L}\\) in the convergence theorem for the smooth case.";

                case 4:
                    return "The theorem states both the asymptotic convergence of the gradients to zero (option b) and the quantitative bound on the minimum gradient norm after \\(S\\) steps (option c).";

                case 5:
                    return "This is the convergence rate stated in the theorem.";

                case 6:
                    return "The text states that if \\(f\\) is \\(m\\)-strongly convex, then 'the global minimizer is unique.'";

                case 7:
                    return "The text emphasizes that strong convexity allows us to relate \\(f(x)\\) and \\(\\nabla f(x)\\) through the inequality \\(f(x) - f(x^*) \\le \\frac{\\|\\nabla f(x)\\|^2}{2m}\\), which is crucial for the faster convergence rate.";

                case 8:
                    return "The text mentions that the proof of the Descent Guarantee for Smooth Functions involves 'applying Taylor's Theorem, then bounding the second-order term.'";
                    
                default:
                    return "";
            }
        }
    </script>
</body>
</html>
