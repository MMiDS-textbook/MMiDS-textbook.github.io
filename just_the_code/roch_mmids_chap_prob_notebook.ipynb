{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4871de6d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell",
     "colab-keep"
    ]
   },
   "source": [
    "***\n",
    "\n",
    "*Course:* [Math 535](https://people.math.wisc.edu/~roch/mmids/) - Mathematical Methods in Data Science (MMiDS)  \n",
    "*Chapter:* 7-Probabilistic models   \n",
    "*Author:* [Sebastien Roch](https://people.math.wisc.edu/~roch/), Department of Mathematics, University of Wisconsin-Madison  \n",
    "*Updated:* Jan 8, 2024   \n",
    "*Copyright:* &copy; 2024 Sebastien Roch\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d3e08a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# IF RUNNING ON GOOGLE COLAB, UNCOMMENT THE FOLLOWING CODE CELL\n",
    "# When prompted, upload: \n",
    "#     * mmids.py\n",
    "# from your local file system\n",
    "# Files at: https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/tree/main/utils\n",
    "# Alternative instructions: https://colab.research.google.com/notebooks/io.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac5c765",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell",
     "colab-uncomment"
    ]
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f0c00c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# PYTHON 3\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "from numpy.random import default_rng\n",
    "rng = default_rng(535)\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from tensorflow import keras\n",
    "import mmids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca658e5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Motivating example: location tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34c3d47",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Suppose we let loose a cyborg corgi in a large park. We would like to know where it is at all time. For this purpose, it has an implanted location device that sends a signal to a tracking app.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981c686f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Here is an example of the data we might have. The red dots are recorded locations at reguler time intervals. The dotted line helps keep track of the time order of the recordings. (We will explain later how this dataset is generated.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5ee085",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "ss = 4\n",
    "os = 2\n",
    "F = np.array([[1., 0., 1., 0.],[0., 1., 0., 1.],[0., 0., 1., 0.],[0., 0., 0., 1.]]) \n",
    "H = np.array([[1., 0., 0., 0.],[0., 1, 0., 0.]])\n",
    "Q = 0.1 * np.diag(np.ones(ss))\n",
    "R = 10 * np.diag(np.ones(os))\n",
    "x_0 = np.array([0., 0., 1., 1.])\n",
    "T = 50\n",
    "x, y = mmids.lgSamplePath(ss, os, F, H, Q, R, x_0, T)\n",
    "plt.plot(y[0,:], y[1,:], marker='o', c='r', linestyle='dotted')\n",
    "plt.xlim((np.min(y[0,:])-5, np.max(y[0,:])+5)) \n",
    "plt.ylim((np.min(y[1,:])-5, np.max(y[1,:])+5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796ceb13",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "By convention, we start at $(0,0)$. Notice how squiggly the trajectory is. One issue might be that the times  at which the location is recorded are too far between. But, in fact, there is another issue: the tracking device is *inaccurate*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dec043a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To get a better estimate of the true trajectory, it is natural to try to model the noise in the measurement as well as the dynamics itself. Probabilistic models are perfectly suite for this. \n",
    "\n",
    "In this chapter, we will encounter of variety of such models and show how to take advantage of them to estimate unknown states (or parameters). In particular, conditional independence will play a key role.\n",
    "\n",
    "We will come back to location tracking later in the chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71cd60a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\newcommand{\\P}{\\mathbb{P}}$\n",
    "$\\newcommand{\\E}{\\mathbb{E}}$\n",
    "$\\newcommand{\\S}{\\mathcal{S}}$\n",
    "$\\newcommand{\\X}{\\mathcal{X}}$\n",
    "$\\newcommand{\\var}{\\mathrm{Var}}$\n",
    "$\\newcommand{\\btheta}{\\boldsymbol{\\theta}}$\n",
    "$\\newcommand{\\bbeta}{\\boldsymbol{\\beta}}$\n",
    "$\\newcommand{\\bphi}{\\boldsymbol{\\phi}}$\n",
    "$\\newcommand{\\bpi}{\\boldsymbol{\\pi}}$\n",
    "$\\newcommand{\\bmu}{\\boldsymbol{\\mu}}$\n",
    "$\\newcommand{\\bSigma}{\\boldsymbol{\\Sigma}}$\n",
    "$\\newcommand{\\balpha}{\\boldsymbol{\\alpha}}$\n",
    "$\\newcommand{\\indep}{\\perp\\!\\!\\!\\perp}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae235919",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Background: introduction to parametric families, generalized linear models and maximum likelihood estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8610b31",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER** In Numpy, the module [numpy.random](https://numpy.org/doc/stable/reference/random/index.html) provides a way to sample from a variety of standard distributions. We first initialize the [pseudorandom number generator](https://en.wikipedia.org/wiki/Pseudorandom_number_generator) with a [random seed](https://en.wikipedia.org/wiki/Random_seed). In particular it allows the results to be reproducible: using the same seed produces the same results again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50ebc71",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "seed = 535\n",
    "rng = np.random.default_rng(535)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab50de5d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We then set the distribution and its parameters. Here's are lists of available [probability distributions](https://numpy.org/doc/stable/reference/random/generator.html#distributions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eead61ee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "p = 0.1 # probability of success\n",
    "N = 5 # number of samples\n",
    "rng.binomial(1,p,size=N) # Bernoulli is special case of binomial with 1 trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59b4aef",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Here are a few other examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aad3444",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "p = [0.1, 0.2, 0.7]\n",
    "n = 100\n",
    "rng.multinomial(n,p,size=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa4b537",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "mu = np.array([0.1, -0.3])\n",
    "sig = np.array([[2., 0.],[0., 3.]])\n",
    "rng.multivariate_normal(mu,sig,size=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07964a3b",
   "metadata": {
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc757567",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\newcommand{\\P}{\\mathbb{P}}$\n",
    "$\\newcommand{\\E}{\\mathbb{E}}$\n",
    "$\\newcommand{\\S}{\\mathcal{S}}$\n",
    "$\\newcommand{\\var}{\\mathrm{Var}}$\n",
    "$\\newcommand{\\btheta}{\\boldsymbol{\\theta}}$\n",
    "$\\newcommand{\\bphi}{\\boldsymbol{\\phi}}$\n",
    "$\\newcommand{\\bpi}{\\boldsymbol{\\pi}}$\n",
    "$\\newcommand{\\bmu}{\\boldsymbol{\\mu}}$\n",
    "$\\newcommand{\\blambda}{\\boldsymbol{\\lambda}}$\n",
    "$\\newcommand{\\bSigma}{\\boldsymbol{\\Sigma}}$\n",
    "$\\newcommand{\\balpha}{\\boldsymbol{\\alpha}}$\n",
    "$\\newcommand{\\indep}{\\perp\\!\\!\\!\\perp}$   \n",
    "$\\newcommand{\\bp}{\\mathbf{p}}$\n",
    "$\\newcommand{\\bx}{\\mathbf{x}}$\n",
    "$\\newcommand{\\bX}{\\mathbf{X}}$\n",
    "$\\newcommand{\\by}{\\mathbf{y}}$\n",
    "$\\newcommand{\\bY}{\\mathbf{Y}}$\n",
    "$\\newcommand{\\bz}{\\mathbf{z}}$\n",
    "$\\newcommand{\\bZ}{\\mathbf{Z}}$\n",
    "$\\newcommand{\\bw}{\\mathbf{w}}$\n",
    "$\\newcommand{\\bW}{\\mathbf{W}}$\n",
    "$\\newcommand{\\pa}{\\mathrm{pa}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeab671a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Modeling correlated random variables: from simple to complex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4198577",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER** In Numpy, the module [numpy.random](https://numpy.org/doc/stable/reference/random/index.html) also provides a way to sample from mixture models by using [numpy.random.Generator.choice](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.choice.html). This is better seen on an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a62982",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Python 3\n",
    "import numpy as np\n",
    "from numpy.random import default_rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3c80f2",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "seed = 535 # setting the seed\n",
    "rng = np.random.default_rng(seed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763610dc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# constructs a mixture of three normal distributions,\n",
    "# with prior probabilities [0.2, 0.5, 0.3]\n",
    "mus = np.array([-2.0, 0.0, 3.0])\n",
    "sigmas = np.array([1.2, 1.0, 2.5])\n",
    "coeffs = np.array([0.2, 0.5, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8fd672",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "N = 5 # number of samples\n",
    "which = rng.choice(3,N,p=coeffs)\n",
    "rng.normal(mus[which],sigmas[which])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5658ab80",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58da396c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER** We implement the Naive Bayes model. We use [Laplace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing) to avoid overfitting issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4b0938",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We use a simple example from [Towards Data Science](https://towardsdatascience.com/all-about-naive-bayes-8e13cef044cf):\n",
    "\n",
    "> **Example:** let’s say we have data on 1000 pieces of fruit. The fruit being a Banana, Orange or some other fruit and imagine we know 3 features of each fruit, whether it’s long or not, sweet or not and yellow or not, as displayed in the table below.\n",
    "\n",
    "![Table](https://miro.medium.com/max/1066/1*B_uXqox7nHfwTa1HH4Fc_A.png)\n",
    "\n",
    "> [...] Which should provide enough evidence to predict the class of another fruit as it’s introduced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94459ecb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We encode the data into a table, where the rows are the classes and the columns are the features. The entries are the corresponding $N_{km}$'s. In addition we provide the vector $N_k$, which is the last column above, and the value $N$, which is the sum of the entries of $N_k$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebf5fd4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "N_km = np.array([[400., 350., 450.],[0., 150., 300.],[100., 150., 50.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb73bad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def nb_fit_table(N_km, alpha=1., beta=1.):\n",
    "    \n",
    "    K, M = N_km.shape\n",
    "    N_k = np.sum(N_km,axis=-1)\n",
    "    N = np.sum(N_k)\n",
    "    \n",
    "    # MLE for pi_k's\n",
    "    pi_k = (N_k+alpha) / (N+K*alpha)\n",
    "    \n",
    "    # MLE for p_km's\n",
    "    p_km = (N_km+beta) / (N_k[:,None]+2*beta)\n",
    "\n",
    "    return pi_k, p_km"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8b8d03",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We run it on our simple dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea894c00",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pi_k, p_km = nb_fit_table(N_km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c05072",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(pi_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77c7874",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(p_km)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd92a54",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Continuing on with our previous example:\n",
    "\n",
    "> So let’s say we’re given the features of a piece of fruit and we need to predict the class. If we’re told that the additional fruit is Long, Sweet and Yellow, we can classify it using the [prediction] formula and subbing in the values for each outcome, whether it’s a Banana, an Orange or Other Fruit. The one with the highest probability (score) being the winner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6db391",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The next function computes the negative logarithm of $\\pi_k \\prod_{m=1}^M p_{km}^{x_m} (1-p_{km})^{1-x_m}$, that is, the score of $k$, and outputs a $k$ achieving the minimum score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dbe9dc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def nb_predict(pi_k, p_km, x, label_set):\n",
    "   \n",
    "    K = len(pi_k)\n",
    "    \n",
    "    # Computing the score for each k\n",
    "    score_k = np.zeros(K)\n",
    "    for k in range(K):\n",
    "       \n",
    "        score_k[k] += - np.log(pi_k[k])\n",
    "        score_k[k] += - np.sum(x * np.log(p_km[k,:]) + (1 - x)*np.log(1 - p_km[k,:]))\n",
    "    \n",
    "    # Computing the minimum\n",
    "    argmin = np.argmin(score_k, axis=0)\n",
    "    minscr = np.max(score_k, axis=0)\n",
    "\n",
    "    return label_set[argmin]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0493af",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We run it on our dataset with the additional fruit from the quote above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faad0a6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "label_set = ['Banana', 'Orange', 'Other']\n",
    "x = np.array([1., 1., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d66c476",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nb_predict(pi_k, p_km, x, label_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acac1cd1",
   "metadata": {
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b010f684",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER** We implement the EM algorithm for mixtures of multivariate Bernoullis. For this purpose, we adapt our previous Naive Bayes routines. We also allow for the possibility of using Laplace smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f9d617",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def responsibility(pi_k, p_km, x):\n",
    "   \n",
    "    K = len(pi_k)\n",
    "        \n",
    "    # Computing the score for each k\n",
    "    score_k = np.zeros(K)\n",
    "    for k in range(K):\n",
    "       \n",
    "        score_k[k] += - np.log(pi_k[k])\n",
    "        score_k[k] += - np.sum(x*np.log(p_km[k,:]) + (1 - x)*np.log(1 - p_km[k,:]))\n",
    "    \n",
    "    # Computing responsibilities for each k\n",
    "    r_k = np.exp(-score_k)/(np.sum(np.exp(-score_k)))\n",
    "        \n",
    "    return r_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdf6946",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def update_parameters(eta_km, eta_k, eta, alpha, beta):\n",
    "        \n",
    "    K, M = N_km.shape\n",
    "    \n",
    "    # MLE for pi_k's\n",
    "    pi_k = (eta_k+alpha) / (eta+K*alpha)\n",
    "    \n",
    "    # MLE for p_km's\n",
    "    p_km = (eta_km+beta) / (eta_k[:,None]+2*beta)\n",
    "\n",
    "    return pi_k, p_km"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c9984d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We implement the E and M Step next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34864c5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def em_bern(X, K, pi_0, p_0, maxiters = 10, alpha=0., beta=0.):\n",
    "    \n",
    "    n, M = X.shape\n",
    "    pi_k = pi_0\n",
    "    p_km = p_0\n",
    "    \n",
    "    for _ in range(maxiters):\n",
    "    \n",
    "        # E Step\n",
    "        r_ki = np.zeros((K,n))\n",
    "        for i in range(n):\n",
    "            r_ki[:,i] = responsibility(pi_k, p_km, X[i,:])\n",
    "        \n",
    "        # M Step     \n",
    "        eta_km = np.zeros((K,M))\n",
    "        eta_k = np.sum(r_ki, axis=-1)\n",
    "        eta = np.sum(eta_k)\n",
    "        for k in range(K):\n",
    "            for m in range(M):\n",
    "                eta_km[k,m] = np.sum(X[:,m] * r_ki[k,:]) \n",
    "        pi_k, p_km = update_parameters(eta_km, eta_k, eta, alpha, beta)\n",
    "        \n",
    "    return pi_k, p_km   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d191eab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We test the algorithm on a very simple dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22d17f4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([[1., 1., 1.],[1., 1., 1.],[1., 1., 1.],[1., 0., 1.],[0., 1., 1.],[0., 0., 0.],[0., 0., 0.],[0., 0., 1.]])\n",
    "n, M = X.shape\n",
    "K = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a956f1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "seed = 535\n",
    "rng = np.random.default_rng(seed)\n",
    "pi_0 = np.ones(K)/K\n",
    "p_0 = rng.random((K,M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4b654d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pi_k, p_km = em_bern(X, K, pi_0, p_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95d1024",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(pi_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d2dd76",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(p_km)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cdffa4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We compute the probability that the vector $(0, 0, 1)$ is in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7090d3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x_test = np.array([0., 0., 1.])\n",
    "responsibility(pi_k, p_km, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d81cac9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To give a more involved example, we return to the MNIST dataset. There are two common ways to write a $2$. Let's see if a mixture of multivariate Bernoullis can find them. We load the dataset and extract the images labelled $2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516f44f8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mnist = keras.datasets.mnist\n",
    "(imgs, labels), (test_imgs, test_labels) = mnist.load_data()\n",
    "len(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687ddf25",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "i2 = [i for i in range(len(labels)) if labels[i]==2]\n",
    "imgs2 = imgs[i2]\n",
    "labels2 = labels[i2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0f19bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Next, we transform the images into vectors and convert into black and white by rounding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382ffc9b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = np.vstack([np.round(imgs2[i].flatten()/255) for i in range(len(labels2))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db6ef21",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We can convert back as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69df21be",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(X[0,:].reshape((28,28)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707be38d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "In this example, the probabilities involved are very small and the responsibilities are close to $0$ or $1$. We use a variant, called hard EM, which replaces responsibilities with the one-hot encoding of the largest responsibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72f9932",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def hard_responsibility(pi_k, p_km, x):\n",
    "   \n",
    "    K = len(pi_k)\n",
    "        \n",
    "    # Computing the score for each k\n",
    "    score_k = np.zeros(K)\n",
    "    for k in range(K):\n",
    "       \n",
    "        score_k[k] += - np.log(pi_k[k])\n",
    "        score_k[k] += - np.sum(x*np.log(p_km[k,:]) + (1 - x)*np.log(1 - p_km[k,:]))\n",
    "    \n",
    "    # Computing responsibilities for each k\n",
    "    argmin = np.argmin(score_k, axis=0)\n",
    "    r_k = np.zeros(K)\n",
    "    r_k[argmin] = 1\n",
    "\n",
    "    return r_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c286ee8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def hard_em_bern(X, K, pi_0, p_0, maxiters = 10, alpha=0., beta=0.):\n",
    "    \n",
    "    n, M = X.shape\n",
    "    pi_k = pi_0\n",
    "    p_km = p_0\n",
    "    \n",
    "    for _ in range(maxiters):\n",
    "    \n",
    "        # E Step\n",
    "        r_ki = np.zeros((K,n))\n",
    "        for i in range(n):\n",
    "            r_ki[:,i] = hard_responsibility(pi_k, p_km, X[i,:])\n",
    "        \n",
    "        # M Step     \n",
    "        eta_km = np.zeros((K,M))\n",
    "        eta_k = np.sum(r_ki, axis=-1)\n",
    "        eta = np.sum(eta_k)\n",
    "        for k in range(K):\n",
    "            for m in range(M):\n",
    "                eta_km[k,m] = np.sum(X[:,m] * r_ki[k,:]) \n",
    "        pi_k, p_km = update_parameters(eta_km, eta_k, eta, alpha, beta)\n",
    "        \n",
    "    return pi_k, p_km   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a8e8b8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We run the algorithm with $2$ clusters. You may have to run it a few times to get a meaningful clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924bf37f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n, M = X.shape\n",
    "K = 2\n",
    "rng = default_rng(12345)\n",
    "pi_0 = np.ones(K)/K\n",
    "p_0 = rng.random((K,M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659d3287",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pi_k, p_km = hard_em_bern(X, K, pi_0, p_0, maxiters=10, alpha=1., beta=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0dd306",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(pi_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eeed448",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We vizualize the two uncovered clusters by rendering their means as an image. Here is one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01150e5c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(p_km[0,:].reshape((28,28)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9b0747",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Here is the other one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2c1966",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(p_km[1,:].reshape((28,28)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eb07c4",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5edb226",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\newcommand{\\P}{\\mathbb{P}}$\n",
    "$\\newcommand{\\E}{\\mathbb{E}}$\n",
    "$\\newcommand{\\S}{\\mathcal{S}}$\n",
    "$\\newcommand{\\var}{\\mathrm{Var}}$\n",
    "$\\newcommand{\\bmu}{\\boldsymbol{\\mu}}$\n",
    "$\\newcommand{\\bSigma}{\\boldsymbol{\\Sigma}}$\n",
    "$\\newcommand{\\btheta}{\\boldsymbol{\\theta}}$\n",
    "$\\newcommand{\\bpi}{\\boldsymbol{\\pi}}$\n",
    "$\\newcommand{\\indep}{\\perp\\!\\!\\!\\perp}$\n",
    "$\\newcommand{\\bp}{\\mathbf{p}}$\n",
    "$\\newcommand{\\bx}{\\mathbf{x}}$\n",
    "$\\newcommand{\\bX}{\\mathbf{X}}$\n",
    "$\\newcommand{\\by}{\\mathbf{y}}$\n",
    "$\\newcommand{\\bY}{\\mathbf{Y}}$\n",
    "$\\newcommand{\\bz}{\\mathbf{z}}$\n",
    "$\\newcommand{\\bZ}{\\mathbf{Z}}$\n",
    "$\\newcommand{\\bw}{\\mathbf{w}}$\n",
    "$\\newcommand{\\bW}{\\mathbf{W}}$\n",
    "$\\newcommand{\\bv}{\\mathbf{v}}$\n",
    "$\\newcommand{\\bV}{\\mathbf{V}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7fd5d4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Linear-Gaussian models and Kalman filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fa2b82",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**Implementing the Kalman filter** We implement the Kalman filter as described above with known covariance matrices. We take $\\Delta = 1$ for simplicity. The code is adapted from [[Mur](https://github.com/probml)]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bac791",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We will test Kalman filtering on a simulated path drawn from the linear-Gaussian model above. The following function creates such a path and its noisy observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989b68ce",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "seed = 535\n",
    "rng = np.random.default_rng(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd77f80",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def lgSamplePath(ss, os, F, H, Q, R, x_0, T):\n",
    "    x = np.zeros((ss,T)) \n",
    "    y = np.zeros((os,T))\n",
    "    x[:,0] = x_0\n",
    "    ey = np.zeros(os)\n",
    "    ey = rng.multivariate_normal(np.zeros(os),R) \n",
    "    y[:,0] = H @ x[:,0] + ey\n",
    "    \n",
    "    for t in range(1,T):\n",
    "        ex = np.zeros(ss)\n",
    "        ex = rng.multivariate_normal(np.zeros(ss),Q) # noise on x_t\n",
    "        x[:,t] = F @ x[:,t-1] + ex\n",
    "        ey = np.zeros(os)\n",
    "        ey = rng.multivariate_normal(np.zeros(os),R) # noise on y_t\n",
    "        y[:,t] = H @ x[:,t] + ey\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab33f7da",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Here is an example. Here $\\bSigma$ is denoted as $V$. In the plot, the blue crosses are the unobserved true path and the orange dots are the noisy observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347fc260",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ss = 4 # state size\n",
    "os = 2 # observation size\n",
    "F = np.array([[1., 0., 1., 0.],[0., 1., 0., 1.],[0., 0., 1., 0.],[0., 0., 0., 1.]]) \n",
    "H = np.array([[1., 0., 0., 0.],[0., 1, 0., 0.]])\n",
    "Q = 0.1 * np.diag(np.ones(ss))\n",
    "R = 10 * np.diag(np.ones(os))\n",
    "x_0 = np.array([0., 0., 1., 1.]) # initial state\n",
    "T = 50\n",
    "x, y = lgSamplePath(ss, os, F, H, Q, R, x_0, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81118d45",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.plot(y[0,:], y[1,:], marker='o', c='r', linestyle='dotted')\n",
    "plt.xlim((np.min(y[0,:])-5, np.max(y[0,:])+5)) \n",
    "plt.ylim((np.min(y[1,:])-5, np.max(y[1,:])+5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5030e1ce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.plot(x[0,:], x[1,:], marker='x', c='g', linestyle='dashed', alpha=0.5)\n",
    "plt.xlim((np.min(x[0,:])-5, np.max(x[0,:])+5)) \n",
    "plt.ylim((np.min(x[1,:])-5, np.max(x[1,:])+5))\n",
    "plt.scatter(y[0,:], y[1,:], c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7372bbad",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The following function implements the Kalman filter. Here $A$ is $F$ and $C$ is $H$. The full recursion is broken up into several steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96405e9e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def kalmanUpdate(ss, A, C, Q, R, y_t, mu_prev, Sig_prev):\n",
    "    mu_pred = A @ mu_prev\n",
    "    Sig_pred = A @ Sig_prev @ A.T + Q\n",
    "    e_t = y_t - C @ mu_pred # error at time t\n",
    "    S = C @ Sig_pred @ C.T + R\n",
    "    Sinv = LA.inv(S)\n",
    "    K = Sig_pred @ C.T @ Sinv # Kalman gain matrix\n",
    "    mu_new = mu_pred + K @ e_t\n",
    "    Sig_new = (np.diag(np.ones(ss)) - K @ C) @ Sig_pred\n",
    "    return mu_new, Sig_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b78bc1a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def kalmanFilter(ss, os, y, A, C, Q, R, init_mu, init_Sig, T):\n",
    "    mu = np.zeros((ss, T))\n",
    "    Sig = np.zeros((ss, ss, T))\n",
    "    mu[:,0] = init_mu\n",
    "    Sig[:,:,0] = init_Sig\n",
    "\n",
    "    for t in range(1,T):\n",
    "        mu[:,t], Sig[:,:,t] = kalmanUpdate(ss, A, C, Q, R, y[:,t], mu[:,t-1], Sig[:,:,t-1])\n",
    "\n",
    "    return mu, Sig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a1cac8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We apply this to the example above. The inferred unobserved states are in green."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574d584f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "init_mu = x_0\n",
    "init_Sig = 1 * np.diag(np.ones(ss))\n",
    "mu, Sig = kalmanFilter(ss, os, y, F, H, Q, R, init_mu, init_Sig, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6528ef",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.plot(x[0,:], x[1,:], marker='x', c='g', linestyle='dashed', alpha=0.5)\n",
    "plt.xlim((np.min(x[0,:])-5, np.max(x[0,:])+5)) \n",
    "plt.ylim((np.min(x[1,:])-5, np.max(x[1,:])+5))\n",
    "plt.scatter(y[0,:], y[1,:], c='r')\n",
    "plt.plot(mu[0,:], mu[1,:], marker='s', linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776c1264",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To quantify the improvement in the inference compared to the observations, we compute the mean squared error in both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd906cd5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dobs = x[0:1,:] - y[0:1,:]\n",
    "mse_obs = np.sqrt(np.sum(dobs**2))\n",
    "print(mse_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb7d356",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dfilt = x[0:1,:] - mu[0:1,:]\n",
    "mse_filt = np.sqrt(np.sum(dfilt**2))\n",
    "print(mse_filt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f43e25",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We indeed observe a reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c8315b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**Missing data** We can also allow for the possibility that some observations are missing. Imagine for instance losing GPS signal while going through a tunnel. The recursions above are still valid, with the only modification that the $\\bY_t$ and $H$ terms are dropped at those times $t$ where there is no observation. In Numpy, we can use [`NaN`](https://numpy.org/doc/stable/reference/constants.html#numpy.nan). (Alternatively, one can use the [numpy.ma](https://numpy.org/doc/stable/reference/maskedarray.generic.html) module.) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7136d2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We use a same sample path as above, but mask observations at times $t=10,\\ldots,20$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5968d8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ss = 4\n",
    "os = 2\n",
    "F = np.array([[1., 0., 1., 0.],[0., 1., 0., 1.],[0., 0., 1., 0.],[0., 0., 0., 1.]]) \n",
    "H = np.array([[1., 0., 0., 0.],[0., 1, 0., 0.]])\n",
    "Q = 0.01 * np.diag(np.ones(ss))\n",
    "R = 10 * np.diag(np.ones(os))\n",
    "x_0 = np.array([0., 0., 1., 1.])\n",
    "T = 30\n",
    "x, y = lgSamplePath(ss, os, F, H, Q, R, x_0, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f07bd80",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(10,20):\n",
    "    y[0,i] = np.nan\n",
    "    y[1,i] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5345af",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Here is the sample we are aiming to infer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e5c305",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.plot(x[0,:], x[1,:], marker='x', c='g', linestyle='dashed', alpha=0.5)\n",
    "plt.xlim((np.min(x[0,:])-5, np.max(x[0,:])+5)) \n",
    "plt.ylim((np.min(x[1,:])-5, np.max(x[1,:])+5))\n",
    "plt.scatter(y[0,:], y[1,:], c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75d2457",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We modify the recursion accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef7da9d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def kalmanUpdate(ss, A, C, Q, R, y_t, mu_prev, Sig_prev):\n",
    "    mu_pred = A @ mu_prev\n",
    "    Sig_pred = A @ Sig_prev @ A.T + Q\n",
    "    if np.isnan(y_t[0]) or np.isnan(y_t[1]):\n",
    "        return mu_pred, Sig_pred\n",
    "    else:\n",
    "        e_t = y_t - C @ mu_pred # error at time t\n",
    "        S = C @ Sig_pred @ C.T + R\n",
    "        Sinv = LA.inv(S)\n",
    "        K = Sig_pred @ C.T @ Sinv # Kalman gain matrix\n",
    "        mu_new = mu_pred + K @ e_t\n",
    "        Sig_new = (np.diag(np.ones(ss)) - K @ C) @ Sig_pred\n",
    "        return mu_new, Sig_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4df63ef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "init_mu = x_0\n",
    "init_Sig = 1 * np.diag(np.ones(ss))\n",
    "mu, Sig = kalmanFilter(ss, os, y, F, H, Q, R, init_mu, init_Sig, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcf636f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.plot(x[0,:], x[1,:], marker='x', c='g', alpha=0.5)\n",
    "plt.xlim((np.min(x[0,:])-5, np.max(x[0,:])+5)) \n",
    "plt.ylim((np.min(x[1,:])-5, np.max(x[1,:])+5))\n",
    "plt.scatter(y[0,:], y[1,:], c='r', alpha=0.5)\n",
    "plt.plot(mu[0,:], mu[1,:], marker='s', linewidth=2)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
