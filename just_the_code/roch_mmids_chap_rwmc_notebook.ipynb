{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2304c18c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell",
     "colab-keep"
    ]
   },
   "source": [
    "***\n",
    "\n",
    "*Course:* [Math 535](https://people.math.wisc.edu/~roch/mmids/) - Mathematical Methods in Data Science (MMiDS)  \n",
    "*Chapter:* 7-Random walks on graphs and Markov chains  \n",
    "*Author:* [Sebastien Roch](https://people.math.wisc.edu/~roch/), Department of Mathematics, University of Wisconsin-Madison  \n",
    "*Updated:* July 16, 2024   \n",
    "*Copyright:* &copy; 2024 Sebastien Roch\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba369ea7",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# You will need the files:\n",
    "#     * mmids.py\n",
    "#     * mathworld-adjacency.csv\n",
    "#     * mathworld-titles.csv\n",
    "# from https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/tree/main/utils\n",
    "#\n",
    "# IF RUNNING ON GOOGLE COLAB (RECOMMENDED):\n",
    "# \"Upload to session storage\" from the Files tab on the left\n",
    "# Alternative instructions: https://colab.research.google.com/notebooks/io.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714b9305",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch\n",
    "import mmids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989192c4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\newcommand{\\bmu}{\\boldsymbol{\\mu}}$ \n",
    "$\\newcommand{\\bSigma}{\\boldsymbol{\\Sigma}}$\n",
    "$\\newcommand{\\bfbeta}{\\boldsymbol{\\beta}}$ \n",
    "$\\newcommand{\\bflambda}{\\boldsymbol{\\lambda}}$\n",
    "$\\newcommand{\\bgamma}{\\boldsymbol{\\gamma}}$ \n",
    "$\\newcommand{\\bsigma}{{\\boldsymbol{\\sigma}}}$\n",
    "$\\newcommand{\\bpi}{\\boldsymbol{\\pi}}$\n",
    "$\\newcommand{\\btheta}{{\\boldsymbol{\\theta}}}$ \n",
    "$\\newcommand{\\bphi}{\\boldsymbol{\\phi}}$ \n",
    "$\\newcommand{\\balpha}{\\boldsymbol{\\alpha}}$\n",
    "$\\newcommand{\\blambda}{\\boldsymbol{\\lambda}}$\n",
    "$\\renewcommand{\\P}{\\mathbb{P}}$\n",
    "$\\newcommand{\\E}{\\mathbb{E}}$\n",
    "$\\newcommand{\\indep}{\\perp\\!\\!\\!\\perp} \\newcommand{\\bx}{\\mathbf{x}}$ \n",
    "$\\newcommand{\\bp}{\\mathbf{p}}$\n",
    "$\\renewcommand{\\bx}{\\mathbf{x}}$\n",
    "$\\newcommand{\\bX}{\\mathbf{X}}$ \n",
    "$\\newcommand{\\by}{\\mathbf{y}}$ \n",
    "$\\newcommand{\\bY}{\\mathbf{Y}}$ \n",
    "$\\newcommand{\\bz}{\\mathbf{z}}$\n",
    "$\\newcommand{\\bZ}{\\mathbf{Z}}$ \n",
    "$\\newcommand{\\bw}{\\mathbf{w}}$ \n",
    "$\\newcommand{\\bW}{\\mathbf{W}}$\n",
    "$\\newcommand{\\bv}{\\mathbf{v}}$ \n",
    "$\\newcommand{\\bV}{\\mathbf{V}}$\n",
    "$\\newcommand{\\bfg}{\\mathbf{g}}$ \n",
    "$\\newcommand{\\bfh}{\\mathbf{h}}$ \n",
    "$\\newcommand{\\horz}{\\rule[.5ex]{2.5ex}{0.5pt}}$\n",
    "$\\renewcommand{\\S}{\\mathcal{S}}$ \n",
    "$\\newcommand{\\X}{\\mathcal{X}}$ \n",
    "$\\newcommand{\\var}{\\mathrm{Var}}$ \n",
    "$\\newcommand{\\pa}{\\mathrm{pa}}$\n",
    "$\\newcommand{\\Z}{\\mathcal{Z}}$ \n",
    "$\\newcommand{\\bh}{\\mathbf{h}}$ \n",
    "$\\newcommand{\\bb}{\\mathbf{b}}$ \n",
    "$\\newcommand{\\bc}{\\mathbf{c}}$ \n",
    "$\\newcommand{\\cE}{\\mathcal{E}}$ \n",
    "$\\newcommand{\\cP}{\\mathcal{P}}$\n",
    "$\\newcommand{\\bbeta}{\\boldsymbol{\\beta}}$\n",
    "$\\newcommand{\\bLambda}{\\boldsymbol{\\Lambda}}$\n",
    "$\\newcommand{\\cov}{\\mathrm{Cov}}$\n",
    "$\\newcommand{\\bfk}{\\mathbf{k}}$\n",
    "$\\newcommand{\\idx}[1]{}$\n",
    "$\\newcommand{\\xdi}{}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f71384e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Motivating example: discovering mathematical topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a416e7d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "A common task in network analysis is to identify \"central\" vertices in a graph. Centrality is a vague concept. It can be defined in many different ways depending on the context and the type of network. Quoting from [Wikipedia](https://en.wikipedia.org/wiki/Centrality):\n",
    "\n",
    "> In graph theory and network analysis, indicators of centrality assign numbers or rankings to nodes within a graph corresponding to their network position. Applications include identifying the most influential person(s) in a social network, key infrastructure nodes in the Internet or urban networks, super-spreaders of disease, and brain networks. [...] Centrality indices are answers to the question \"What characterizes an important vertex?\" The answer is given in terms of a real-valued function on the vertices of a graph, where the values produced are expected to provide a ranking which identifies the most important nodes. The word \"importance\" has a wide number of meanings, leading to many different definitions of centrality. \n",
    "\n",
    "In an undirected graph, a natural approach is to look at the degree of a vertex as a measure of its importance (also referred to as degree centrality). But it is hardly the only one. One could for instance look at the average distance to all other nodes (its reciprocal is the [closeness centrality](https://en.wikipedia.org/wiki/Closeness_centrality)) or at the number of shortest paths between pairs of vertices going through the vertex (known as [betweenness centrality](https://en.wikipedia.org/wiki/Betweenness_centrality)). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ed212a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "What if the graph is directed? Things are somewhat more complicated there. For instance, there is now the in-degree as well as the out-degree. \n",
    "\n",
    "Let us look at a particular example of practical importance, the World Wide Web (from now on, the Web). In this case, the vertices are webpages and a directed edge from $u$ to $v$ indicates a hyperlink from page $u$ to page $v$. The Web is much too large to analyze here. Instead, we will consider a tiny (but still interesting!) subset of it, the pages of [Wolfram's MathWorld](https://mathworld.wolfram.com), a wonderful mathematics resource. \n",
    "\n",
    "Each page of MathWorld concerns a particular mathematical concept, e.g., [scale-free network](https://mathworld.wolfram.com/Scale-FreeNetwork.html). A definition and notable properties are described. Importantly for us, in a section entitled \"SEE ALSO\", other related mathematical concepts are listed with a link to their MathWorld page. In the case of scale-free networks, the [small world network](https://mathworld.wolfram.com/SmallWorldNetwork.html) topic is referenced, among others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fbc614",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The resulting directed graph is available through the [NetSet](https://netset.telecom-paris.fr/index.html) datasets and can be downloaded [here](https://netset.telecom-paris.fr/pages/mathworld.html). We load it now. For convenience, we have reformatted it into the files `mathworld-adjacency.csv` and `mathworld-titles.csv`, which are available on the [GitHub of the book](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/tree/main/utils/datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec467474",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data_edges = pd.read_csv('mathworld-adjacency.csv')\n",
    "data_edges.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1179836a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "It consists in a list of directed edges. For example, the first one is an edge from vertex `0` to vertex `2`. The second one is from `1` to `47` and so on. \n",
    "\n",
    "There is a total of $49069$ edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9e2e8c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data_edges.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f8ce6e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The second file contains the titles of the pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843d1182",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data_titles = pd.read_csv('mathworld-titles.csv')\n",
    "data_titles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d944de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "So the first edge above is from `Alexander's Horned Sphere` to `Antoine's Horned Sphere`. That is, the [latter](https://mathworld.wolfram.com/AntoinesHornedSphere.html) is listed in the \"SEE ALSO\" section of the [former](https://mathworld.wolfram.com/AlexandersHornedSphere.html). \n",
    "\n",
    "There are $12362$ topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc0b469",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n = data_titles.shape[0]\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5f40ff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We construct the graph by adding the edges one by one. We first convert `df_edges` into a Numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d1fde0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "edgelist = data_edges[['from','to']].to_numpy()\n",
    "print(edgelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906a1ea7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "G = nx.empty_graph(n, create_using=nx.DiGraph)\n",
    "for i in range(edgelist.shape[0]):\n",
    "    G.add_edge(edgelist[i,0], edgelist[i,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e7ec4b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Returning to question of centrality, we can now try to measure the importance of different nodes. For instance, the in-degree of `Alexander's Horned Sphere` is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9860522",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "G.in_degree(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe1d0b3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "while that of `Antoine's Horned Sphere` is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844de2f3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "G.in_degree(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0ed7f8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "suggesting that the former is more central than the latter, at least in the sense that it is referenced more often.\n",
    "\n",
    "But is that the right measure? Consider the following: `Antoine's Horned Sphere` receives only one reference, but it is from a seemingly relatively important vertex, `Alexander's Horned Sphere`. How can one take this into account in quantifying its importance in the network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad829b39",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We will come back to this question later in this chapter. To hint at things to come, it will turn out that \"exploring the graph at random\" provides a powerful perspective on centrality.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fdfeba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Background: elements of finite Markov chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983111c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**EXAMPLE:** **(Random Walk on the Petersen Graph)** Let $G = (V,E)$ be the Petersen graph. Each vertex $i$ has degree $3$, that is, it has three neighbors which we denote $v_{i,1}, v_{i,2}, v_{i,3}$ in some arbitrary order. For instance, denoting the vertices by $1,\\ldots, 10$ as above, vertex $9$ has neighbors $v_{9,1} = 4, v_{9,2} = 6, v_{9,3} = 7$.\n",
    "\n",
    "We consider the following random walk on $G$. We start at $X_0 = 1$. Then, for each $t\\geq 0$, we let $X_{t+1}$ be a uniformly chosen neighbor of $X_t$, independently of the previous history. That is, we jump at random from neighbor to neighbor. Formally, fix $X_0 = 1$ and let $(Z_t)_{t \\geq 0}$ be an i.i.d. sequence of random variables taking values in $\\{1,2,3\\}$ satisfying\n",
    "\n",
    "$$\n",
    "\\mathbb{P}[Z_t = 1] = \\mathbb{P}[Z_t = 2] = \\mathbb{P}[Z_t = 3] = 1/3.\n",
    "$$\n",
    "\n",
    "Then define, for all $t \\geq 0$,\n",
    "$\n",
    "X_{t+1}\n",
    "= f(X_t, Z_t)\n",
    "= v_{i,Z_t}\n",
    "$\n",
    "if $X_t = v_i$.\n",
    "\n",
    "By an argument similar to the previous example, $(X_t)_{t \\geq 0}$ is a Markov chain.\n",
    "Also as in the previous example, one can pick $X_0$ according to an initial distribution, independently from the sequence $(Z_t)_{t \\geq 0}$. $\\lhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5789598",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**EXAMPLE:** **(Random Walk on the Petersen Graph, continued)** Consider again the random walk on the Petersen graph $G = (V,E)$. We number the vertices $1, 2,\\ldots, 10$. To compute the transition matrix, we list for each vertex its neighbors and put the value $1/3$ in the corresponding columns. For instance, vertex $1$ has neighbors $2$, $5$ and $6$, so row $1$ has $1/3$ in columns $2$, $5$, and $6$. And so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dc0504",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We get:\n",
    "\n",
    "$$\n",
    "P = \\begin{pmatrix}\n",
    "0 & 1/3 & 0 & 0 & 1/3 & 1/3 & 0 & 0 & 0 & 0\\\\\n",
    "1/3 & 0 & 1/3 & 0 & 0 & 0 & 1/3 & 0 & 0 & 0\\\\\n",
    "0 & 1/3 & 0 & 1/3 & 0 & 0 & 0 & 1/3 & 0 & 0\\\\\n",
    "0 & 0 & 1/3 & 0 & 1/3 & 0 & 0 & 0 & 1/3 & 0\\\\\n",
    "1/3 & 0 & 0 & 1/3 & 0 & 0 & 0 & 0 & 0 & 1/3\\\\\n",
    "1/3 & 0 & 0 & 0 & 0 & 0 & 0 & 1/3 & 1/3 & 0\\\\\n",
    "0 & 1/3 & 0 & 0 & 0 & 0 & 0 & 0 & 1/3 & 1/3\\\\\n",
    "0 & 0 & 1/3 & 0 & 0 & 1/3 & 0 & 0 & 0 & 1/3\\\\\n",
    "0 & 0 & 0 & 1/3 & 0 & 1/3 & 1/3 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 1/3 & 0 & 1/3 & 1/3 & 0 & 0\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f97e41",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We have already encountered a matrix that encodes the neighbors of each vertex, the adjacency matrix. Here we can recover the transition matrix by multiplying the adjacency matrix by $1/3$. $\\lhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa41d3c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** Returning to our *Robot Vacuum Example*, the transition graph of the chain can be obtained by thinking of $P$ as the weighted adjacency matrix of the transition graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5a6095",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "P_robot = np.array([[0, 0.8, 0, 0.2, 0, 0, 0, 0, 0],\n",
    "                    [0.3, 0, 0.2, 0, 0, 0.5, 0, 0, 0],\n",
    "                    [0, 0.6, 0, 0, 0, 0.4, 0, 0, 0],\n",
    "                    [0.1, 0.1, 0, 0, 0.8, 0, 0, 0, 0],\n",
    "                    [0, 0, 0, 0.25, 0, 0, 0.75, 0, 0],\n",
    "                    [0, 0.15, 0.15, 0, 0, 0, 0, 0.35, 0.35],\n",
    "                    [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "                    [0, 0, 0, 0, 0.3, 0.4, 0.2, 0, 0.1],\n",
    "                    [0, 0, 0, 0, 0, 1, 0, 0, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1a6966",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We define a graph from its adjancency matrix. See [`networkx.from_numpy_array()`](https://networkx.org/documentation/stable/reference/generated/networkx.convert_matrix.from_numpy_array.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47fac1a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "G_robot = nx.from_numpy_array(P_robot, create_using=nx.DiGraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05eb3a82",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Drawing edge weights on a directed graph in a readable fashion is not straighforward. We will not do this here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70bef51",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "n_robot = P_robot.shape[0]\n",
    "nx.draw_networkx(G_robot, pos=nx.circular_layout(G_robot), \n",
    "                 labels={i: i+1 for i in range(n_robot)}, \n",
    "                 node_color='black', font_color='white', \n",
    "                 connectionstyle='arc3, rad = 0.2')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48217a2",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050cd955",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Once we have specified a transition matrix (and an initial distribution), we can simulate the corresponding Markov chain. This is useful to compute (approximately) probabilities of complex events through the law of large numbers. Here is some code to generate one sample path up to some given time $T$. We assume that the state space is $[n]$. We use [`rng.choice`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.choice.html) to generate each transition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb84acb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def SamplePath(rng, mu, P, T):\n",
    "    \n",
    "    n = mu.shape[0]\n",
    "    X = np.zeros(T+1)\n",
    "    for i in range(T+1):\n",
    "        if i == 0:\n",
    "            X[i] = rng.choice(a=np.arange(start=1,stop=n+1),p=mu)\n",
    "        else:\n",
    "            X[i] = rng.choice(a=np.arange(start=1,stop=n+1),p=P[int(X[i-1]-1),:])\n",
    "            \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e909ce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** Let's try with our *Robot Vacuum*. We take the initial distribution to be the uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48574c4c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "seed = 535\n",
    "rng = np.random.default_rng(seed)\n",
    "mu = np.ones(n_robot) / n_robot\n",
    "print(SamplePath(rng, mu, P_robot, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8d0e65",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "For example, we can use a simulation to approximate the expected number of times that room $9$ is visited up to time $10$. To do this, we run the simulation a large number of times (say $1000$) and count the average number of visits to $9$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cfda86",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "z = 9\n",
    "N_samples = 1000\n",
    "visits_to_z = np.zeros(N_samples)\n",
    "\n",
    "for i in range(N_samples):\n",
    "    visits_to_z[i] = np.count_nonzero(SamplePath(rng, mu, P_robot, 10) == z)\n",
    "\n",
    "print(np.mean(visits_to_z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc30bf6a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e5d547",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Limit behavior 1: stationary distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1329adce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**EXAMPLE:** **(Robot Vacuum, continued)** Going back to the *Robot Vacuum Example*, recall the transition graph. While there is no direct edge from $4$ to $3$, we do have $4 \\to 3$ through the path $(4,2), (2,3)$. Do we have $3 \\to 4$? $\\lhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b16ed7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** Consider random walk on the following digraph, which we refer to as the *Two Sinks Example* (why do you think?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3071e495",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "G_sinks = nx.DiGraph()\n",
    "n_sinks = 5\n",
    "\n",
    "for i in range(n_sinks):\n",
    "    G_sinks.add_node(i)\n",
    "\n",
    "G_sinks.add_edge(0, 0, weight=1/3)\n",
    "G_sinks.add_edge(0, 1, weight=1/3)\n",
    "G_sinks.add_edge(1, 1, weight=1/3)\n",
    "G_sinks.add_edge(1, 2, weight=1/3)\n",
    "G_sinks.add_edge(2, 2, weight=1)\n",
    "G_sinks.add_edge(3, 3, weight=1)\n",
    "G_sinks.add_edge(0, 4, weight=1/3)\n",
    "G_sinks.add_edge(1, 4, weight=1/3)\n",
    "G_sinks.add_edge(4, 3, weight=1)\n",
    "\n",
    "nx.draw_networkx(G_sinks, pos=nx.circular_layout(G_sinks), \n",
    "                 labels={i: i+1 for i in range(n_sinks)}, \n",
    "                 node_color='black', font_color='white', \n",
    "                 connectionstyle='arc3, rad = -0.2')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c870c6c7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Here we have $1 \\to 4$ (Why?). The *Communication Lemma* implies that, when started at $1$, $(X_t)_{t \\geq 0}$ visits $4$ with positive probability. But that probability is not one. Indeed we also have $1 \\to 3$ (Why?), so there is a positive probability of visiting $3$ as well. But if we do so before visiting $4$, we stay at $3$ forever hence cannot subsequently reach $4$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6b13fc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "In fact, intuitively, if we run this chain long enough we will either get stuck at $3$ or get stuck at $4$. These give rise to different stationary distributions. The transition probability is the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86202ddb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "P_sinks = nx.adjacency_matrix(G_sinks).toarray()\n",
    "print(P_sinks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eda194",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "It is easy to check that $\\bpi = (0,0,1,0,0)$ and $\\bpi' = (0,0,0,1,0)$ are both stationary distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5473051",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pi = np.array([0.,0.,1.,0.,0.])\n",
    "pi_prime = np.array([0.,0.,0.,1.,0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609e4f33",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "P_sinks.T @ pi.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107a0b61",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "P_sinks.T @ pi_prime.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81616f45",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "In fact, there are infinitely many stationary distributions in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77764262",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08089f5b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** Because irreducibility is ultimately a graph-theoretic property, it is easy to check using `NetworkX`. For this, we use the function [`is_strongly_connected()`](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.components.is_strongly_connected.html). Revisiting the *Robot Vacuum Example*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bbad6d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "P_robot = np.array([[0, 0.8, 0, 0.2, 0, 0, 0, 0, 0],\n",
    "                    [0.3, 0, 0.2, 0, 0, 0.5, 0, 0, 0],\n",
    "                    [0, 0.6, 0, 0, 0, 0.4, 0, 0, 0],\n",
    "                    [0.1, 0.1, 0, 0, 0.8, 0, 0, 0, 0],\n",
    "                    [0, 0, 0, 0.25, 0, 0, 0.75, 0, 0],\n",
    "                    [0, 0.15, 0.15, 0, 0, 0, 0, 0.35, 0.35],\n",
    "                    [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "                    [0, 0, 0, 0, 0.3, 0.4, 0.2, 0, 0.1],\n",
    "                    [0, 0, 0, 0, 0, 1, 0, 0, 0]])\n",
    "\n",
    "G_robot = nx.from_numpy_array(P_robot, create_using=nx.DiGraph)\n",
    "\n",
    "print(nx.is_strongly_connected(G_robot))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03f0029",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Consider again the *Two Sinks Example*. It turns out not to be irreducible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afce132a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(nx.is_strongly_connected(G_sinks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad12266",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b03f57",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** In general, computing stationary distributions is not as straigthforward as in the simple example we considered above. We conclude this subsection with some numerical recipes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31942515",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Going back to the *Robot Vacuum*, finding a solution to $\\bpi P =\\bpi$ in this case is not obvious. One way to do this is to note that, taking transposes, this condition is equivalent to $P^T \\bpi^T = \\bpi^T$. That is, $\\bpi^T$ is an eigenvector of $P^T$ with eigenvalue $1$. (Or, as we noted previously, the row vector $\\bpi$ is a left eigenvector of $P$ with eigenvalue $1$.) It must also satisfy $\\bpi \\geq 0$ with at least one entry non-zero. Here, we use NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3996ad",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "w, v = LA.eig(P_robot.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1583eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The first eigenvalue is approximately $1$, as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6fc5c4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e5d75b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The corresponding eigenvector is approximately non-negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f13c11",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(v[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e2b216",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To obtain a stationary distribution, we remove the imaginary part and normalize it to sum to $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f177cb8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pi_robot = np.real(v[:,0]) / np.sum(np.real(v[:,0]))\n",
    "print(pi_robot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5314306",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Alternatively, we can solve the linear system\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n \\pi_i p_{i,j} = \\pi_j, \\qquad \\forall j \\in [n].\n",
    "$$\n",
    "\n",
    "It turns out that the last equation is a linear combination over the other equations (see *Exercise 3.48*), so we remove it and replace it instead with the condition $\\sum_{i=1}^n \\pi_i = 1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51aeccd5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The left-hand side of the resulting linear system is (after taking the transpose to work with column vectors):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdecd4a2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "n_robot = P_robot.shape[0]\n",
    "A = np.copy(P_robot.T) - np.diag(np.ones(n_robot))\n",
    "A[n_robot-1,:] = np.ones(n_robot)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613b9f61",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The right-hand side of the resulting linear system is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f1fed1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "b = np.concatenate((np.zeros(n_robot-1),[1.]))\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c068c01",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We solve the linear system using [`numpy.linalg.solve()`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777ba8f8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pi_robot_solve = LA.solve(A,b)\n",
    "print(pi_robot_solve)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c190f071",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "This last approach is known as \"Replace an Equation\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2047ac98",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb575260",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Limit behavior 2: convergence to equilibrium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f813c8e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** The *Convergence to Equilibrium Theorem* implies that we can use power iteration to compute the unique stationary diistribution in the irreducible case. We revisit the *Robot Vaccum Example*. We initialize with the uniform distribution, then repeatedly multiply by $P$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76373f8f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "P_robot = np.array([[0, 0.8, 0, 0.2, 0, 0, 0, 0, 0],\n",
    "                    [0.3, 0, 0.2, 0, 0, 0.5, 0, 0, 0],\n",
    "                    [0, 0.6, 0, 0, 0, 0.4, 0, 0, 0],\n",
    "                    [0.1, 0.1, 0, 0, 0.8, 0, 0, 0, 0],\n",
    "                    [0, 0, 0, 0.25, 0, 0, 0.75, 0, 0],\n",
    "                    [0, 0.15, 0.15, 0, 0, 0, 0, 0.35, 0.35],\n",
    "                    [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "                    [0, 0, 0, 0, 0.3, 0.4, 0.2, 0, 0.1],\n",
    "                    [0, 0, 0, 0, 0, 1, 0, 0, 0]])\n",
    "n_robot = P_robot.shape[0]\n",
    "mu = np.ones(n_robot)/n_robot\n",
    "print(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db66bcc2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "mu = mu @ P_robot\n",
    "print(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31939d1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mu = mu @ P_robot\n",
    "print(mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ccde7a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We repeat, say, $10$ more times and compare to the truth `pi_robot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9580a17",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    mu = mu @ P_robot\n",
    "print(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7162c5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "w, v = LA.eig(P_robot.T)\n",
    "pi_robot = np.real(v[:,0]) / np.sum(np.real(v[:,0]))\n",
    "print(pi_robot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899470d4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We see that a small number of iterations sufficed to get an accurate answer. In general, the speed of convergence depends on the eigenvalues of $P$ that are strictly smaller than $1$ in absolute value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233169a2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We can also check the *Ergodic Theorem* through simulation. We generate a long sample path and compare the state visit frequencies to `pi_robot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c1d665",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "seed = 535\n",
    "rng = np.random.default_rng(seed)\n",
    "\n",
    "mu = np.ones(n_robot) / n_robot\n",
    "path_length = 50000\n",
    "visit_freq = np.zeros(n_robot)\n",
    "\n",
    "path = mmids.SamplePath(rng, mu, P_robot, path_length)\n",
    "for i in range(n_robot):\n",
    "    visit_freq[i] = np.count_nonzero(path == i+1)/(path_length+1)\n",
    "\n",
    "print(visit_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610f98fa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(pi_robot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617299ac",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3f8f00",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Application: random walks on graphs and PageRank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8192318c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Here is an implementation of the PageRank algorithm. We will need a function that takes as input an adjacency matrix $A$ and returns the corresponding transition matrix $P$. Some vertices have no outgoing links. To avoid dividing by $0$, we add a self-loop to *all vertices with out-degree $0$*. We [`numpy.fill_diagonal`](https://numpy.org/doc/stable/reference/generated/numpy.fill_diagonal.html) for this purpose.\n",
    "\n",
    "Also, because the adjacency matrix and the vector of out-degrees have different shapes, we turn `out_deg` into a column vector using [`numpy.newaxis`](https://numpy.org/doc/stable/reference/constants.html#numpy.newaxis) to ensure that the division is [done one column at a time](https://numpy.org/doc/stable/user/basics.broadcasting.html#broadcastable-arrays). (There are many ways of doing this, [but some are slower than others](https://stackoverflow.com/questions/18522216/multiplying-across-in-a-numpy-array).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca77fd2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def transition_from_adjacency(A):\n",
    "    \n",
    "    n = A.shape[0]\n",
    "    sinks = (A @ np.ones(n)) == 0.\n",
    "    P = A.copy()\n",
    "    np.fill_diagonal(P, sinks)\n",
    "    out_deg = P @ np.ones(n)\n",
    "    P = P / out_deg[:, np.newaxis]\n",
    "    \n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374139ad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The following function adds the damping factor. Here `mu` will be the uniform distribution. It gets added (after scaling by `1-alpha`) one row at a time to `P` (again after scaling by `alpha`). This time we do not need to reshape `mu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125c8acd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def add_damping(P, alpha, mu):\n",
    "    Q = alpha * P + (1-alpha) * mu\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47271881",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "When computing PageRank, we take the transpose of $Q$ to turn multiplication from the left into multiplication from the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceeff94",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def pagerank(A, alpha=0.85, max_iter=100):\n",
    "    \n",
    "    n = A.shape[0]\n",
    "    mu = np.ones(n)/n\n",
    "    P = transition_from_adjacency(A)\n",
    "    Q = add_damping(P, alpha, mu)\n",
    "    v = mu\n",
    "    for _ in range(max_iter):\n",
    "        v = Q.T @ v\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefb89d3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** Let's try a star with edges pointing out. Along the way, we check that our functions work how we expect them to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8962ef17",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "n = 8\n",
    "G_outstar = nx.DiGraph()\n",
    "for i in range(1,n):\n",
    "    G_outstar.add_edge(0,i)\n",
    "\n",
    "nx.draw_networkx(G_outstar, labels={i: i+1 for i in range(n)}, \n",
    "                 node_color='black', font_color='white')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac78a4b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "A_outstar = nx.adjacency_matrix(G_outstar).toarray()\n",
    "print(A_outstar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a8a585",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We compute the matrices $P$ and $Q$. We use [`numpy.set_printoptions`](https://numpy.org/doc/stable/reference/generated/numpy.set_printoptions.html) to condense the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423ade12",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "P_outstar = transition_from_adjacency(A_outstar)\n",
    "np.set_printoptions(formatter={'float': '{: 0.3f}'.format})\n",
    "print(P_outstar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0dc865",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "alpha = 0.85\n",
    "mu = np.ones(n)/n\n",
    "Q_outstar = add_damping(P_outstar, alpha, mu)\n",
    "print(Q_outstar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bba6902",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "While it is tempting to guess that $1$ is the most central node of the network, no edge actually points to it. In this case, the center of the star has a low PageRank value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3e8a19",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(pagerank(A_outstar))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf93f6e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We then try a star with edges pointing in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb61890",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "n = 8\n",
    "G_instar = nx.DiGraph()\n",
    "G_instar.add_node(0)\n",
    "for i in range(1,n):\n",
    "    G_instar.add_edge(i,0)\n",
    "    \n",
    "nx.draw_networkx(G_instar, labels={i: i+1 for i in range(n)}, \n",
    "                 node_color='black', font_color='white')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6766cd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "A_instar = nx.adjacency_matrix(G_instar).toarray()\n",
    "print(A_instar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1a980c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "P_instar = transition_from_adjacency(A_instar)\n",
    "print(P_instar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbd9023",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Q_instar = add_damping(P_instar, alpha, mu)\n",
    "print(Q_instar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d08240c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "In this case, the center of the star does indeed have a high PageRank value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4332acd3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(pagerank(A_instar))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121fc565",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a442a3f0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We revisit the star example in the undirected case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250cb6b4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "n = 8\n",
    "G_star = nx.Graph()\n",
    "for i in range(1,n):\n",
    "    G_star.add_edge(0,i)\n",
    "    \n",
    "nx.draw_networkx(G_star, labels={i: i+1 for i in range(n)}, \n",
    "                 node_color='black', font_color='white')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13224ba5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We first compute the PageRank vector without damping. Here the random walk is periodic (Why?) so power iteration may fail (Try it!). Instead, we use a small amount of damping and increase the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c15041",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A_star = nx.adjacency_matrix(G_star).toarray()\n",
    "print(A_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d6eddb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(pagerank(A_star, max_iter=10000, alpha=0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6ba684",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The PageRank value for the center node is indeed roughly $7$ times larger than the other ones, as can be expected from the ratio of their degrees. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14feb686",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We try again with more damping. This time the ratio of PageRank values is not quite the same as the ratio of degrees, but the center node continues to have a higher value than the other nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d723787f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(pagerank(A_star))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2ddeb0",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae23593",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We load the dataset again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c39ffe",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data_edges = pd.read_csv('mathworld-adjacency.csv')\n",
    "data_edges.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7b54e8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The second file contains the titles of the pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca1cf51",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data_titles = pd.read_csv('mathworld-titles.csv')\n",
    "data_titles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ee988a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We construct the graph by adding the edges one by one. We first convert `df_edges` into a Numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adaaf99",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "edgelist = data_edges[['from','to']].to_numpy()\n",
    "print(edgelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec47562f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "n = 12362\n",
    "G_mw = nx.empty_graph(n, create_using=nx.DiGraph)\n",
    "for i in range(edgelist.shape[0]):\n",
    "    G_mw.add_edge(edgelist[i,0], edgelist[i,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a050f1a2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To apply PageRank, we construct the adjacency matric of the graph. We also define a vector of title pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a36ff12",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A_mw = nx.adjacency_matrix(G_mw).toarray()\n",
    "titles_mw = data_titles['title'].to_numpy()\n",
    "pr_mw = pagerank(A_mw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d543caf8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We use [`numpy.argsort`](https://numpy.org/doc/stable/reference/generated/numpy.argsort.html) to identify the pages with highest scores. We apply it to `-pr_mw` to sort from the highest to lowest value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb910c8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "top_pages = np.argsort(-pr_mw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86db5430",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The top 25 topics are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b20e2f9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(titles_mw[top_pages[:25]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46bb055",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We indeed get a list of central concepts in mathematics -- including several we have encountered previously such as `Normal Distribution`, `Tree`, `Vector` or `Derivative`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9020f0",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d7d7c1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "There is a variant of PageRank, referred to as Personalized PageRank (PPR)$\\idx{Personalized PageRank}\\xdi$, which aims to tailor the outcome to specific interests. This is accomplished from a simple change to the algorithm. When teleporting, rather than jumping to a uniformly random page, we instead jump to an arbitrary distribution which is meant to capture some specific interests. In the context of the web for instance, this distribution might be uniform over someone's bookmarks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196bb9e4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We adapt `pagerank` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77263004",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def ppr(A, mu, alpha=0.85, max_iter=100):\n",
    "    n = A.shape[0]\n",
    "    P = transition_from_adjacency(A)\n",
    "    Q = add_damping(P, alpha, mu)\n",
    "    v = mu\n",
    "    for _ in range(max_iter):\n",
    "        v = Q.T @ v\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ab05c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** To test PPR, consider the distribution concentrated on a single topic `Normal Distribution`. This is topic number `1270`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1f0486",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(np.argwhere(titles_mw == 'Normal Distribution')[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4df204",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mu = np.zeros(n)\n",
    "mu[1270] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213a1f35",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We now run PPR and list the top 25 pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cb5c44",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ppr_mw = ppr(A_mw, mu)\n",
    "top_pers_pages = np.argsort(-ppr_mw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a836b8f3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The top 25 topics are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f091c620",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(titles_mw[top_pers_pages[:25]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e669f57",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "This indeed returns various statistical concepts, particularly related to the normal dsitribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2befa1eb",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e317cbdf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Further applications: Gibbs sampling and generating images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d01f6a1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**Sampling from simple distributions** When $\\bpi$ is a standard distribution or $\\S$ is relatively small, this can be done efficiently by using a random number generator, as we have done previously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c9a1df",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** Recall how this works. We first initialize the random number generator and use a `seed` for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0b98fe",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "seed = 535\n",
    "rng = np.random.default_rng(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d32fb3b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To generate, say $1000$, samples from a multivariate normal, say with mean $(0, 0)$ and covariance $\\begin{pmatrix}5 & 0\\\\0 & 1\\end{pmatrix}$, we use [`numpy.random.Generator.multivariate_normal`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.multivariate_normal.html#numpy.random.Generator.multivariate_normal) as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbce01e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mean = np.array([0., 0.])\n",
    "cov = np.array([[5., 0.], [0., 1.]])\n",
    "x, y = rng.multivariate_normal(mean, cov, 1000).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f807da66",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Computing the mean of each component we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032e3856",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(np.mean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aa6ec8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(np.mean(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7e95bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "This is somewhat close to the expected answer: $(0,0)$. \n",
    "\n",
    "Using a larger number of samples, say $10,000$, gives a better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80562fc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x, y = rng.multivariate_normal(mean, cov, 10000).T\n",
    "print(np.mean(x))\n",
    "print(np.mean(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a513c6c7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Sampling from an arbitrary distribution on a finite set is also straightforward -- as long as the set is not too big. This can be done using [`numpy.random.Generator.choice`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.choice.html). Borrowing the example from the documentation, the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05dfbec",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "aa_milne_arr = ['pooh', 'rabbit', 'piglet', 'christopher']\n",
    "print(rng.choice(aa_milne_arr, 5, p=[0.5, 0.1, 0.1, 0.3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22718db2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "generates $5$ samples from the set $\\S = \\{\\tt{pooh}, \\tt{rabbit}, \\tt{piglet}, \\tt{christopher}\\}$ with respective probabilities $0.5, 0.1, 0.1, 0.3$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354a235d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "But this may not be practical when the state space $\\S$ is very large. As an example, later in this section, we will learn a \"realistic\" distribution of handwritten digits. We will do so using the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9e8c60",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "mnist = datasets.MNIST(root='./data', train=True, \n",
    "                       download=True, transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(mnist, batch_size=len(mnist), shuffle=False)\n",
    "\n",
    "imgs, labels = next(iter(train_loader))\n",
    "imgs = imgs.squeeze().numpy()\n",
    "labels = labels.numpy()\n",
    "imgs = np.round(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebe2a6c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Each image is $28 \\times 28$, so the total number of (black and white) pixels is $784$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa19d533",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nx_pixels, ny_pixels = imgs[0].shape\n",
    "nx_pixels, ny_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae39cdb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n_pixels = nx_pixels * ny_pixels\n",
    "n_pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ab3cd5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To specify the a distribution over all possible black and white images of this size, we need in principle to assign a probability to a very large number of states. Our space here is $\\S = \\{0,1\\}^{784}$, imagining that $0$ encodes white and $1$ encodes black and that we have ordered the pixels in some arbitrary way. How big is this space?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d46a57c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Answer: $2^{784}$.\n",
    "\n",
    "Or in base $10$, we compute $\\log_{10}(2^{784})$, which is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4d02ea",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "784 * np.log(2) / np.log(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0150c705",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "So a little more than $10^{236}$. \n",
    "\n",
    "This is much too large to naively plug into `rng.choice`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e41394",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fea4e5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** Suppose $\\S = \\{1,\\cdots, n\\} = [n]$ for some positive integer $n$ and $\\bpi$ is proportional to a Poisson distribution with mean $\\lambda > 0$. That is, \n",
    "\n",
    "$$\n",
    "\\pi_i = C e^{-\\lambda} \\frac{\\lambda^i}{i!}, \\quad \\forall i \\in \\S\n",
    "$$\n",
    "\n",
    "for some constant $C$ chosen so that $\\sum_{i=1}^{n} \\pi_i = 1$. Recall that we do not need to determine $C$ as it is enough to know the target distribution up to a scaling factor by the previous remark. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902b18e2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To apply Metropolis-Hastings, we need a proposal chain. Consider the following choice. For each $1 < i < n$, move to $i+1$ or $i-1$ with probability $1/2$ each. For $i=1$ (respectively $i = n$), move to $2$ (respectively $n-1$) with probability $1/2$, otherwise stay where you are. For instance, if $n = 4$, then \n",
    "\n",
    "$$\n",
    "Q\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "1/2 & 1/2 & 0 & 0\\\\\n",
    "1/2 & 0 & 1/2 & 0\\\\\n",
    "0 & 1/2 & 0 & 1/2\\\\\n",
    "0 & 0 & 1/2 & 1/2\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "\n",
    "which is indeed a stochastic matrix. It is also symmetric, so it does not enter into the acceptance probability by the previous remark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f22b87",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To compute the acceptance probability, we only need to consider pairs of adjacent integers as they are the only one that have non-zero probability under $Q$. Consider state $1 < i < n$. Observe that\n",
    "\n",
    "$$\n",
    "\\frac{\\pi_{i+1}}{\\pi_{i}}\n",
    "= \\frac{C e^{-\\lambda} \\lambda^{i+1}/(i+1)!}{C e^{-\\lambda} \\lambda^{i}/i!}\n",
    "= \\frac{\\lambda}{i+1}\n",
    "$$\n",
    "\n",
    "so a move to $i+1$ happens with probability\n",
    "\n",
    "$$\n",
    "\\frac{1}{2} \\min\\left\\{1, \\frac{\\lambda}{i+1}\\right\\},\n",
    "$$\n",
    "\n",
    "where the $1/2$ factor from the proposal distribution.\n",
    "Similarly, it can be checked (try it!) that a move to $i-1$ occurs with probability\n",
    "\n",
    "$$\n",
    "\\frac{1}{2} \\min\\left\\{1, \\frac{i}{\\lambda}\\right\\}.\n",
    "$$\n",
    "\n",
    "And we stay at $i$ with probability $1 - \\frac{1}{2} \\min\\left\\{1, \\frac{\\lambda}{i+1}\\right\\} - \\frac{1}{2} \\min\\left\\{1, \\frac{i}{\\lambda}\\right\\}$. (Why is this guaranteed to be a probability?) \n",
    "\n",
    "A similar formula applies to $i = 1, n$. (Try it!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917539d3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We are ready to apply Metropolis-Hastings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3654b1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def mh_transition_poisson(lmbd, n):\n",
    "    P = np.zeros((n,n))\n",
    "    for idx in range(n):\n",
    "        i = idx + 1 # index starts at 0 rather than 1\n",
    "        if (i > 1 and i < n):\n",
    "            P[idx, idx+1] = (1/2) * np.min(np.array([1, lmbd/(i+1)]))\n",
    "            P[idx, idx-1] = (1/2) * np.min(np.array([1, i/lmbd]))\n",
    "            P[idx, idx] = 1 - P[idx, idx+1] - P[idx, idx-1]\n",
    "        elif i == 1:\n",
    "            P[idx, idx+1] = (1/2) * np.min(np.array([1, lmbd/(i+1)]))\n",
    "            P[idx, idx] = 1 - P[idx, idx+1]\n",
    "        elif i == n:\n",
    "            P[idx, idx-1] = (1/2) * np.min(np.array([1, i/lmbd]))\n",
    "            P[idx, idx] = 1 - P[idx, idx-1]\n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ea6ddc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Take $\\lambda = 1$ and $n = 6$. We get the following transition matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2239a09f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lmbd = 1\n",
    "n = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b43056a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "P = mh_transition_poisson(lmbd, n)\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30bd5a9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**TRY IT!** Rewrite the function `mh_transition_poisson` without an explicit loop by using [broadcasting and vectorization](https://numpy.org/doc/stable/user/basics.broadcasting.html).  ([Open in Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c5b40f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We use our simulator from a previous chapter. We start from the uniform distribution and take $100$ steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ba2af6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "seed = 535\n",
    "rng = np.random.default_rng(seed)\n",
    "\n",
    "mu = np.ones(n) / n\n",
    "T = 100\n",
    "X = mmids.SamplePath(rng, mu, P, T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f939205",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Our sample is the final state of the trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf01b75",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X[T]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c64d0d9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We repeat $1000$ times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba924964",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "N_samples = 1000 # number of repetitions\n",
    "\n",
    "freq_z = np.zeros(n) # init of frequencies sampled\n",
    "for i in range(N_samples):\n",
    "    X = mmids.SamplePath(rng, mu, P, T)\n",
    "    freq_z[int(X[T])-1] += 1 # adjust for index starting at 0\n",
    "    \n",
    "freq_z = freq_z/N_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ecce7a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We plot the frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e9c0ab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.bar(range(1,n+1),freq_z, color='lightblue', edgecolor='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6a493b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "If we increase the parameter $\\lambda$ (which is not quite the mean; why?), what would you expect will happen to the sampled distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d367e9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**TRY IT!** Redo the simulations, but this time implement a general Metropolis-Hastings algorithm rather than specifying the transition matrix directly. That is, implement the algorithm for an arbitrary $\\bpi$ and $Q$. Assume the state space is $[n]$. ([Open in Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25d96d8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9434f8b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "*Gibbs sampling:* We sample from the joint distribution $\\pi$ and observe only $\\bv$.\n",
    "\n",
    "We need to compute the conditional probabilities given every other variable. The sigmoid function, which we have encountered previously, $\\sigma(x)$, will once again make an appearance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda98450",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x): \n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0248948",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We implement the Gibbs sampler for an RBM. Rather than updating the units at random, we use a block approach. Specifically, we update all hidden units independently, given the visible units; then we update all visible units independently, given the hidden units. In each case, this is warranted by the conditional independence structure revealed above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc490f5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We first implement the conditional means using the formulas previously derived. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15397e28",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def rbm_mean_hidden(v, W, c):\n",
    "    return sigmoid(W @ v + c)\n",
    "\n",
    "def rbm_mean_visible(h, W, b):\n",
    "    return sigmoid(W.T @ h + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81a44a1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We next implement one step of the sampler, which consists in updating all hidden units, followed by updating all visible units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf7408a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def rbm_gibbs_update(rng, v, W, b, c):\n",
    "    p_hidden = rbm_mean_hidden(v, W, c)\n",
    "    h = rng.binomial(1, p_hidden, p_hidden.shape)\n",
    "    p_visible = rbm_mean_visible(h, W, b)\n",
    "    v = rng.binomial(1, p_visible, p_visible.shape)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76e7a71",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Finally, we repeat these steps `k` times. We only return the visible units `v`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c5251b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def rbm_gibbs_sampling(rng, k, v_0, W, b, c):\n",
    "    counter = 0\n",
    "    v = v_0\n",
    "    while counter < k:\n",
    "        v = rbm_gibbs_update(rng, v, W, b, c)\n",
    "        counter += 1\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9602911",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Here `v_0` is the initial visible unit states. We do not need to initialize the hidden ones as this is done automatically in the first update step. In the next subsection, we will take the initial distribution of $\\bv$ to be independent Bernoullis with success probability $1/2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3520cbb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We apply our Gibbs sampler to generating images. As mentioned previously, we use the MNIST dataset to learn a \"realistic\" distribution of handwritten digit images. Here the images are encoded by the visible units of an RBM. Then we sample from this model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3834e3c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We first need to train the model on the data. We will not show how this is done here, but instead use [`sklearn.neural_network.BernoulliRBM`](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.BernoulliRBM.html). (Some details of how this training is done is provided [here](https://scikit-learn.org/stable/modules/neural_networks_unsupervised.html#stochastic-maximum-likelihood-learning).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87023a7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import BernoulliRBM\n",
    "\n",
    "rbm = BernoulliRBM(random_state=seed, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6f01f5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To simplify the analysis and speed up the training, we only keep digits $0$, $1$ and $5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4b8786",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mask = (labels == 0) | (labels == 1) | (labels == 5)\n",
    "imgs = imgs[mask]\n",
    "labels = labels[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37d6449",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We flatten the images (which have already been \"rounded\" to black-and-white; see the first subsection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf4a024",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = imgs.reshape(len(imgs), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cd8a95",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We now fit the model. Choosing the hyperparameters of the training algorithm is tricky. The following seem to work reasonably well. (For a more systematic approach to tuning hyperparameters, see [here](https://scikit-learn.org/stable/modules/grid_search.html).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d70dc04",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "rbm.n_components = 100\n",
    "rbm.learning_rate = 0.02\n",
    "rbm.batch_size = 50\n",
    "rbm.n_iter = 20\n",
    "rbm.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2d4554",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We are ready to sample from the trained RBM. We extract the learned parameters from `rbm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb68bf82",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "W = rbm.components_\n",
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eca582",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "b = rbm.intercept_visible_\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc97d954",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "c = rbm.intercept_hidden_\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8979478c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To generate $25$ samples, we first generate $25$ independent initial states. We stack them into a matrix, where each row is a different flattened random noise image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bf1d1e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n_samples = 25\n",
    "z = rng.binomial(1, 0.5, (n_samples, n_pixels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13db60fe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To process all samples simultaneously, we make a small change to the code. We use [`numpy.reshape`](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html)\n",
    "to make the offsets into column vectors, which are then automatically added to all columns of the resulting weighted sum. \n",
    "(This is known as [broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaddeca",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def rbm_mean_hidden(v, W, c):\n",
    "    return sigmoid(W @ v + c.reshape(len(c),1))\n",
    "\n",
    "def rbm_mean_visible(h, W, b):\n",
    "    return sigmoid(W.T @ h + b.reshape(len(b),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3fea19",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "For plotting, we use a script [adapted from here](https://scikit-learn.org/stable/auto_examples/neural_networks/plot_rbm_logistic_classification.html) (with help from CHatGPT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908f7d92",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_imgs(z, n_imgs, nx_pixels, ny_pixels):\n",
    "    nx_imgs = np.floor(np.sqrt(n_imgs))\n",
    "    ny_imgs = np.ceil(np.sqrt(n_imgs))\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    for i, comp in enumerate(z):\n",
    "        plt.subplot(int(nx_imgs), int(ny_imgs), i + 1)\n",
    "        plt.imshow(comp.reshape((nx_pixels, ny_pixels)), cmap='gray_r')\n",
    "        plt.xticks([]), plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41ea159",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We are now ready to run our Gibbs sampler. The outcome depends on the number of steps we take. After $100$ steps, the outcome is somewhat realistic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f6a03c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "v_0 = z.T\n",
    "gen_v = rbm_gibbs_sampling(rng, 100, v_0, W, b, c)\n",
    "\n",
    "plot_imgs(gen_v.T, n_samples, nx_pixels, ny_pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f749dc",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
