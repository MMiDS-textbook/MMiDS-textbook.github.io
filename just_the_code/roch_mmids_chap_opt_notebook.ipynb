{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "046fc3de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell",
     "colab-keep"
    ]
   },
   "source": [
    "***\n",
    "\n",
    "*Course:* [Math 535](https://people.math.wisc.edu/~roch/mmids/) - Mathematical Methods in Data Science (MMiDS)  \n",
    "*Chapter:* 6-Optimization theory and algorithms   \n",
    "*Author:* [Sebastien Roch](https://people.math.wisc.edu/~roch/), Department of Mathematics, University of Wisconsin-Madison  \n",
    "*Updated:* Jan 22, 2024   \n",
    "*Copyright:* &copy; 2024 Sebastien Roch\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65ff76a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# You will need the files:\n",
    "#     * mmids.py\n",
    "#     * advertising.csv \n",
    "# from https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/tree/main/utils\n",
    "#\n",
    "# IF RUNNING ON GOOGLE COLAB (RECOMMENDED):\n",
    "# \"Upload to session storage\" from the Files tab on the left\n",
    "# Alternative instructions: https://colab.research.google.com/notebooks/io.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc37fdb8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# PYTHON 3\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import mmids\n",
    "seed = 535\n",
    "rng = np.random.default_rng(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccb5801",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Motivating example:  deciphering handwriting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b730bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We now turn to classification.\n",
    "\n",
    "Quoting [Wikipedia](https://en.wikipedia.org/wiki/Statistical_classification):\n",
    "\n",
    "> In machine learning and statistics, classification is the problem of identifying to which of a set of categories (sub-populations) a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known. Examples are assigning a given email to the \"spam\" or \"non-spam\" class, and assigning a diagnosis to a given patient based on observed characteristics of the patient (sex, blood pressure, presence or absence of certain symptoms, etc.). Classification is an example of pattern recognition. In the terminology of machine learning, classification is considered an instance of supervised learning, i.e., learning where a training set of correctly identified observations is available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0983547d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We will illustrate this problem on the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset. Quoting [Wikipedia](https://en.wikipedia.org/wiki/MNIST_database) again:\n",
    "\n",
    "> The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning. It was created by \"re-mixing\" the samples from NIST's original datasets. The creators felt that since NIST's training dataset was taken from American Census Bureau employees, while the testing dataset was taken from American high school students, it was not well-suited for machine learning experiments. Furthermore, the black and white images from NIST were normalized to fit into a 28x28 pixel bounding box and anti-aliased, which introduced grayscale levels. The MNIST database contains 60,000 training images and 10,000 testing images. Half of the training set and half of the test set were taken from NIST's training dataset, while the other half of the training set and the other half of the test set were taken from NIST's testing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c76a744",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Here is a sample of the images:\n",
    "\n",
    "![MNIST sample images](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)\n",
    "\n",
    "**Figure:** MNIST sample images ([Source](https://commons.wikimedia.org/wiki/File:MnistExamples.png))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a34e06",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We first load the data and convert it to an appropriate matrix representation. The data can be accessed with [`torchvision.datasets.MNIST`](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0631c811",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b67117a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Download and load the MNIST dataset\n",
    "mnist = datasets.MNIST(root='./data', \n",
    "                       train=True, \n",
    "                       download=True, \n",
    "                       transform=transforms.ToTensor())\n",
    "\n",
    "# Convert the dataset to a PyTorch DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(mnist, \n",
    "                                           batch_size=len(mnist), \n",
    "                                           shuffle=False)\n",
    "\n",
    "# Extract images and labels from the DataLoader\n",
    "imgs, labels = next(iter(train_loader))\n",
    "imgs = imgs.squeeze().numpy()\n",
    "labels = labels.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd81e66f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The [`squeeze()`](https://pytorch.org/docs/stable/generated/torch.Tensor.squeeze.html) above removes the color dimension in the image, which is grayscale. The [`numpy()`](https://pytorch.org/docs/stable/generated/torch.Tensor.numpy.html) converts the PyTorch tensors into Numpy arrays. See [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for details on the data loading. We will say more about PyTorch later in this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839683f3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "For example, the first image and its label are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecf5ed1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(imgs[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ab07af",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "outputs": [],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c9dfd6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "For now, we look at a subset of the samples: the 0's and 1's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5879373c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Filter out images with labels 0 and 1\n",
    "mask = (labels == 0) | (labels == 1)\n",
    "imgs01 = imgs[mask]\n",
    "labels01 = labels[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9c40c6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "In this new dataset, the first sample is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11641891",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(imgs01[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8245d022",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "labels01[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373eb404",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Next, we transform the images into vectors. For this we use the [`reshape()`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.reshape.html) function, which changes the dimensions of an array without changing its data. Here the first dimension, which runs across the samples, remains of the same length `len(imgs01)`. The `-1` is understood to be whatever is needed to \"fit\" the remaining dimensions, here $28 \\times 28 = 784$. In other words, we are effectively \"flattening\" each $28 \\times 28$ image into a single vector of length $784$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc42dccd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "imgs01.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63543711",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = imgs01.reshape(len(imgs01), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efd67db",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c8efd2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y = labels01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416365b3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The input data is now of the form $\\{(\\mathbf{x}_i, y_i) : i=1,\\ldots, n\\}$ where $\\mathbf{x}_i \\in \\mathbb{R}^d$ are the features and $y_i \\in \\{0,1\\}$ is the label. Above we use the matrix representation $X \\in \\mathbb{R}^{d \\times n}$ with columns $\\mathbf{x}_i$, $i = 1,\\ldots, n$ and $\\mathbf{y} = (y_1, \\ldots, y_n)^T \\in \\{0,1\\}^n$. \n",
    "\n",
    "Our goal: \n",
    "\n",
    "> to learn a classifier from the examples $\\{(\\mathbf{x}_i, y_i) : i=1,\\ldots, n\\}$, that is, a function $\\hat{f} : \\mathbb{R}^d \\to \\mathbb{R}$ such that $\\hat{f}(\\mathbf{x}_i) \\approx y_i$.\n",
    "\n",
    "This problem is referred to as [binary classification](https://en.wikipedia.org/wiki/Binary_classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7250b60c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Background: review of differentiable functions of several variables and introduction to automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89796a04",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We illustrate the use of [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) to compute gradients. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bafc962",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Quoting [Wikipedia](https://en.wikipedia.org/wiki/Automatic_differentiation):\n",
    "\n",
    "> In mathematics and computer algebra, automatic differentiation (AD), also called algorithmic differentiation or computational differentiation, is a set of techniques to numerically evaluate the derivative of a function specified by a computer program. AD exploits the fact that every computer program, no matter how complicated, executes a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (exp, log, sin, cos, etc.). By applying the chain rule repeatedly to these operations, derivatives of arbitrary order can be computed automatically, accurately to working precision, and using at most a small constant factor more arithmetic operations than the original program. Automatic differentiation is distinct from symbolic differentiation and numerical differentiation (the method of finite differences). Symbolic differentiation can lead to inefficient code and faces the difficulty of converting a computer program into a single expression, while numerical differentiation can introduce round-off errors in the discretization process and cancellation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1facfa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We will use the [PyTorch](https://pytorch.org/tutorials/). It uses [tensors](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html), which in many ways behave similarly to Numpy arrays. See [here](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html) for a quick introduction. Here is an example. We first initialize the tensors. Here each corresponds to a single real variable. With the option [`requires_grad=True`](https://pytorch.org/docs/stable/generated/torch.Tensor.requires_grad.html#torch.Tensor.requires_grad), we indicate that these are variables with respect to which a gradient will be taken later. We initialize the tensors where the derivatives will be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a543f652",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y = torch.tensor(2.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2ff6af",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The function [`.backward()`](https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html) computes the gradient using backpropagation, to which we will return later. The partial derivatives are accessed with [`.grad`](https://pytorch.org/docs/stable/generated/torch.Tensor.grad.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa3aaec",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Perform automatic differentiation\n",
    "f = 3 * x**2 + torch.exp(x) + y\n",
    "f.backward()  # Compute gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6677432e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Print gradients\n",
    "print(x.grad)  # df/dx\n",
    "print(y.grad)  # df/dy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416543aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The input parameters can also be vectors, which allows to consider function of large numbers of variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ffd271",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# New variables for the second example\n",
    "z = torch.tensor([1., 2., 3.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d7b9a4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Perform automatic differentiation\n",
    "g = torch.sum(z**2)\n",
    "g.backward()  # Compute gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e6736d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Print gradient\n",
    "print(z.grad)  # gradient is (2 z_1, 2 z_2, 2 z_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3fda1c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Here is another typical example in a data science context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd96c164",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Variables for the third example\n",
    "X = torch.randn(3, 2)  # Random dataset (features)\n",
    "y = torch.tensor([[1., 0., 1.]])  # Dataset (labels)\n",
    "theta = torch.ones(2, 1, requires_grad=True)  # Parameter assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db551d7f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Perform automatic differentiation\n",
    "predict = X @ theta  # Classifier with parameter vector θ\n",
    "loss = torch.sum((predict - y)**2)  # Loss function\n",
    "loss.backward()  # Compute gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09049c37",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Print gradient\n",
    "print(theta.grad)  # gradient of loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47b81d7",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87880319",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\newcommand{\\bSigma}{\\boldsymbol{\\Sigma}}$ $\\newcommand{\\bmu}{\\boldsymbol{\\mu}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e6668f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Optimality conditions and convexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352efcd3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**EXAMPLE:** Consider $f(x) = e^x$. Then $f'(x) = f''(x) = e^x$. Suppose we are interested in approximating $f$ in the interval $[0,1]$. We take $a=0$ and $b=1$ in *Taylor's Theorem*. The linear term is \n",
    "\n",
    "$$\n",
    "f(a) + (x-a) f'(a) = 1 + x e^0 = 1 + x.\n",
    "$$\n",
    "\n",
    "Then for any $x \\in [0,1]$\n",
    "\n",
    "$$\n",
    "f(x) = 1 + x + \\frac{1}{2}x^2 e^{\\xi_x}\n",
    "$$\n",
    "\n",
    "where $\\xi_x \\in (0,1)$ depends on $x$. We get a uniform bound on the error over $[0,1]$ by replacing $\\xi_x$ with its worst possible value over $[0,1]$ \n",
    "\n",
    "$$\n",
    "|f(x) - (1+x)| \\leq \\frac{1}{2}x^2 e^{\\xi_x} \\leq \\frac{e}{2} x^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f7b778",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0,1,100)\n",
    "y = np.exp(x)\n",
    "taylor = 1 + x\n",
    "err = (np.exp(1)/2) * x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36791e1b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.plot(x,y,label='f')\n",
    "plt.plot(x,taylor,label='taylor')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf4e4ca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "If we plot the upper and lower bounds, we see that $f$ indeed falls within them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f249b17c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.plot(x,y,label='f')\n",
    "plt.plot(x,taylor,label='taylor')\n",
    "plt.plot(x,taylor-err,linestyle=':',color='green',label='lower')\n",
    "plt.plot(x,taylor+err,linestyle='--',color='green',label='upper')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd438cbd",
   "metadata": {
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\lhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190dbff6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**EXAMPLE:** Let $f(x) = x^3$. Then $f'(x) = 3 x^2$ and $f''(x) = 6 x$ so that $f'(0) = 0$ and $f''(0) \\geq 0$. Hence $x=0$ is a stationary point. But $x=0$ is not a local minimizer. Indeed $f(0) = 0$ but, for any $\\delta > 0$, $f(-\\delta) < 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bf9d7d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-2,2,100)\n",
    "y = x**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1763f046",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.plot(x,y)\n",
    "plt.ylim(-5,5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82c3fb5",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\lhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6607bf2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Gradient descent and its convergence analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0f22d1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We implement gradient descent in Python. We assume that a function `f` and its gradient `grad_f` are provided. We first code the basic steepest descent step with a step size $\\alpha = $`alpha`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535e016e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def desc_update(grad_f, x, alpha):\n",
    "    return x - alpha*grad_f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec048ad",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def gd(f, grad_f, x0, alpha=1e-3, niters=int(1e6)):\n",
    "    \n",
    "    xk = x0\n",
    "    for _ in range(niters):\n",
    "        xk = desc_update(grad_f, xk, alpha)\n",
    "\n",
    "    return xk, f(xk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb263b05",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We illustrate on a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c98773",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def f(x): \n",
    "    return (x-1)**2 + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afac34ad",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "xgrid = np.linspace(-5,5,100)\n",
    "plt.plot(xgrid, f(xgrid))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae3afb1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def grad_f(x):\n",
    "    return 2*(x-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b41f6b",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gd(f, grad_f, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f980aa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We found a global minmizer in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fd9490",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The next example shows that a different local minimizer may be reached depending on the starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9554fc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def f(x): \n",
    "    return 4 * (x-1)**2 * (x+1)**2 - 2*(x-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18bb888",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "xgrid = np.linspace(-2,2,100)\n",
    "plt.plot(xgrid, f(xgrid), label='f')\n",
    "plt.ylim((-1,10))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2e397a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def grad_f(x): \n",
    "    return 8 * (x-1) * (x+1)**2 + 8 * (x-1)**2 * (x+1) - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f48b6b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "xgrid = np.linspace(-2,2,100)\n",
    "plt.plot(xgrid, f(xgrid), label='f')\n",
    "plt.plot(xgrid, grad_f(xgrid), label='grad_f')\n",
    "plt.ylim((-10,10))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3956d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "gd(f, grad_f, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07897d46",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gd(f, grad_f, -2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa33cc3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "In the final example, we end up at a stationary point that is not a local minimizer. Here both the first and second derivatives are zero. This is known as a [saddle point](https://en.wikipedia.org/wiki/Saddle_point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82ec8e1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a543591",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "xgrid = np.linspace(-2,2,100)\n",
    "plt.plot(xgrid, f(xgrid), label='f')\n",
    "plt.ylim((-10,10))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95205877",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def grad_f(x):\n",
    "    return 3 * x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a516acc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "xgrid = np.linspace(-2,2,100)\n",
    "plt.plot(xgrid, f(xgrid), label='f')\n",
    "plt.plot(xgrid, grad_f(xgrid), label='grad_f')\n",
    "plt.ylim((-10,10))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a08497",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "gd(f, grad_f, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f881408f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gd(f, grad_f, -2, niters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96934017",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Rather than explicitly specifying the gradient function, we could use PyTorch to compute it automatically. This is done next. Note that the descent update is done within [`with torch.no_grad()`](https://pytorch.org/docs/stable/generated/torch.no_grad.html), which ensures that the update operation itself is not tracked for gradient computation. Here the input `x0` as well as the output `xk.numpy(force=True)` are Numpy arrays. The function [`numpy()`](https://pytorch.org/docs/stable/generated/torch.Tensor.numpy.html) converts a PyTorch tensor to a Numpy array (see the documentation for an explanation of the `force=True` option)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef430ae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def gd_with_ad(f, x0, alpha=1e-3, niters=int(1e6)):\n",
    "    xk = torch.tensor(x0, \n",
    "                      requires_grad=True, \n",
    "                      dtype=torch.float)\n",
    "    \n",
    "    for _ in range(niters):\n",
    "        # Compute the function value and its gradient\n",
    "        value = f(xk)\n",
    "        value.backward()\n",
    "\n",
    "        # Perform a gradient descent step\n",
    "        with torch.no_grad():  # Temporarily set all requires_grad flags to False\n",
    "            xk -= alpha * xk.grad\n",
    "\n",
    "        # Zero the gradients for the next iteration\n",
    "        xk.grad.zero_()\n",
    "\n",
    "    return xk.numpy(force=True), f(xk).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f67023",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We revisit our last example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427bdcb3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gd_with_ad(f, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0744ea31",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gd_with_ad(f, -2, niters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51fea42",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05abed3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We give a numerical example using a special case of logistic regression. We illustrate it on a random dataset. The functions $\\hat{f}$, $\\mathcal{L}$ and $\\frac{\\partial}{\\partial x}\\mathcal{L}$ are defined next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4efa99",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(535)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6545cf5f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def fhat(x,a):\n",
    "    return 1 / ( 1 + np.exp(-np.outer(x,a)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58bcf66",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def loss(x,a,b): \n",
    "    return np.mean(-b*np.log(fhat(x,a)) - (1 - b)*np.log(1 - fhat(x,a)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6307438f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def grad(x,a,b):\n",
    "    return -np.mean((b - fhat(x,a))*a, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6ec9ab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "n = 10000\n",
    "a = 2*rng.uniform(0,1,n) - 1\n",
    "b = rng.integers(2, size=n)\n",
    "x = np.linspace(-1,1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967a41e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.plot(x, loss(x,a,b), label='loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2216c6d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We plot next the upper and lower bounds in the *Quadratic Bound for Smooth Functions* around $x = x_0$. Based on *Exercise 4.17*, we can take $L=1$. Observe that minimizing the upper quadratic bound leads to a decrease in $\\mathcal{L}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bcc245",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x0 = -0.3\n",
    "x = np.linspace(x0-0.05,x0+0.05,100)\n",
    "upper = loss(x0,a,b) + (x - x0)*grad(x0,a,b) + (1/2)*(x - x0)**2 # upper approximation\n",
    "lower = loss(x0,a,b) + (x - x0)*grad(x0,a,b) - (1/2)*(x - x0)**2 # lower approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c6710c",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.plot(x, loss(x,a,b), label='loss')\n",
    "plt.plot(x, upper, label='upper')\n",
    "plt.plot(x, lower, label='lower')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1560ff2",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0bd588",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We revisit our first simple single-variable example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf24c1b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def f(x): \n",
    "    return (x-1)**2 + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef12da6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "xgrid = np.linspace(-5,5,100)\n",
    "plt.plot(xgrid, f(xgrid))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90480869",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Recall that the first derivative is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9d7146",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def grad_f(x):\n",
    "    return 2*(x-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7979df04",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "So the second derivative is $f''(x) = 2$. Hence, this $f$ is $L$-smooth and $m$-strongly convex with $L = m = 2$. The theory we developed suggests taking step size $\\alpha_t = \\alpha = 1/L = 1/2$. It also implies that\n",
    "\n",
    "$$\n",
    "f(x^1) - f(x^*)\n",
    "\\leq \\left(1 - \\frac{m}{L}\\right) [f(x^0) - f(x^*)]\n",
    "= 0.\n",
    "$$\n",
    "\n",
    "We converge in one step! And that holds for any starting point $x^0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989e4dd8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Let's try this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cff7fd",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gd(f, grad_f, 0, alpha=0.5, niters=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206ad7be",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Let's try a different starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3b7cbd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gd(f, grad_f, 100, alpha=0.5, niters=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b4b38b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22990d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Backpropagation and application to neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad898e38",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**The `Advertising` dataset and the least-squares solution** We return to the `Advertising` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccedffb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('advertising.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acc93e7",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "n = len(df.index)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5741a2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We first compute the solution using the least-squares approach we detailed previously. We use [`numpy.column_stack`](https://numpy.org/doc/stable/reference/generated/numpy.column_stack.html#numpy.column_stack) to add a column of ones to the feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cda529",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "TV = df['TV'].to_numpy()\n",
    "radio = df['radio'].to_numpy()\n",
    "newspaper = df['newspaper'].to_numpy()\n",
    "sales = df['sales'].to_numpy()\n",
    "features = np.stack((TV, radio, newspaper), axis=-1)\n",
    "A = np.column_stack((np.ones(n), features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3577aa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "coeff = mmids.ls_by_qr(A, sales)\n",
    "print(coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07e049c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.mean((A @ coeff - sales)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6143da",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**Solving the problem using PyTorch** We will be using PyTorch to implement the previous method. We first convert the data into PyTorch tensors. We then use [`torch.utils.data.TensorDataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset) to create the dataset. Finally, [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) provides the utilities to load the data in batches for training. We take mini-batches of size `BATCH_SIZE = 64` and we apply a random permutation of the samples on every pass (with the option `shuffle=True`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeffc2f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "features_tensor = torch.tensor(features, \n",
    "                               dtype=torch.float32)\n",
    "sales_tensor = torch.tensor(sales, \n",
    "                            dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79758a53",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create a dataset and dataloader for training\n",
    "BATCH_SIZE = 64\n",
    "train_dataset = TensorDataset(features_tensor, sales_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c71917",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Now we construct our model. It is simply an affine map from $\\mathbb{R}^3$ to $\\mathbb{R}$. Note that there is no need to pre-process the inputs by adding $1$s. A constant term (or \"bias variable\") is automatically added by PyTorch (unless one chooses the option [`bias=False`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5550b84c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define the model using nn.Sequential\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(3, 1)  # 3 input features, 1 output value\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454b25fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Finally, we are ready to run an optimization method of our choice on the loss function, which are specified next. There are many [optimizers](https://pytorch.org/docs/stable/optim.html#algorithms) available. (See this [post](https://hackernoon.com/demystifying-different-variants-of-gradient-descent-optimization-algorithm-19ae9ba2e9bc) for a brief explanation of many common optimizers.) Here we use SGD as the optimizer. And the loss function is the MSE. A quick tutorial is [here](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html).\n",
    "\n",
    "Choosing the right number of passes (i.e. epochs) through the data requires some experimenting. Here $10^4$ suffices. But in the interest of time, we will run it only for $100$ epochs. As you will see from the results, this is not quite enough. On each pass, we compute the output of the current model, use `backward()` to obtain the gradient, and then perform a descent update with `step()`. We also have to reset the gradients first (otherwise they add up by default). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ecc5e1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Compile the model: define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5397aaa3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ecf224",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The final parameters and loss are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a26e19",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Get and print the model weights and bias\n",
    "weights = model[0].weight.detach().numpy()\n",
    "bias = model[0].bias.detach().numpy()\n",
    "print(\"Weights:\", weights)\n",
    "print(\"Bias:\", bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b46e1a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f\"Mean Squared Error on Training Set: {total_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16c876c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**MNIST dataset** We will use the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset introduced earlier in the chapter. This example is inspired by [these](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html) [tutorial](https://www.tensorflow.org/tutorials/keras/classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350dc5ef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Here is a sample of the images:\n",
    "\n",
    "![MNIST sample images](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)\n",
    "\n",
    "**Figure:** MNIST sample images ([Source](https://commons.wikimedia.org/wiki/File:MnistExamples.png))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1953aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We first load the data. As before, the training dataset is a tensor -- think matrix with $3$ indices. One index runs through the $60,000$ training images, while the other two indices run through the horizontal and vertical pixel axes of each image. Here each image is $28 \\times 28$. The training labels are between $0$ and $9$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f33e0f5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Load and normalize the MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', \n",
    "                               train=True, \n",
    "                               download=True, \n",
    "                               transform=transforms.ToTensor())\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data', \n",
    "                              train=False, \n",
    "                              download=True, \n",
    "                              transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3102f12",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                          batch_size=BATCH_SIZE, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508c38d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**Implementation** We implement multinomial logistic regression to learn a classifier for the MNIST data. We first check for the availability of GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff28bd16",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115a8366",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "In PyTorch, composition of functions can be achieved with [`torch.nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html). Our model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35429470",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define the model using nn.Sequential and move it to the device (GPU if available)\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28 * 28, 10)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425d0454",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The [`torch.nn.Flatten`](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html) layer turns each input image into a vector of size $784$ (where $784 = 28^2$ is the number of pixels in each image). The final output is $10$-dimensional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa487cd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Here we use the [`torch.optim.Adam`](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) optimizer (you can try SGD, but it is slow). The loss function is the [cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy), as implemented by [`torch.nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33fe931",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Compile the model: define loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cccf6b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We train for $5$ epochs. An epoch is one training iteration where all samples are iterated once (in a randomly shuffled order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef71e872",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(dataloader, model, loss_fn, optimizer, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3039864e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "epochs = 5  # Adjust the number of epochs as needed\n",
    "for epoch in range(epochs):\n",
    "    train(train_loader, model, loss_fn, optimizer, device)\n",
    "    \n",
    "    if (epoch+1) % 1 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cb7211",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Because of the issue of [overfitting](https://en.wikipedia.org/wiki/Overfitting), we use the *test* images to assess the performance of the final classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7b8bac",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def test(dataloader, model, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(dim=1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    accuracy = correct / size\n",
    "    print(f\"Test error: {(100*accuracy):>0.1f}% accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b37576",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test(test_loader, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6597d3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To make a prediction, we take a [`torch.nn.functional.softmax`](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html) of the output of our model. It transforms the output into a probability for each label. It is implicitly included in the cross-entropy loss, but is not actually part of the trained model. (Note that the softmax itself has no parameter.) \n",
    "\n",
    "As an illustration, we do this for each test image. We use [`torch.cat`](https://pytorch.org/docs/stable/generated/torch.cat.html) to concatenate a sequence of tensors into a single tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabc1823",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def predict_softmax(dataloader, model, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            probabilities = F.softmax(pred, dim=1)\n",
    "            predictions.append(probabilities.cpu())  # Move predictions to CPU\n",
    "\n",
    "    return torch.cat(predictions, dim=0)\n",
    "\n",
    "predictions = predict_softmax(test_loader, model, device).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cda3a7d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The result for the first test image is shown below. To make a prediction, we choose the label with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4153536a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c789a354",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "predictions[0].argmax(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a632d395",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The truth is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10064cc5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "images, labels = next(iter(test_loader))\n",
    "images = images.squeeze().numpy()\n",
    "labels = labels.numpy()\n",
    "labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dd6508",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Above, `next(iter(test_loader))` loads the first batch of test images. (See [here](https://docs.python.org/3/tutorial/classes.html#iterators) for background on iterators in Python.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f4b502",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The following code, adapted from [here](https://www.tensorflow.org/tutorials/keras/classification), provides a neat vizualization of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774575e4",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "class_names = ['0', '1', '2', '3', '4',\n",
    "               '5', '6', '7', '8', '9']\n",
    "\n",
    "def plot_image(predictions_array, true_label, img):\n",
    "    \n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.imshow(img, cmap=plt.cm.binary)\n",
    "\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "    if predicted_label == true_label:\n",
    "        color = 'blue'\n",
    "    else:\n",
    "        color = 'red'\n",
    "\n",
    "    plt.xlabel(f\"{class_names[predicted_label]} {100*np.max(predictions_array):2.0f}% ({class_names[true_label]})\", \n",
    "               color=color)\n",
    "\n",
    "def plot_value_array(predictions_array, true_label):\n",
    "    \n",
    "    plt.grid(False)\n",
    "    plt.xticks(range(10))\n",
    "    plt.yticks([])\n",
    "    thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
    "    plt.ylim([0, 1])\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    " \n",
    "    thisplot[predicted_label].set_color('red')\n",
    "    thisplot[true_label].set_color('blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e85f04",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Here's the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e147ab",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Visualization code for individual image\n",
    "i = 0\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.subplot(1,2,1)\n",
    "plot_image(predictions[i], labels[i], images[i])  \n",
    "plt.subplot(1,2,2)\n",
    "plot_value_array(predictions[i], labels[i]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f916192",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "This one is a little less clear. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83cbd6f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Visualization code for individual and multiple images\n",
    "i = 11\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.subplot(1,2,1)\n",
    "plot_image(predictions[i], labels[i], images[i])  \n",
    "plt.subplot(1,2,2)\n",
    "plot_value_array(predictions[i], labels[i]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aafd764",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "This one is wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40666139",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Visualization code for individual and multiple images\n",
    "i = 8\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.subplot(1,2,1)\n",
    "plot_image(predictions[i], labels[i], images[i])  \n",
    "plt.subplot(1,2,2)\n",
    "plot_value_array(predictions[i], labels[i]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308ec42d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**Implementation** We implement a neural network in PyTorch. We use the MNIST dataset again. We have already loaded it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8d35ad",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We construct a three-layer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19576af8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define the model using nn.Sequential\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(),                      # Flatten the input\n",
    "    nn.Linear(28 * 28, 32),            # First Linear layer with 32 nodes\n",
    "    nn.Sigmoid(),                      # Sigmoid activation function\n",
    "    nn.Linear(32, 10)                  # Second Linear layer with 10 nodes (output layer)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2eafcb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "As we did for multinomial logistic regression, we use the Adam optimizer and the cross-entropy loss. We also monitor progress by keeping track of the accuracy on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6679511d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define the loss function and the optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abde68ba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We train for 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ee83c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "epochs = 5  # Adjust the number of epochs as needed\n",
    "for epoch in range(epochs):\n",
    "    train(train_loader, model, loss_fn, optimizer, device)\n",
    "    \n",
    "    if (epoch+1) % 1 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e4f095",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "On the test data, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ca4498",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test(test_loader, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa45a73",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "This is a significantly more accurate model than what we obtained using multinomial logistic regression. One can do even better using a neural network tailored for images, known as [convolutional neural networks](https://cs231n.github.io/convolutional-networks/)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
