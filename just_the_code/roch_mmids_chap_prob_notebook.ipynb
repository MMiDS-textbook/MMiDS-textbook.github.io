{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79aef1c7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell",
     "colab-keep"
    ]
   },
   "source": [
    "***\n",
    "\n",
    "*Course:* [Math 535](https://people.math.wisc.edu/~roch/mmids/) - Mathematical Methods in Data Science (MMiDS)  \n",
    "*Chapter:* 6-Probabilistic models: from simple to complex   \n",
    "*Author:* [Sebastien Roch](https://people.math.wisc.edu/~roch/), Department of Mathematics, University of Wisconsin-Madison  \n",
    "*Updated:* July 15, 2024   \n",
    "*Copyright:* &copy; 2024 Sebastien Roch\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57474b75",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# You will need the files:\n",
    "#     * mmids.py\n",
    "# from https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/tree/main/utils\n",
    "#\n",
    "# IF RUNNING ON GOOGLE COLAB (RECOMMENDED):\n",
    "# \"Upload to session storage\" from the Files tab on the left\n",
    "# Alternative instructions: https://colab.research.google.com/notebooks/io.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f18e25",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch\n",
    "import mmids\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9250538d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\newcommand{\\bmu}{\\boldsymbol{\\mu}}$ \n",
    "$\\newcommand{\\bSigma}{\\boldsymbol{\\Sigma}}$\n",
    "$\\newcommand{\\bfbeta}{\\boldsymbol{\\beta}}$ \n",
    "$\\newcommand{\\bflambda}{\\boldsymbol{\\lambda}}$\n",
    "$\\newcommand{\\bgamma}{\\boldsymbol{\\gamma}}$ \n",
    "$\\newcommand{\\bsigma}{{\\boldsymbol{\\sigma}}}$\n",
    "$\\newcommand{\\bpi}{\\boldsymbol{\\pi}}$\n",
    "$\\newcommand{\\btheta}{{\\boldsymbol{\\theta}}}$ \n",
    "$\\newcommand{\\bphi}{\\boldsymbol{\\phi}}$ \n",
    "$\\newcommand{\\balpha}{\\boldsymbol{\\alpha}}$\n",
    "$\\newcommand{\\blambda}{\\boldsymbol{\\lambda}}$\n",
    "$\\renewcommand{\\P}{\\mathbb{P}}$\n",
    "$\\newcommand{\\E}{\\mathbb{E}}$\n",
    "$\\newcommand{\\indep}{\\perp\\!\\!\\!\\perp} \\newcommand{\\bx}{\\mathbf{x}}$ \n",
    "$\\newcommand{\\bp}{\\mathbf{p}}$\n",
    "$\\renewcommand{\\bx}{\\mathbf{x}}$\n",
    "$\\newcommand{\\bX}{\\mathbf{X}}$ \n",
    "$\\newcommand{\\by}{\\mathbf{y}}$ \n",
    "$\\newcommand{\\bY}{\\mathbf{Y}}$ \n",
    "$\\newcommand{\\bz}{\\mathbf{z}}$\n",
    "$\\newcommand{\\bZ}{\\mathbf{Z}}$ \n",
    "$\\newcommand{\\bw}{\\mathbf{w}}$ \n",
    "$\\newcommand{\\bW}{\\mathbf{W}}$\n",
    "$\\newcommand{\\bv}{\\mathbf{v}}$ \n",
    "$\\newcommand{\\bV}{\\mathbf{V}}$\n",
    "$\\newcommand{\\bfg}{\\mathbf{g}}$ \n",
    "$\\newcommand{\\bfh}{\\mathbf{h}}$ \n",
    "$\\newcommand{\\horz}{\\rule[.5ex]{2.5ex}{0.5pt}}$\n",
    "$\\renewcommand{\\S}{\\mathcal{S}}$ \n",
    "$\\newcommand{\\X}{\\mathcal{X}}$ \n",
    "$\\newcommand{\\var}{\\mathrm{Var}}$ \n",
    "$\\newcommand{\\pa}{\\mathrm{pa}}$\n",
    "$\\newcommand{\\Z}{\\mathcal{Z}}$ \n",
    "$\\newcommand{\\bh}{\\mathbf{h}}$ \n",
    "$\\newcommand{\\bb}{\\mathbf{b}}$ \n",
    "$\\newcommand{\\bc}{\\mathbf{c}}$ \n",
    "$\\newcommand{\\cE}{\\mathcal{E}}$ \n",
    "$\\newcommand{\\cP}{\\mathcal{P}}$\n",
    "$\\newcommand{\\bbeta}{\\boldsymbol{\\beta}}$\n",
    "$\\newcommand{\\bLambda}{\\boldsymbol{\\Lambda}}$\n",
    "$\\newcommand{\\cov}{\\mathrm{Cov}}$\n",
    "$\\newcommand{\\bfk}{\\mathbf{k}}$\n",
    "$\\newcommand{\\idx}[1]{}$\n",
    "$\\newcommand{\\xdi}{}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18be832",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Motivating example: tracking location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec658af",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Suppose we let loose a cyborg corgi in a large park. We would like to know where it is at all time. For this purpose, it has an implanted location device that sends a signal to a tracking app.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc082c9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Here is an example of the data we might have, through a simulation we will explain later on in this chapter. The dots are recorded locations at regular time intervals. The dotted line helps keep track of the time order of the recordings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a7c983",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "By convention, we start at $(0,0)$. Notice how squiggly the trajectory is. One issue might be that the times  at which the location is recorded are too far between. But, in fact, there is another issue: the tracking device is *inaccurate*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4161fc4f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To get a better estimate of the true trajectory, it is natural to try to model the noise in the measurement as well as the dynamics itself. Probabilistic models are perfectly suited for this. \n",
    "\n",
    "In this chapter, we will encounter of variety of such models and show how to take advantage of them to estimate unknown states (or parameters). In particular, conditional independence will play a key role.\n",
    "\n",
    "We will come back to location tracking later in the chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6604577a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\newcommand{\\bmu}{\\boldsymbol{\\mu}}$ \n",
    "$\\newcommand{\\bSigma}{\\boldsymbol{\\Sigma}}$\n",
    "$\\newcommand{\\bfbeta}{\\boldsymbol{\\beta}}$ \n",
    "$\\newcommand{\\bflambda}{\\boldsymbol{\\lambda}}$\n",
    "$\\newcommand{\\bgamma}{\\boldsymbol{\\gamma}}$ \n",
    "$\\newcommand{\\bsigma}{{\\boldsymbol{\\sigma}}}$\n",
    "$\\newcommand{\\bpi}{\\boldsymbol{\\pi}}$\n",
    "$\\newcommand{\\btheta}{{\\boldsymbol{\\theta}}}$ \n",
    "$\\newcommand{\\bphi}{\\boldsymbol{\\phi}}$ \n",
    "$\\newcommand{\\balpha}{\\boldsymbol{\\alpha}}$\n",
    "$\\newcommand{\\blambda}{\\boldsymbol{\\lambda}}$\n",
    "$\\renewcommand{\\P}{\\mathbb{P}}$\n",
    "$\\newcommand{\\E}{\\mathbb{E}}$\n",
    "$\\newcommand{\\indep}{\\perp\\!\\!\\!\\perp} \\newcommand{\\bx}{\\mathbf{x}}$ \n",
    "$\\newcommand{\\bp}{\\mathbf{p}}$\n",
    "$\\renewcommand{\\bx}{\\mathbf{x}}$\n",
    "$\\newcommand{\\bX}{\\mathbf{X}}$ \n",
    "$\\newcommand{\\by}{\\mathbf{y}}$ \n",
    "$\\newcommand{\\bY}{\\mathbf{Y}}$ \n",
    "$\\newcommand{\\bz}{\\mathbf{z}}$\n",
    "$\\newcommand{\\bZ}{\\mathbf{Z}}$ \n",
    "$\\newcommand{\\bw}{\\mathbf{w}}$ \n",
    "$\\newcommand{\\bW}{\\mathbf{W}}$\n",
    "$\\newcommand{\\bv}{\\mathbf{v}}$ \n",
    "$\\newcommand{\\bV}{\\mathbf{V}}$\n",
    "$\\newcommand{\\bfg}{\\mathbf{g}}$ \n",
    "$\\newcommand{\\bfh}{\\mathbf{h}}$ \n",
    "$\\newcommand{\\horz}{\\rule[.5ex]{2.5ex}{0.5pt}}$\n",
    "$\\renewcommand{\\S}{\\mathcal{S}}$ \n",
    "$\\newcommand{\\X}{\\mathcal{X}}$ \n",
    "$\\newcommand{\\var}{\\mathrm{Var}}$ \n",
    "$\\newcommand{\\pa}{\\mathrm{pa}}$\n",
    "$\\newcommand{\\Z}{\\mathcal{Z}}$ \n",
    "$\\newcommand{\\bh}{\\mathbf{h}}$ \n",
    "$\\newcommand{\\bb}{\\mathbf{b}}$ \n",
    "$\\newcommand{\\bc}{\\mathbf{c}}$ \n",
    "$\\newcommand{\\cE}{\\mathcal{E}}$ \n",
    "$\\newcommand{\\cP}{\\mathcal{P}}$\n",
    "$\\newcommand{\\bbeta}{\\boldsymbol{\\beta}}$\n",
    "$\\newcommand{\\bLambda}{\\boldsymbol{\\Lambda}}$\n",
    "$\\newcommand{\\cov}{\\mathrm{Cov}}$\n",
    "$\\newcommand{\\bfk}{\\mathbf{k}}$\n",
    "$\\newcommand{\\idx}[1]{}$\n",
    "$\\newcommand{\\xdi}{}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4375e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Background: introduction to parametric families and maximum likelihood estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecfef12",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** The following code, which plots the density in the bivariate case, was adapted from [gauss_plot_2d.ipynb](https://github.com/probml/pyprobml/blob/master/notebooks/book1/03/gauss_plot_2d.ipynb) by ChatGPT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18992a62",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**CHAT & LEARN** Ask your favorite AI chatbot to explain the code! Try different covariance matrices. ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb)) $\\ddagger$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51de816",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "def gaussian_pdf(X, Y, mean, cov):\n",
    "    xy = np.stack([X.flatten(), Y.flatten()], axis=-1)\n",
    "    return multivariate_normal.pdf(\n",
    "        xy, mean=mean, cov=cov).reshape(X.shape)\n",
    "\n",
    "def make_surface_plot(X, Y, Z):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    surf = ax.plot_surface(\n",
    "        X, Y, Z, cmap=plt.cm.viridis, antialiased=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5d6d36",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We plot the density for mean $(0,0)$ with two different covariance matrices:\n",
    "\n",
    "$$\n",
    "\\bSigma_1 = \\begin{bmatrix} 1.0 & 0 \\\\ 0 & 1.0 \\end{bmatrix}\n",
    "\\quad \\text{and} \\quad \n",
    "\\bSigma_2 = \\begin{bmatrix} \\sigma_1^2 & \\rho \\sigma_1 \\sigma_2 \\\\ \\rho \\sigma_1 \\sigma_2 & \\sigma_2^2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $\\sigma_1 = 1.5$, $\\sigma_2 = 0.5$ and $\\rho = -0.75$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a36c6a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "start_point = 5\n",
    "stop_point = 5\n",
    "num_samples = 100\n",
    "points = np.linspace(-start_point, stop_point, num_samples)\n",
    "X, Y = np.meshgrid(points, points)\n",
    "\n",
    "mean = np.array([0., 0.])\n",
    "cov = np.array([[1., 0.], [0., 1.]])\n",
    "make_surface_plot(X, Y, gaussian_pdf(X, Y, mean, cov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6899d1e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "mean = np.array([0., 0.])\n",
    "cov = np.array([[1.5 ** 2., -0.75 * 1.5 * 0.5], \n",
    "                 [-0.75 * 1.5 * 0.5, 0.5 ** 2.]])\n",
    "make_surface_plot(X, Y, gaussian_pdf(X, Y, mean, cov))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c707c82",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af77d78",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** In Numpy, as we have seen before, the module [numpy.random](https://numpy.org/doc/stable/reference/random/index.html) provides a way to sample from a variety of standard distributions. We first initialize the [pseudorandom number generator](https://en.wikipedia.org/wiki/Pseudorandom_number_generator)$\\idx{pseudorandom number generator}\\xdi$ with a [random seed](https://en.wikipedia.org/wiki/Random_seed). Recall that it allows the results to be reproducible: using the same seed produces the same results again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b8904c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "seed = 535\n",
    "rng = np.random.default_rng(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6651c271",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Here's are lists of available [probability distributions](https://numpy.org/doc/stable/reference/random/generator.html#distributions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f35a2d8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "p = 0.1\n",
    "N = 5\n",
    "print(rng.binomial(1, p, size=N))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a77578b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Here are a few other examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599c2373",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "p = [0.1, 0.2, 0.7]\n",
    "n = 100\n",
    "print(rng.multinomial(n, p, size=N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0844f8b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "mu = np.array([0.1, -0.3])\n",
    "sig = np.array([[2., 0.],[0., 3.]])\n",
    "print(rng.multivariate_normal(mu, sig, size=N))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539fd339",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515a3737",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\newcommand{\\bmu}{\\boldsymbol{\\mu}}$ \n",
    "$\\newcommand{\\bSigma}{\\boldsymbol{\\Sigma}}$\n",
    "$\\newcommand{\\bfbeta}{\\boldsymbol{\\beta}}$ \n",
    "$\\newcommand{\\bflambda}{\\boldsymbol{\\lambda}}$\n",
    "$\\newcommand{\\bgamma}{\\boldsymbol{\\gamma}}$ \n",
    "$\\newcommand{\\bsigma}{{\\boldsymbol{\\sigma}}}$\n",
    "$\\newcommand{\\bpi}{\\boldsymbol{\\pi}}$\n",
    "$\\newcommand{\\btheta}{{\\boldsymbol{\\theta}}}$ \n",
    "$\\newcommand{\\bphi}{\\boldsymbol{\\phi}}$ \n",
    "$\\newcommand{\\balpha}{\\boldsymbol{\\alpha}}$\n",
    "$\\newcommand{\\blambda}{\\boldsymbol{\\lambda}}$\n",
    "$\\renewcommand{\\P}{\\mathbb{P}}$\n",
    "$\\newcommand{\\E}{\\mathbb{E}}$\n",
    "$\\newcommand{\\indep}{\\perp\\!\\!\\!\\perp} \\newcommand{\\bx}{\\mathbf{x}}$ \n",
    "$\\newcommand{\\bp}{\\mathbf{p}}$\n",
    "$\\renewcommand{\\bx}{\\mathbf{x}}$\n",
    "$\\newcommand{\\bX}{\\mathbf{X}}$ \n",
    "$\\newcommand{\\by}{\\mathbf{y}}$ \n",
    "$\\newcommand{\\bY}{\\mathbf{Y}}$ \n",
    "$\\newcommand{\\bz}{\\mathbf{z}}$\n",
    "$\\newcommand{\\bZ}{\\mathbf{Z}}$ \n",
    "$\\newcommand{\\bw}{\\mathbf{w}}$ \n",
    "$\\newcommand{\\bW}{\\mathbf{W}}$\n",
    "$\\newcommand{\\bv}{\\mathbf{v}}$ \n",
    "$\\newcommand{\\bV}{\\mathbf{V}}$\n",
    "$\\newcommand{\\bfg}{\\mathbf{g}}$ \n",
    "$\\newcommand{\\bfh}{\\mathbf{h}}$ \n",
    "$\\newcommand{\\horz}{\\rule[.5ex]{2.5ex}{0.5pt}}$\n",
    "$\\renewcommand{\\S}{\\mathcal{S}}$ \n",
    "$\\newcommand{\\X}{\\mathcal{X}}$ \n",
    "$\\newcommand{\\var}{\\mathrm{Var}}$ \n",
    "$\\newcommand{\\pa}{\\mathrm{pa}}$\n",
    "$\\newcommand{\\Z}{\\mathcal{Z}}$ \n",
    "$\\newcommand{\\bh}{\\mathbf{h}}$ \n",
    "$\\newcommand{\\bb}{\\mathbf{b}}$ \n",
    "$\\newcommand{\\bc}{\\mathbf{c}}$ \n",
    "$\\newcommand{\\cE}{\\mathcal{E}}$ \n",
    "$\\newcommand{\\cP}{\\mathcal{P}}$\n",
    "$\\newcommand{\\bbeta}{\\boldsymbol{\\beta}}$\n",
    "$\\newcommand{\\bLambda}{\\boldsymbol{\\Lambda}}$\n",
    "$\\newcommand{\\cov}{\\mathrm{Cov}}$\n",
    "$\\newcommand{\\bfk}{\\mathbf{k}}$\n",
    "$\\newcommand{\\idx}[1]{}$\n",
    "$\\newcommand{\\xdi}{}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa31dc56",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Modeling more complex dependencies 1: using conditional independence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31383a6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We implement the Naive Bayes model with Laplace smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938cf6b1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We encode the data into a table, where the rows are the classes and the columns are the features. The entries are the corresponding $N_{k,m}$s. In addition we provide the vector $N_k$, which is the last column above, and the value $N$, which is the sum of the entries of $N_k$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc6f920",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def nb_fit_table(N_km, alpha=1., beta=1.):\n",
    "    \n",
    "    K, M = N_km.shape\n",
    "    N_k = np.sum(N_km,axis=-1)\n",
    "    N = np.sum(N_k)\n",
    "    pi_k = (N_k+alpha) / (N+K*alpha)\n",
    "    p_km = (N_km+beta) / (N_k[:,None]+2*beta)\n",
    "\n",
    "    return pi_k, p_km"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53be720",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The next function computes the negative logarithm of $\\pi_k \\prod_{m=1}^M p_{k,m}^{x_m} (1-p_{k,m})^{1-x_m}$, that is, the score of $k$, and outputs a $k$ achieving the minimum score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4546c6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def nb_predict(pi_k, p_km, x, label_set):\n",
    "   \n",
    "    K = len(pi_k)\n",
    "    \n",
    "    score_k = np.zeros(K)\n",
    "    for k in range(K):\n",
    "       \n",
    "        score_k[k] -= np.log(pi_k[k])\n",
    "        score_k[k] -= np.sum(x * np.log(p_km[k,:]) \n",
    "                               + (1 - x)*np.log(1 - p_km[k,:]))\n",
    "\n",
    "    return label_set[np.argmin(score_k, axis=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d6c0b9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We use a simple example from [Towards Data Science](https://towardsdatascience.com/all-about-naive-bayes-8e13cef044cf):\n",
    "\n",
    "> **Example:** let’s say we have data on 1000 pieces of fruit. The fruit being a Banana, Orange or some other fruit and imagine we know 3 features of each fruit, whether it’s long or not, sweet or not and yellow or not, as displayed in the table below.\n",
    "\n",
    "| Fruit   | Long | Sweet | Yellow | Total |\n",
    "|---------|------|-------|--------|-------|\n",
    "| Banana  | 400  | 350   | 450    | 500   |\n",
    "| Orange  | 0    | 150   | 300    | 300   |\n",
    "| Other   | 100  | 150   | 50     | 200   |\n",
    "| Total   | 500  | 650   | 800    | 1000  |\n",
    "\n",
    "> [...] Which should provide enough evidence to predict the class of another fruit as it’s introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f236403",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "N_km = np.array([[400., 350., 450.],\n",
    "                 [0., 150., 300.],\n",
    "                 [100., 150., 50.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1010f4fb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We run `nb_fit_table` on our simple dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9953e67e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pi_k, p_km = nb_fit_table(N_km)\n",
    "print(pi_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e994f9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(p_km)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f0c1c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Continuing on with our previous example:\n",
    "\n",
    "> So let’s say we’re given the features of a piece of fruit and we need to predict the class. If we’re told that the additional fruit is Long, Sweet and Yellow, we can classify it using the [prediction] formula and subbing in the values for each outcome, whether it’s a Banana, an Orange or Other Fruit. The one with the highest probability (score) being the winner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9c6f6d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We run `nb_predict` on our dataset with the additional fruit from the quote above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0d08cd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "label_set = ['Banana', 'Orange', 'Other']\n",
    "x = np.array([1., 1., 1.])\n",
    "nb_predict(pi_k, p_km, x, label_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7261d9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965daf48",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\newcommand{\\bmu}{\\boldsymbol{\\mu}}$ \n",
    "$\\newcommand{\\bSigma}{\\boldsymbol{\\Sigma}}$\n",
    "$\\newcommand{\\bfbeta}{\\boldsymbol{\\beta}}$ \n",
    "$\\newcommand{\\bflambda}{\\boldsymbol{\\lambda}}$\n",
    "$\\newcommand{\\bgamma}{\\boldsymbol{\\gamma}}$ \n",
    "$\\newcommand{\\bsigma}{{\\boldsymbol{\\sigma}}}$\n",
    "$\\newcommand{\\bpi}{\\boldsymbol{\\pi}}$\n",
    "$\\newcommand{\\btheta}{{\\boldsymbol{\\theta}}}$ \n",
    "$\\newcommand{\\bphi}{\\boldsymbol{\\phi}}$ \n",
    "$\\newcommand{\\balpha}{\\boldsymbol{\\alpha}}$\n",
    "$\\newcommand{\\blambda}{\\boldsymbol{\\lambda}}$\n",
    "$\\renewcommand{\\P}{\\mathbb{P}}$\n",
    "$\\newcommand{\\E}{\\mathbb{E}}$\n",
    "$\\newcommand{\\indep}{\\perp\\!\\!\\!\\perp} \\newcommand{\\bx}{\\mathbf{x}}$ \n",
    "$\\newcommand{\\bp}{\\mathbf{p}}$\n",
    "$\\renewcommand{\\bx}{\\mathbf{x}}$\n",
    "$\\newcommand{\\bX}{\\mathbf{X}}$ \n",
    "$\\newcommand{\\by}{\\mathbf{y}}$ \n",
    "$\\newcommand{\\bY}{\\mathbf{Y}}$ \n",
    "$\\newcommand{\\bz}{\\mathbf{z}}$\n",
    "$\\newcommand{\\bZ}{\\mathbf{Z}}$ \n",
    "$\\newcommand{\\bw}{\\mathbf{w}}$ \n",
    "$\\newcommand{\\bW}{\\mathbf{W}}$\n",
    "$\\newcommand{\\bv}{\\mathbf{v}}$ \n",
    "$\\newcommand{\\bV}{\\mathbf{V}}$\n",
    "$\\newcommand{\\bfg}{\\mathbf{g}}$ \n",
    "$\\newcommand{\\bfh}{\\mathbf{h}}$ \n",
    "$\\newcommand{\\horz}{\\rule[.5ex]{2.5ex}{0.5pt}}$\n",
    "$\\renewcommand{\\S}{\\mathcal{S}}$ \n",
    "$\\newcommand{\\X}{\\mathcal{X}}$ \n",
    "$\\newcommand{\\var}{\\mathrm{Var}}$ \n",
    "$\\newcommand{\\pa}{\\mathrm{pa}}$\n",
    "$\\newcommand{\\Z}{\\mathcal{Z}}$ \n",
    "$\\newcommand{\\bh}{\\mathbf{h}}$ \n",
    "$\\newcommand{\\bb}{\\mathbf{b}}$ \n",
    "$\\newcommand{\\bc}{\\mathbf{c}}$ \n",
    "$\\newcommand{\\cE}{\\mathcal{E}}$ \n",
    "$\\newcommand{\\cP}{\\mathcal{P}}$\n",
    "$\\newcommand{\\bbeta}{\\boldsymbol{\\beta}}$\n",
    "$\\newcommand{\\bLambda}{\\boldsymbol{\\Lambda}}$\n",
    "$\\newcommand{\\cov}{\\mathrm{Cov}}$\n",
    "$\\newcommand{\\bfk}{\\mathbf{k}}$\n",
    "$\\newcommand{\\idx}[1]{}$\n",
    "$\\newcommand{\\xdi}{}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00106287",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Modeling more complex dependencies 2: marginalizing out an unobserved variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819d191d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**EXAMPLE:** **(Gaussian mixture model)** $\\idx{Gaussian mixture model}\\xdi$ For $i=1,\\ldots,K$, let $\\bmu_i$ and $\\bSigma_i$ be the mean and covariance matrix of a multivariate Gaussian. Let $\\bpi \\in \\Delta_K$. A Gaussian Mixture Model (GMM) is obtained as follows: take $Y \\sim \\mathrm{Cat}(\\bpi)$ and\n",
    "\n",
    "$$\n",
    "\\bX|\\{Y=i\\} \\sim N_d(\\bmu_i, \\bSigma_i).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6c60fd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Its probability density function (PDF) takes the form\n",
    "\n",
    "$$\n",
    "f_\\bX(\\bx)\n",
    "= \\sum_{i=1}^K \\pi_i \\frac{1}{(2\\pi)^{d/2} \\,|\\bSigma_i|^{1/2}}\n",
    "\\exp\\left(-\\frac{1}{2}(\\mathbf{x} - \\bmu_i)^T \\bSigma_i^{-1} (\\bx - \\bmu_i)\\right).\n",
    "$$\n",
    "\n",
    "$\\lhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0619e1d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We plot the density for means $\\bmu_1 = (-2,-2)$ and $\\bmu_2 = (2,2)$ and covariance matrices\n",
    "\n",
    "$$\n",
    "\\bSigma_1 = \\begin{bmatrix} 1.0 & 0 \\\\ 0 & 1.0 \\end{bmatrix}\n",
    "\\quad \\text{and} \\quad \n",
    "\\bSigma_2 = \\begin{bmatrix} \\sigma_1^2 & \\rho \\sigma_1 \\sigma_2 \\\\ \\rho \\sigma_1 \\sigma_2 & \\sigma_2^2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $\\sigma_1 = 1.5$, $\\sigma_2 = 0.5$ and $\\rho = -0.75$. The mixing weights are $\\pi_1 = 0.25$ and $\\pi_2 = 0.75$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dad819",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "def gmm2_pdf(X, Y, mean1, cov1, pi1, mean2, cov2, pi2):\n",
    "    xy = np.stack([X.flatten(), Y.flatten()], axis=-1)\n",
    "    Z1 = multivariate_normal.pdf(\n",
    "        xy, mean=mean1, cov=cov1).reshape(X.shape) \n",
    "    Z2 = multivariate_normal.pdf(\n",
    "        xy, mean=mean2, cov=cov2).reshape(X.shape) \n",
    "    return pi1 * Z1 + pi2 * Z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a2a378",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "start_point = 6\n",
    "stop_point = 6\n",
    "num_samples = 100\n",
    "points = np.linspace(-start_point, stop_point, num_samples)\n",
    "X, Y = np.meshgrid(points, points)\n",
    "\n",
    "mean1 = np.array([-2., -2.])\n",
    "cov1 = np.array([[1., 0.], [0., 1.]])\n",
    "pi1 = 0.5\n",
    "mean2 = np.array([2., 2.])\n",
    "cov2 = np.array([[1.5 ** 2., -0.75 * 1.5 * 0.5], \n",
    "                 [-0.75 * 1.5 * 0.5, 0.5 ** 2.]])\n",
    "pi2 = 0.5\n",
    "Z = gmm2_pdf(X, Y, mean1, cov1, pi1, mean2, cov2, pi2)\n",
    "mmids.make_surface_plot(X, Y, Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4997d1",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1384777d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "In Numpy, as we have seen before, the module [numpy.random](https://numpy.org/doc/stable/reference/random/index.html) also provides a way to sample from mixture models by using [numpy.random.Generator.choice](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.choice.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0d4bce",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "For instance, we consider mixtures of multivariate Gaussians. We chage the notation slightly to track Python's indexing. For $i=0,1$, we have a mean $\\bmu_i \\in \\mathbb{R}^d$ and a positive definite covariance matrix $\\bSigma_i \\in \\mathbb{R}^{d \\times d}$. We also have mixture weights $\\phi_0, \\phi_1 \\in (0,1)$ such that $\\phi_0 + \\phi_1 = 1$. Suppose we want to generate a total of $n$ samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6ba514",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "For each sample $j=1,\\ldots, n$, independently from everything else:\n",
    "\n",
    "1. We first pick a component $i \\in \\{0,1\\}$ at random according to the mixture weights, that is, $i=0$ is chosen with probability $\\phi_0$ and $i=1$ is chosen with probability $\\phi_1$.\n",
    "\n",
    "2. We generate a sample $\\bX_j = (X_{j,1},\\ldots,X_{j,d})$ according to a multivariate Gaussian with mean $\\bmu_i$ and covariance $\\bSigma_i$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e853384",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "This is straightforward to implement by using again [`numpy.random.Generator.choice`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.choice.html) to choose the component of each sample and [`numpy.random.Generator.multivariate_normal`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.multivariate_normal.html) to generate multivariate Gaussians. For convenience, we will stack the means and covariances into one array with a new dimension. So, for instance, the covariance matrices will now be in a 3d-array, that is, an array with $3$ indices. The first index corresponds to the component (here $0$ or $1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f942858",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The code is the following. It returns an `d` by `n` array `X`, where each row is a sample from a 2-component Gaussian mixture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d994002f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def gmm2(rng, d, n, phi0, phi1, mu0, sigma0, mu1, sigma1):\n",
    "    \n",
    "    phi = np.stack((phi0, phi1))\n",
    "    mu = np.stack((mu0, mu1))\n",
    "    sigma = np.stack((sigma0,sigma1))\n",
    "    \n",
    "    X = np.zeros((n,d))\n",
    "    component = rng.choice(2, size=n, p=phi)\n",
    "    for i in range(n):\n",
    "        X[i,:] = rng.multivariate_normal(\n",
    "            mu[component[i],:],\n",
    "            sigma[component[i],:,:])\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cbc054",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** Let us try it with following parameters. We first define the covariance matrices and show what happens when they are stacked into a 3d array (as is done within `gmm2`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b223ede1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "d = 2\n",
    "sigma0 = np.outer(np.array([2., 2.]), np.array([2., 2.])) \n",
    "sigma0 += np.outer(np.array([-0.5, 0.5]), np.array([-0.5, 0.5]))\n",
    "sigma1 = 2 * np.identity(d)\n",
    "sigma = np.stack((sigma0,sigma1))\n",
    "print(sigma[0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a43ae92",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(sigma[1,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3a6a51",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Then we define the rest of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e559a1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "seed = 535\n",
    "rng = np.random.default_rng(seed) \n",
    "\n",
    "n, w = 200, 5.\n",
    "phi0 = 0.8\n",
    "phi1 = 0.2\n",
    "mu0 = np.concatenate(([w], np.zeros(d-1)))\n",
    "mu1 = np.concatenate(([-w], np.zeros(d-1)))\n",
    "X = gmm2(rng, d, n, phi0, phi1, mu0, sigma0, mu1, sigma1)\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], s=5, marker='o', c='k')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1f4a4f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4fbbbd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "### Example: Mixtures of multivariate Bernoullis and the EM algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1486968e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We implement the EM algorithm for mixtures of multivariate Bernoullis. For this purpose, we adapt our previous Naive Bayes routines. We also allow for the possibility of using Laplace smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b516e631",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def responsibility(pi_k, p_km, x):\n",
    "   \n",
    "    K = len(pi_k)\n",
    "    score_k = np.zeros(K)\n",
    "    for k in range(K):\n",
    "       \n",
    "        score_k[k] -= np.log(pi_k[k])\n",
    "        score_k[k] -= np.sum(x * np.log(p_km[k,:]) \n",
    "                             + (1 - x) * np.log(1 - p_km[k,:]))\n",
    "    r_k = np.exp(-score_k)/(np.sum(np.exp(-score_k)))\n",
    "        \n",
    "    return r_k\n",
    "\n",
    "def update_parameters(eta_km, eta_k, eta, alpha, beta):\n",
    "\n",
    "    K = len(eta_k)\n",
    "    pi_k = (eta_k+alpha) / (eta+K*alpha)\n",
    "    p_km = (eta_km+beta) / (eta_k[:,None]+2*beta)\n",
    "\n",
    "    return pi_k, p_km"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396d0003",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We implement the E and M Step next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4ac561",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def em_bern(X, K, pi_0, p_0, maxiters = 10, alpha=0., beta=0.):\n",
    "    \n",
    "    n, M = X.shape\n",
    "    pi_k = pi_0\n",
    "    p_km = p_0\n",
    "        \n",
    "    for _ in range(maxiters):\n",
    "    \n",
    "        # E Step\n",
    "        r_ki = np.zeros((K,n))\n",
    "        for i in range(n):\n",
    "            r_ki[:,i] = responsibility(pi_k, p_km, X[i,:])\n",
    "        \n",
    "        # M Step     \n",
    "        eta_km = np.zeros((K,M))\n",
    "        eta_k = np.sum(r_ki, axis=-1)\n",
    "        eta = np.sum(eta_k)\n",
    "        for k in range(K):\n",
    "            for m in range(M):\n",
    "                eta_km[k,m] = np.sum(X[:,m] * r_ki[k,:]) \n",
    "        pi_k, p_km = update_parameters(\n",
    "            eta_km, eta_k, eta, alpha, beta)\n",
    "        \n",
    "    return pi_k, p_km   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5a16ab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We test the algorithm on a very simple dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b3a2fb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([[1., 1., 1.],[1., 1., 1.],[1., 1., 1.],[1., 0., 1.],\n",
    "              [0., 1., 1.],[0., 0., 0.],[0., 0., 0.],[0., 0., 1.]])\n",
    "n, M = X.shape\n",
    "K = 2\n",
    "pi_0 = np.ones(K)/K\n",
    "p_0 = rng.random((K,M))\n",
    "\n",
    "pi_k, p_km = em_bern(\n",
    "    X, K, pi_0, p_0, maxiters=100, alpha=0.01, beta=0.01)\n",
    "\n",
    "print(pi_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dba29eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(p_km)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20829b8a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We compute the probability that the vector $(0, 0, 1)$ is in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b75d24",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x_test = np.array([0., 0., 1.])\n",
    "print(responsibility(pi_k, p_km, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f768a22",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**CHAT & LEARN** The EM algorithm can sometimes get stuck in local optima. Ask your favorite AI chatbot to discuss strategies for initializing the EM algorithm to avoid this issue, such as using multiple random restarts or using the k-means algorithm for initialization. ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb)) $\\ddagger$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fe7e59",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e718fa0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To give a more involved example, we use the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1de2ea",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Quoting [Wikipedia](https://en.wikipedia.org/wiki/MNIST_database) again:\n",
    "\n",
    "> The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning. It was created by \"re-mixing\" the samples from NIST's original datasets. The creators felt that since NIST's training dataset was taken from American Census Bureau employees, while the testing dataset was taken from American high school students, it was not well-suited for machine learning experiments. Furthermore, the black and white images from NIST were normalized to fit into a 28x28 pixel bounding box and anti-aliased, which introduced grayscale levels. The MNIST database contains 60,000 training images and 10,000 testing images. Half of the training set and half of the test set were taken from NIST's training dataset, while the other half of the training set and the other half of the test set were taken from NIST's testing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a405fc8c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We load it from PyTorch. The data can be accessed with [`torchvision.datasets.MNIST`](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html). The [`squeeze()`](https://pytorch.org/docs/stable/generated/torch.Tensor.squeeze.html) below removes the color dimension in the image, which is grayscale. The [`numpy()`](https://pytorch.org/docs/stable/generated/torch.Tensor.numpy.html) converts the PyTorch tensors into Numpy arrays. See [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for details on the data loading. We will say more about PyTorch in a later chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf959e9e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "mnist = datasets.MNIST(root='./data', train=True, \n",
    "                       download=True, transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(mnist, batch_size=len(mnist), shuffle=False)\n",
    "\n",
    "imgs, labels = next(iter(train_loader))\n",
    "imgs = imgs.squeeze().numpy()\n",
    "labels = labels.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de511d25",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We turn the grayscale images (more precisely, each pixel is an integer between $0$ and $255$) into a black-and-white images by rounding the pixels (after dividing by $255$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132d04a9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "imgs = np.round(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dda6cf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "There are two common ways to write a $2$. Let's see if a mixture of multivariate Bernoullis can find them. We extract the images labelled $2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5e8c37",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mask = labels == 2\n",
    "imgs2 = imgs[mask]\n",
    "labels2 = labels[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabef4e1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The first image is the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42398929",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.imshow(imgs2[0], cmap='gray_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2606c649",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Next, we transform the images into vectors and convert into black and white by rounding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e188f37",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = imgs2.reshape(len(imgs2), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9bd0ab",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We run the algorithm with $2$ clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bcc2b4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "n, M = X.shape\n",
    "K = 2\n",
    "pi_0 = np.ones(K)/K\n",
    "p_0 = rng.random((K,M))\n",
    "\n",
    "pi_k, p_km = em_bern(\n",
    "    X, K, pi_0, p_0, maxiters=10, alpha=1., beta=1.)\n",
    "\n",
    "print(pi_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb3d393",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Uh-oh. Something went wrong. We encountered a numerical issue, underflow, which we discussed briefly previously. To confirm this, we run the code again but ask Python to warn us about it using [`numpy.seterr`](https://numpy.org/doc/stable/reference/generated/numpy.seterr.html). (By default, warnings are turned off in the book, but they can be reactivated using [`warnings.resetwarnings`](https://docs.python.org/3/library/warnings.html#warnings.resetwarnings).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a4b04b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "warnings.resetwarnings()\n",
    "old_settings = np.seterr(all='warn')\n",
    "\n",
    "pi_k, p_km = em_bern(\n",
    "    X, K, pi_0, p_0, maxiters=10, alpha=1., beta=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d263487",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52664fd4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "When we compute the responsibilities \n",
    "\n",
    "$$\n",
    "r_{k,i}^t\n",
    "= \\frac{\\pi_k^t \\prod_{m=1}^M (p_{k,m}^t)^{x_{i,m}} (1-p_{k,m}^t)^{1-x_{i,m}}}\n",
    "{\\sum_{k'=1}^K \\pi_{k'}^t \\prod_{m=1}^M (p_{k',m}^t)^{x_{i,m}} (1-p_{k',m}^t)^{1-x_{i,m}}},\n",
    "$$\n",
    "\n",
    "we first compute the negative logarithm of each term in the numerator as we did in the Naive Bayes case. But then we applied the function $e^{-x}$, because this time we are not simply computing an optimal score. When all scores are high, this last step may result in underflow, that is, produces numbers so small that they get rounded down to zero by Numpy. Then the ratio defining `r_k` is not well-defined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac34843",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To deal with this, we introduce a technique called the log-sum-exp trick$\\idx{log-sum-exp trick}\\xdi$ (with some help from ChatGPT). Consider the computation of a function of $\\mathbf{a} = (a_1, \\ldots, a_n)$ of the form\n",
    "\n",
    "$$\n",
    "h(\\mathbf{a}) = \\log \\left( \\sum_{i=1}^{n} e^{-a_i} \\right).\n",
    "$$\n",
    "\n",
    "When the $a_i$ values are large positive numbers, the terms $e^{-a_i}$ can be so small that they underflow to zero. To counter this, the log-sum-exp trick involves a shift to bring these terms into a more favorable numerical range. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef9fcee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "It proceeds as follows: \n",
    "\n",
    "1. Identify the minimum value $M$ among the $a_i$s\n",
    "   \n",
    "   $$\n",
    "   M = \\min\\{a_1, a_2, \\ldots, a_n\\}.\n",
    "   $$\n",
    "\n",
    "2. Subtract $M$ from each $a_i$ before exponentiation\n",
    "\n",
    "   $$\n",
    "   \\log \\left( \\sum_{i=1}^{n} e^{-a_i} \\right) \n",
    "   = \\log \\left( e^{-M} \\sum_{i=1}^{n} e^{- (a_i - M)} \\right).\n",
    "   $$\n",
    "\n",
    "3. Rewrite using log properties\n",
    "\n",
    "   $$\n",
    "   = -M + \\log \\left( \\sum_{i=1}^{n} e^{-(a_i - M)} \\right).\n",
    "   $$\n",
    "\n",
    "Why does this help with underflow? By subtracting $M$, the smallest value in the set, from each $a_i$:\n",
    "(i) the largest term in $\\{e^{-(a_i - M)} : = 1,\\ldots,n\\}$ becomes $e^0 = 1$; and (ii)\n",
    "all other terms are between 0 and 1, as they are exponentiations of negative numbers or zero. This manipulation avoids terms underflowing to zero because even very large values, when shifted by $M$, are less likely to hit the underflow threshold. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402a11c7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Here is an example. Imagine you have $\\mathbf{a} = (-1000, -1001, -1002)$. \n",
    "\n",
    "- Direct computation: $e^{-1000}$, $e^{-1001}$, and $e^{-1002}$ might all underflow to zero.\n",
    "\n",
    "- With the log-sum-exp trick: Subtract $M = 1000$, leading to $e^{0}$, $e^{-1}$, and $e^{-2}$, all meaningful, non-zero results that accurately contribute to the sum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16013f5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We implement in Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd632e6f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def log_sum_exp_trick(a):\n",
    "    min_val = np.min(a)\n",
    "    return - min_val + np.log(np.sum(np.exp(- a + min_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946464e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We try it on a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffcf840",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "a = np.array([1000, 1001, 1002])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f056441",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We first attempt a direct computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052c40b1",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "np.log(np.sum(np.exp(-a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a3be07",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Predictly, we get an underflow error and a useless output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2703625",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Next, we try the log-sum-exp trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bacab9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "log_sum_exp_trick(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef181f6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "This time we get an output which seems reasonable, something slightly larger than $-1000$ as expected (Why?)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4a835b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5b9535",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "After this long -- but important! -- parenthesis, we return to the EM algorithm. We modify it by implementing the log-sum-exp trick in the subroutine `responsibility`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb76f8e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def responsibility(pi_k, p_km, x):\n",
    "   \n",
    "    K = len(pi_k)\n",
    "    score_k = np.zeros(K)\n",
    "    for k in range(K):\n",
    "       \n",
    "        score_k[k] -= np.log(pi_k[k])\n",
    "        score_k[k] -= np.sum(x * np.log(p_km[k,:]) \n",
    "                             + (1 - x) * np.log(1 - p_km[k,:]))\n",
    "    r_k = np.exp(-score_k - log_sum_exp_trick(score_k))\n",
    "            \n",
    "    return r_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caf783b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We go back to the MNIST example with only the 2s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b59fbd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pi_k, p_km = em_bern(X, K, pi_0, p_0, maxiters=10, alpha=1., beta=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f9bac0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Here is one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984c9c38",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(p_km[0,:].reshape((28,28)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acab9f81",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Here is the other one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e99c35",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(p_km[1,:].reshape((28,28)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d0ee5e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Now that the model is trained, we compute the probability that an example image is in each cluster. We use the first image in the dataset that we plotted earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb4937b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "responsibility(pi_k, p_km, X[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9c2448",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "It indeed identifies the second cluster as significantly more likely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8b4a8e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**TRY IT!** In the MNIST example, as we have seen, the probabilities involved are extremely small and the responsibilities are close to $0$ or $1$. Implement a variant of EM, called hard EM, which replaces responsibilities with the one-hot encoding of the largest responsibility. Test it on the MNIST example again. ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c6f57d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9ad0e4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\newcommand{\\bmu}{\\boldsymbol{\\mu}}$ \n",
    "$\\newcommand{\\bSigma}{\\boldsymbol{\\Sigma}}$\n",
    "$\\newcommand{\\bfbeta}{\\boldsymbol{\\beta}}$ \n",
    "$\\newcommand{\\bflambda}{\\boldsymbol{\\lambda}}$\n",
    "$\\newcommand{\\bgamma}{\\boldsymbol{\\gamma}}$ \n",
    "$\\newcommand{\\bsigma}{{\\boldsymbol{\\sigma}}}$\n",
    "$\\newcommand{\\bpi}{\\boldsymbol{\\pi}}$\n",
    "$\\newcommand{\\btheta}{{\\boldsymbol{\\theta}}}$ \n",
    "$\\newcommand{\\bphi}{\\boldsymbol{\\phi}}$ \n",
    "$\\newcommand{\\balpha}{\\boldsymbol{\\alpha}}$\n",
    "$\\newcommand{\\blambda}{\\boldsymbol{\\lambda}}$\n",
    "$\\renewcommand{\\P}{\\mathbb{P}}$\n",
    "$\\newcommand{\\E}{\\mathbb{E}}$\n",
    "$\\newcommand{\\indep}{\\perp\\!\\!\\!\\perp} \\newcommand{\\bx}{\\mathbf{x}}$ \n",
    "$\\newcommand{\\bp}{\\mathbf{p}}$\n",
    "$\\renewcommand{\\bx}{\\mathbf{x}}$\n",
    "$\\newcommand{\\bX}{\\mathbf{X}}$ \n",
    "$\\newcommand{\\by}{\\mathbf{y}}$ \n",
    "$\\newcommand{\\bY}{\\mathbf{Y}}$ \n",
    "$\\newcommand{\\bz}{\\mathbf{z}}$\n",
    "$\\newcommand{\\bZ}{\\mathbf{Z}}$ \n",
    "$\\newcommand{\\bw}{\\mathbf{w}}$ \n",
    "$\\newcommand{\\bW}{\\mathbf{W}}$\n",
    "$\\newcommand{\\bv}{\\mathbf{v}}$ \n",
    "$\\newcommand{\\bV}{\\mathbf{V}}$\n",
    "$\\newcommand{\\bfg}{\\mathbf{g}}$ \n",
    "$\\newcommand{\\bfh}{\\mathbf{h}}$ \n",
    "$\\newcommand{\\horz}{\\rule[.5ex]{2.5ex}{0.5pt}}$\n",
    "$\\renewcommand{\\S}{\\mathcal{S}}$ \n",
    "$\\newcommand{\\X}{\\mathcal{X}}$ \n",
    "$\\newcommand{\\var}{\\mathrm{Var}}$ \n",
    "$\\newcommand{\\pa}{\\mathrm{pa}}$\n",
    "$\\newcommand{\\Z}{\\mathcal{Z}}$ \n",
    "$\\newcommand{\\bh}{\\mathbf{h}}$ \n",
    "$\\newcommand{\\bb}{\\mathbf{b}}$ \n",
    "$\\newcommand{\\bc}{\\mathbf{c}}$ \n",
    "$\\newcommand{\\cE}{\\mathcal{E}}$ \n",
    "$\\newcommand{\\cP}{\\mathcal{P}}$\n",
    "$\\newcommand{\\bbeta}{\\boldsymbol{\\beta}}$\n",
    "$\\newcommand{\\bLambda}{\\boldsymbol{\\Lambda}}$\n",
    "$\\newcommand{\\cov}{\\mathrm{Cov}}$\n",
    "$\\newcommand{\\bfk}{\\mathbf{k}}$\n",
    "$\\newcommand{\\idx}[1]{}$\n",
    "$\\newcommand{\\xdi}{}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a156db44",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Application: linear-Gaussian models and Kalman filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08be4d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We apply Kalman filtering to location tracking. Returning to our cyborg corgi example, we imagine that we get noisy observations about its successive positions in a park. (Think of GPS measurements.) We seek to get a better estimate of its location using the method above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f3d0ac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We model the true location as a linear-Gaussian system over the 2d position $(z_{1,t}, z_{2,t})_t$ and velocity $(\\dot{z}_{1,t}, \\dot{z}_{2,t})_t$ sampled at $\\Delta$ intervals of time. Formally,\n",
    "\n",
    "$$\n",
    "\\bX_t = (z_{1,t}, z_{2,t}, \\dot{z}_{1,t}, \\dot{z}_{2,t}),\n",
    "\\quad\n",
    "F = \\begin{pmatrix}\n",
    "1 & 0 & \\Delta & 0\\\\\n",
    "0 & 1 & 0 & \\Delta\\\\\n",
    "0 & 0 & 1 & 0\\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "\n",
    "so the unobserved dynamics are\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "z_{1,t+1}\\\\ \n",
    "z_{2,t+1}\\\\ \n",
    "\\dot{z}_{1,t+1}\\\\ \n",
    "\\dot{z}_{2,t+1}\n",
    "\\end{pmatrix}\n",
    "= \\bX_{t+1}\n",
    "= F \\,\\bX_t + \\bW_t\n",
    "= \n",
    "\\begin{pmatrix}\n",
    "z_{1,t} + \\Delta \\dot{z}_{1,t} + W_{1,t}\\\\\n",
    "z_{2,t} + \\Delta \\dot{z}_{2,t} + W_{2,t}\\\\\n",
    "\\dot{z}_{1,t} + \\dot{W}_{1,t}\\\\\n",
    "\\dot{z}_{2,t} + \\dot{W}_{2,t}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "where the $\\bW_t = (W_{1,t}, W_{2,t}, \\dot{W}_{1,t}, \\dot{W}_{2,t}) \\sim N_{d_0}(\\mathbf{0}, Q)$ with $Q$ known.\n",
    "\n",
    "In words, the velocity is unchanged, up to Gaussian perturbation. The position changes proportionally to the velocity in the corresponding dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc43cfb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The observations $(\\tilde{z}_{1,t}, \\tilde{z}_{2,t})_t$ are modeled as\n",
    "\n",
    "$$\n",
    "\\bY_t = (\\tilde{z}_{1,t}, \\tilde{z}_{2,t}),\n",
    "\\quad \n",
    "H = \\begin{pmatrix}\n",
    "1 & 0 & 0 & 0\\\\\n",
    "0 & 1 & 0 & 0\\\\\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "so the observed process satisfies\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\tilde{z}_{1,t}\\\\ \n",
    "\\tilde{z}_{2,t}\n",
    "\\end{pmatrix}\n",
    "= \\bY_t\n",
    "= H\\,\\bX_t + \\bV_t\n",
    "= \n",
    "\\begin{pmatrix}\n",
    "\\tilde{z}_{1,t} + \\tilde{V}_{1,t}\\\\ \n",
    "\\tilde{z}_{2,t} + \\tilde{V}_{2,t}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "where the $\\bV_t = (\\tilde{V}_{1,t}, \\tilde{V}_{2,t}) \\sim N_d(\\mathbf{0}, R)$ with $R$ known. \n",
    "\n",
    "In words, we only observe the positions, up to Gaussian noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135c1330",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**Implementing the Kalman filter** We implement the Kalman filter as described above with known covariance matrices. We take $\\Delta = 1$ for simplicity. The code is adapted from [[Mur](https://github.com/probml)]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed377903",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We will test Kalman filtering on a simulated path drawn from the linear-Gaussian model above. The following function creates such a path and its noisy observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe3ba00",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def lgSamplePath(rng, ss, os, F, H, Q, R, init_mu, init_Sig, T):\n",
    "    x = np.zeros((ss,T)) \n",
    "    y = np.zeros((os,T))\n",
    "\n",
    "    x[:,0] = rng.multivariate_normal(init_mu, init_Sig)\n",
    "    for t in range(1,T):\n",
    "        x[:,t] = rng.multivariate_normal(F @ x[:,t-1],Q)\n",
    "        y[:,t] = rng.multivariate_normal(H @ x[:,t],R)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9cf5ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** Here is an example. In the plots, the red dots are the noisy observations and the green crosses are the unobserved true path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050a0b07",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "seed = 535\n",
    "rng = np.random.default_rng(seed)\n",
    "ss = 4 # state size\n",
    "os = 2 # observation size\n",
    "F = np.array([[1., 0., 1., 0.],\n",
    "              [0., 1., 0., 1.],\n",
    "              [0., 0., 1., 0.],\n",
    "              [0., 0., 0., 1.]]) \n",
    "H = np.array([[1., 0., 0., 0.],\n",
    "              [0., 1, 0., 0.]])\n",
    "Q = 0.1 * np.diag(np.ones(ss))\n",
    "R = 10 * np.diag(np.ones(os))\n",
    "init_mu = np.array([0., 0., 1., 1.])\n",
    "init_Sig = 1 * np.diag(np.ones(ss))\n",
    "T = 50\n",
    "x, y = lgSamplePath(rng, ss, os, F, H, Q, R, init_mu, init_Sig, T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dbbfde",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "In the next plot (and throughout this section), the dots are the noisy observations. The unobserved true path is also shown as a dotted line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30b5be0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.scatter(y[0,:], y[1,:], s=5, c='r', alpha=0.5)\n",
    "plt.plot(x[0,:], x[1,:], c='g', linestyle='dotted')\n",
    "plt.xlim((np.min(y[0,:])-5, np.max(y[0,:])+5)) \n",
    "plt.ylim((np.min(y[1,:])-5, np.max(y[1,:])+5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906bbd3c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb65af63",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The following function implements the Kalman filter. The full recursion is broken up into several steps. We use [`numpy.linalg.inv`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.inv.html) to compute the Kalman gain matrix. Below, `mu_pred` is $F \\bmu_{t-1}$ and `Sig_pred` is $P_{t-1} = F \\bSigma_{t-1} F^T + Q$, which are the mean vector and covariance matrix of $\\bX_{t}$ given $\\bY_{1:t-1}$ as computed in the *Predict* step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dfdd6e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def kalmanUpdate(ss, F, H, Q, R, y_t, mu_prev, Sig_prev):\n",
    "    \n",
    "    mu_pred = F @ mu_prev\n",
    "    Sig_pred = F @ Sig_prev @ F.T + Q\n",
    "    \n",
    "    e_t = y_t - H @ mu_pred\n",
    "    S = H @ Sig_pred @ H.T + R\n",
    "    Sinv = LA.inv(S)\n",
    "    K = Sig_pred @ H.T @ Sinv\n",
    "    \n",
    "    mu_new = mu_pred + K @ e_t\n",
    "    Sig_new = (np.diag(np.ones(ss)) - K @ H) @ Sig_pred\n",
    "    \n",
    "    return mu_new, Sig_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c4b3b1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def kalmanFilter(ss, os, y, F, H, Q, R, init_mu, init_Sig, T):\n",
    "    \n",
    "    mu = np.zeros((ss, T))\n",
    "    Sig = np.zeros((ss, ss, T))\n",
    "    mu[:,0] = init_mu\n",
    "    Sig[:,:,0] = init_Sig\n",
    "    for t in range(1,T):\n",
    "        mu[:,t], Sig[:,:,t] = kalmanUpdate(ss, F, H, Q, R, y[:,t], mu[:,t-1], Sig[:,:,t-1])\n",
    "\n",
    "    return mu, Sig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0507c93",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We apply this to the location tracking example. The inferred states, or more precisely their estimated mean, are in blue. Note that we also inferred the velocity at each time point, but we are not plotting that information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8123367b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "init_mu = np.array([0., 0., 1., 1.])\n",
    "init_Sig = 1 * np.diag(np.ones(ss))\n",
    "mu, Sig = kalmanFilter(ss, os, y, F, H, Q, R, init_mu, init_Sig, T)\n",
    "plt.plot(mu[0,:], mu[1,:], c='b', marker='s', markersize=2, linewidth=1)\n",
    "plt.scatter(y[0,:], y[1,:], s=5, c='r', alpha=0.5)\n",
    "plt.plot(x[0,:], x[1,:], c='g', linestyle='dotted', alpha=0.5)\n",
    "plt.xlim((np.min(y[0,:])-5, np.max(y[0,:])+5)) \n",
    "plt.ylim((np.min(y[1,:])-5, np.max(y[1,:])+5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483135dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To quantify the improvement in the inferred means compared to the observations, we compute the mean squared error in both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a913fb4d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dobs = x[0:1,:] - y[0:1,:]\n",
    "mse_obs = np.sqrt(np.sum(dobs**2))\n",
    "print(mse_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f1b17c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dfilt = x[0:1,:] - mu[0:1,:]\n",
    "mse_filt = np.sqrt(np.sum(dfilt**2))\n",
    "print(mse_filt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9633bae3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We indeed observe a substantial reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5757581c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
