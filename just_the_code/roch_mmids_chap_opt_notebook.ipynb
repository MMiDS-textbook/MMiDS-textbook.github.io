{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a57e3201",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell",
     "colab-keep"
    ]
   },
   "source": [
    "***\n",
    "\n",
    "*Course:* [Math 535](https://people.math.wisc.edu/~roch/mmids/) - Mathematical Methods in Data Science (MMiDS)  \n",
    "*Chapter:* 6-Optimization theory and algorithms  \n",
    "*Author:* [Sebastien Roch](https://people.math.wisc.edu/~roch/), Department of Mathematics, University of Wisconsin-Madison  \n",
    "*Updated:* May 26, 2024   \n",
    "*Copyright:* &copy; 2024 Sebastien Roch\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c18545",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# You will need the files:\n",
    "#     * mmids.py\n",
    "#     * customer_airline_satisfaction.csv\n",
    "#     * advertising.csv \n",
    "#     * SAHeart.csv \n",
    "# from https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/tree/main/utils\n",
    "#\n",
    "# IF RUNNING ON GOOGLE COLAB (RECOMMENDED):\n",
    "# \"Upload to session storage\" from the Files tab on the left\n",
    "# Alternative instructions: https://colab.research.google.com/notebooks/io.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3235c3b9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# PYTHON 3\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch\n",
    "import mmids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58136842",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Motivating example:  analyzing customer satisfaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec29864",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**Figure:** Helpful map of ML by scitkit-learn ([Source](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html))\n",
    "\n",
    "![ml-cheat-sheet](https://scikit-learn.org/1.4/_static/ml_map.png)\n",
    "\n",
    "$\\bowtie$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db19253c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We now turn to classification.\n",
    "\n",
    "Quoting [Wikipedia](https://en.wikipedia.org/wiki/Statistical_classification):\n",
    "\n",
    "> In machine learning and statistics, classification is the problem of identifying to which of a set of categories (sub-populations) a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known. Examples are assigning a given email to the \"spam\" or \"non-spam\" class, and assigning a diagnosis to a given patient based on observed characteristics of the patient (sex, blood pressure, presence or absence of certain symptoms, etc.). Classification is an example of pattern recognition. In the terminology of machine learning, classification is considered an instance of supervised learning, i.e., learning where a training set of correctly identified observations is available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf46c4dc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We will illustrate this problem on an [airline customer satisfaction](https://www.kaggle.com/datasets/sjleshrac/airlines-customer-satisfaction) dataset available on [Kaggle](https://www.kaggle.com), an excellent source of data and community contributed analyses. The background is the following: \n",
    "\n",
    "> The dataset consists of the details of customers who have already flown with them. The feedback of the customers on various context and their flight data has been consolidated. The main purpose of this dataset is to predict whether a future customer would be satisfied with their service given the details of the other parameters values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c13078d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We first load the data and convert it to an appropriate matrix representation. We (or, more precisely, ChatGPT) pre-processed the original file to removing rows with missing data or 0 ratings, convert categorical variables into one-hot encodings, and keep only a subset of the rows and columns. You can see the details of the pre-processing in this [chat history](https://chatgpt.com/share/c5070b9c-f33f-4a37-a793-fde0d7cb7b06)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79db5d54",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('customer_airline_satisfaction.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17520218",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "It has 24 columns and the number of rows is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933e8022",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "len(data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d54e89d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The column names are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2d7d73",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(data.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588a09dc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The first column indicates whether a customer was satisfied (with `1` meaning satisfied). The next 6 columns give some information about the customers, e.g., their age or whether they are members of a loyalty program with the airline. The following three columns give information about the flight, with names that should be self-explanatory: `Flight Distance`, `Departure Delay in Minutes`, and `Arrival Delay in Minutes`. The remaining columns give the customers' ratings, between `1` and `5`, of various feature, e.g., `Baggage handling`, `Checkin service`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb75082",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Our goal will be to predict the first column, `Satisfied`, from the rest of the columns. For this, we transform our data into Numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8135846",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y = data['Satisfied'].to_numpy()\n",
    "X = data.drop(columns=['Satisfied']).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76a7b6f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "print('y=',y)\n",
    "print('X=',X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13146a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Some features may affect satisfication more than others. Let us look at age for instance. The following code extracts the `Age` column from `X` (i.e., column $0$) and computes the fraction of satisfied customers in several age bins.\n",
    "\n",
    "Explanation by ChatGPT (who wrote the code):\n",
    "\n",
    "1. [`numpy.digitize`](https://numpy.org/doc/stable/reference/generated/numpy.digitize.html) bins the age data into the specified age bins. The `-1` adjustment is to match zero-based indexing.\n",
    "2. [`numpy.bincount`](https://numpy.org/doc/stable/reference/generated/numpy.bincount.html) counts the occurrences of each bin index. The `minlength` parameter ensures that the resulting array length matches the number of age bins (`age_labels`). This is important if some bins have zero counts, ensuring the counts array covers all bins.\n",
    "3. `freq_satisfied = counts_satisfied / counts_all` calculates the satisfaction frequency for each age group by dividing the counts of satisfied customers by the total counts in each age group.\n",
    "4. The results are plotted using matplotlib's [`matplotlib.pyplot.bar`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.bar.html) function.\n",
    "\n",
    "We see in particular that younger people tend to be more dissatisfied. Of course, this might be because they cannot afford the most expensive services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0f227e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the 'Age' column (index 0 in the array)\n",
    "age_col_index = 0\n",
    "age_data = X[:, age_col_index]\n",
    "\n",
    "# Define the age bins and labels\n",
    "age_bins = [0, 18, 25, 35, 45, 55, 65, 100]\n",
    "age_labels = ['0-17', '18-24', '25-34', '35-44', '45-54', '55-64', '65+']\n",
    "\n",
    "# Use np.digitize to bin the age data\n",
    "age_bin_indices = np.digitize(age_data, bins=age_bins) - 1\n",
    "\n",
    "# Use np.bincount to count occurrences\n",
    "counts_all = np.bincount(age_bin_indices, minlength=len(age_labels))\n",
    "counts_satisfied = np.bincount(age_bin_indices[y == 1], minlength=len(age_labels))\n",
    "\n",
    "# Calculate the frequencies\n",
    "freq_satisfied = counts_satisfied / counts_all\n",
    "\n",
    "# Prepare data for plotting\n",
    "age_group_labels = np.array(age_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4f10ce",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(age_group_labels, freq_satisfied, color='blue', alpha=0.7)\n",
    "plt.title('Frequency of Satisfaction by Age Group')\n",
    "plt.xlabel('Age Group')\n",
    "plt.ylabel('Frequency of Satisfied Customers')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3916fd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The input data is now of the form $\\{(\\mathbf{x}_i, y_i) : i=1,\\ldots, n\\}$ where $\\mathbf{x}_i \\in \\mathbb{R}^d$ are the features and $y_i \\in \\{0,1\\}$ is the label. Above we use the matrix representation $X \\in \\mathbb{R}^{d \\times n}$ with columns $\\mathbf{x}_i$, $i = 1,\\ldots, n$ and $\\mathbf{y} = (y_1, \\ldots, y_n)^T \\in \\{0,1\\}^n$. \n",
    "\n",
    "Our goal: \n",
    "\n",
    "> learn a classifier from the examples $\\{(\\mathbf{x}_i, y_i) : i=1,\\ldots, n\\}$, that is, a function $\\hat{f} : \\mathbb{R}^d \\to \\mathbb{R}$ such that $\\hat{f}(\\mathbf{x}_i) \\approx y_i$.\n",
    "\n",
    "We may want to enforce that the output is in $\\{0,1\\}$ as well. This problem is referred to as [binary classification](https://en.wikipedia.org/wiki/Binary_classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18e4f4e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "A natural approach to this type of [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning) problem is to define two objects:\n",
    "\n",
    "1. **Family of classifiers:** A class $\\widehat{\\mathcal{F}}$ of classifiers from which to pick $\\hat{f}$.\n",
    "\n",
    "2. **Loss function:** A loss function $\\ell(\\hat{f}, (\\mathbf{x},y))$ which quantifies how good of a fit $\\hat{f}(\\mathbf{x})$ is to $y$.\n",
    "\n",
    "Our goal is then to solve\n",
    "\n",
    "$$\n",
    "\\min_{\\hat{f} \\in \\widehat{\\mathcal{F}}} \\frac{1}{n} \\sum_{i=1}^n \\ell(\\hat{f}, (\\mathbf{x}_i, y_i)),\n",
    "$$\n",
    "\n",
    "that is, we seek to find a classifier among $\\widehat{\\mathcal{F}}$ that minimizes the average loss over the examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03257a66",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "For instance, in [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression), we consider linear classifiers of the form\n",
    "\n",
    "$$\n",
    "\\hat{f}(\\mathbf{x})\n",
    "= \\sigma(\\mathbf{x}^T \\boldsymbol{\\theta})\n",
    "\\qquad\n",
    "\\text{with}\n",
    "\\qquad\n",
    "\\sigma(t) = \\frac{1}{1 + e^{-t}}\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\theta} \\in \\mathbb{R}^d$ is a parameter vector. And we use the [cross-entropy loss](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression)\n",
    "\n",
    "$$\n",
    "\\ell(\\hat{f}, (\\mathbf{x}, y))\n",
    "= -  y \\log(\\sigma(\\mathbf{x}^T \\boldsymbol{\\theta}))\n",
    "- (1-y) \\log(1- \\sigma(\\mathbf{x}^T \\boldsymbol{\\theta})).\n",
    "$$\n",
    "\n",
    "In parametric form, the problem boils down to\n",
    "\n",
    "$$\n",
    "\\min_{\\boldsymbol{\\theta} \\in \\mathbb{R}^d}\n",
    "- \\frac{1}{n} \\sum_{i=1}^n y_i \\log(\\sigma(\\mathbf{x}_i^T \\boldsymbol{\\theta}))\n",
    "- \\frac{1}{n} \\sum_{i=1}^n (1-y_i) \\log(1- \\sigma(\\mathbf{x}_i^T \\boldsymbol{\\theta})).\n",
    "$$\n",
    "\n",
    "To obtain a prediction in $\\{0,1\\}$ here, we could cutoff $\\hat{f}(\\mathbf{x})$ at a threshold $\\tau \\in [0,1]$, that is, return $\\mathbf{1}\\{\\hat{f}(\\mathbf{x}) > \\tau\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9231a063",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We will explain in a later chapter where this choice comes from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ac15ff",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The purpose of this chapter is to develop some of the mathematical theory and algorithms needed to solve this type of optimization formulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aac81b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Background: review of differentiable functions of several variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a1cb40",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\newcommand{\\bSigma}{\\boldsymbol{\\Sigma}}$ $\\newcommand{\\bmu}{\\boldsymbol{\\mu}}$ $\\newcommand{\\blambda}{\\boldsymbol{\\lambda}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c53846c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Optimality conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d68f9f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**EXAMPLE:** Consider $f(x) = e^x$. Then $f'(x) = f''(x) = e^x$. Suppose we are interested in approximating $f$ in the interval $[0,1]$. We take $a=0$ and $b=1$ in *Taylor's Theorem*. The linear term is \n",
    "\n",
    "$$\n",
    "f(a) + (x-a) f'(a) = 1 + x e^0 = 1 + x.\n",
    "$$\n",
    "\n",
    "Then for any $x \\in [0,1]$\n",
    "\n",
    "$$\n",
    "f(x) = 1 + x + \\frac{1}{2}x^2 e^{\\xi_x}\n",
    "$$\n",
    "\n",
    "where $\\xi_x \\in (0,1)$ depends on $x$. We get a uniform bound on the error over $[0,1]$ by replacing $\\xi_x$ with its worst possible value over $[0,1]$ \n",
    "\n",
    "$$\n",
    "|f(x) - (1+x)| \\leq \\frac{1}{2}x^2 e^{\\xi_x} \\leq \\frac{e}{2} x^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350ec5f8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0,1,100)\n",
    "y = np.exp(x)\n",
    "taylor = 1 + x\n",
    "err = (np.exp(1)/2) * x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142b4a57",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.plot(x,y,label='f')\n",
    "plt.plot(x,taylor,label='taylor')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da45dd74",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "If we plot the upper and lower bounds, we see that $f$ indeed falls within them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5974fc56",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.plot(x,y,label='f')\n",
    "plt.plot(x,taylor,label='taylor')\n",
    "plt.plot(x,taylor-err,linestyle=':',color='green',label='lower')\n",
    "plt.plot(x,taylor+err,linestyle='--',color='green',label='upper')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f60551",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\lhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbf2726",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**EXAMPLE:** Let $f(x) = x^3$. Then $f'(x) = 3 x^2$ and $f''(x) = 6 x$ so that $f'(0) = 0$ and $f''(0) \\geq 0$. Hence $x=0$ is a stationary point. But $x=0$ is not a local minimizer. Indeed $f(0) = 0$ but, for any $\\delta > 0$, $f(-\\delta) < 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69d9c52",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-2,2,100)\n",
    "y = x**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5ede91",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.plot(x,y)\n",
    "plt.ylim(-5,5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc78e249",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\lhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e549a623",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**EXAMPLE:** If we want to minimize $2 x_1^2 + 3 x_2^2$ over all two-dimensional unit vectors $\\mathbf{x} = (x_1, x_2)$, then we can let\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = 2 x_1^2 + 3 x_2^2\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "h_1(\\mathbf{x}) = 1 - x_1^2 - x_2^2 = 1 - \\|\\mathbf{x}\\|^2.\n",
    "$$\n",
    "\n",
    "Observe that we could have chosen a different equality constraint to express the same minimization problem. $\\lhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdaedec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**EXAMPLE:** **(continued)** Returning to the previous example,\n",
    "\n",
    "$$\n",
    "\\nabla f(\\mathbf{x})\n",
    "= \\left(\n",
    "\\frac{\\partial f(\\mathbf{x})}{\\partial x_1},\n",
    "\\frac{\\partial f(\\mathbf{x})}{\\partial x_2}\n",
    "\\right)\n",
    "= (4 x_1, 6 x_2)\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\nabla h_1(\\mathbf{x})\n",
    "= \\left(\n",
    "\\frac{\\partial h_1(\\mathbf{x})}{\\partial x_1},\n",
    "\\frac{\\partial h_1(\\mathbf{x})}{\\partial x_2}\n",
    "\\right)\n",
    "= (- 2 x_1, - 2 x_2).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09dbca1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The conditions in the theorem read\n",
    "\n",
    "\\begin{align*}\n",
    "&4 x_1 - 2 \\lambda_1 x_1  = 0\\\\\n",
    "&6 x_2 - 2 \\lambda_1 x_2  = 0.\n",
    "\\end{align*}\n",
    "\n",
    "The constraint $x_1^2 + x_2^2 = 1$ must also be satisfied. Observe that the linear independence condition is automatically satisfied since there is only one constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6845f95f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "There are several cases to consider. \n",
    "\n",
    "1- If neither $x_1$ nor $x_2$ is $0$, then the first equation gives $\\lambda_1 = 2$ while the second one gives $\\lambda_1 = 3$. So that case cannot happen.\n",
    "\n",
    "2- If $x_1 = 0$, then $x_2 = 1$ or $x_2 = -1$ by the constraint and the second equation gives $\\lambda_1 = 3$ in either case.\n",
    "\n",
    "3- If $x_2 = 0$, then $x_1 = 1$ or $x_1 = -1$ by the constraint and the first equation gives $\\lambda_1 = 2$ in either case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dd82b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Does any of these last four solutions, i.e., $(x_1,x_2,\\lambda_1) = (0,1,3)$, $(x_1,x_2,\\lambda_1) = (0,-1,3)$, $(x_1,x_2,\\lambda_1) = (1,0,2)$ and $(x_1,x_2,\\lambda_1) = (-1,0,2)$, actually correspond to a local minimizer?\n",
    "\n",
    "This problem can be solved manually. Indeed, replace $x_2^2 = 1 - x_1^2$ into the objective function to obtain \n",
    "\n",
    "$$\n",
    "2 x_1^2 + 3(1 - x_1^2)\n",
    "= -x_1^2 + 3.\n",
    "$$\n",
    "\n",
    "This is minimized for the largest value that $x_1^2$ can take, namely when $x_1 = 1$ or $x_1 = -1$. Indeed, we must have $0 \\leq x_1^2 \\leq x_1^2 + x_2^2 = 1$. So both $(x_1, x_2) = (1,0)$ and $(x_1, x_2) = (-1,0)$ are global minimizers. A fortiori, they must be local minimizers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7182f1b2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "What about $(x_1,x_2) = (0,1)$ and $(x_1,x_2) = (0,-1)$? Arguing as above, they in fact correspond to global *maximizers* of the objective function. $\\lhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c592ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**EXAMPLE:** **(continued)** Returning to the previous example, the points satisfying $h_1(\\mathbf{x}) = 0$ sit on the circle of radius $1$ around the origin. We have already seen that \n",
    "\n",
    "$$\n",
    "\\nabla h_1(\\mathbf{x})\n",
    "= \\left(\n",
    "\\frac{\\partial h_1(\\mathbf{x})}{\\partial x_1},\n",
    "\\frac{\\partial h_1(\\mathbf{x})}{\\partial x_2}\n",
    "\\right)\n",
    "= (- 2 x_1, - 2 x_2).\n",
    "$$\n",
    "\n",
    "Here is code plotting these (courtesy of ChatGPT 4). It uses [`numpy.meshgrid`](https://numpy.org/doc/stable/reference/generated/numpy.meshgrid.html) to generate a grid of points for $x_1$ and $x_2$, and [`matplotlib.pyplot.contour`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.contour.html) to plot the constraint set as a [contour line](https://en.wikipedia.org/wiki/Contour_line) (for the constant value $0$) of $h_1$. The gradients are plotted with the [`matplotlib.pyplot.quiver`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.quiver.html) function, which is used for plotting vectors as arrows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c259ee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define the constraint function\n",
    "def h1(x1, x2):\n",
    "    return 1 - x1**2 - x2**2\n",
    "\n",
    "# Generate a grid of points for x1 and x2\n",
    "x1 = np.linspace(-1.5, 1.5, 400)\n",
    "x2 = np.linspace(-1.5, 1.5, 400)\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "\n",
    "# Compute constraint function on grid\n",
    "H1 = h1(X1, X2)\n",
    "\n",
    "# Points on the constraint where the gradients will be plotted\n",
    "points = [\n",
    "    (0.5, np.sqrt(3)/2),\n",
    "    (-0.5, np.sqrt(3)/2),\n",
    "    (0.5, -np.sqrt(3)/2),\n",
    "    (-0.5, -np.sqrt(3)/2),\n",
    "    (1, 0),\n",
    "    (-1, 0),\n",
    "    (0, 1),\n",
    "    (0, -1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6026242d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.grid(True)\n",
    "plt.axis('equal')\n",
    "\n",
    "# Plot the constraint set where h1(x1, x2) = 0\n",
    "plt.contour(X1, X2, H1, levels=[0], colors='blue')\n",
    "\n",
    "# Plot gradients of h1 (red) at specified points\n",
    "for x1, x2 in points:\n",
    "    plt.quiver(x1, x2, -2*x1, -2*x2, scale=10, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd93cff0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.grid(True)\n",
    "plt.axis('equal')\n",
    "plt.contour(X1, X2, H1, levels=[0], colors='blue')\n",
    "for x1, x2 in points:\n",
    "    plt.quiver(x1, x2, -x1/np.sqrt(x1**2 + x2**2), \n",
    "               -x2/np.sqrt(x1**2 + x2**2), \n",
    "               scale=10, color='red')\n",
    "    plt.quiver(x1, x2, 4*x1/np.sqrt(16 * x1**2 + 36 * x2**2), \n",
    "               6*x2/np.sqrt(16 * x1**2 + 36 * x2**2), \n",
    "               scale=10, color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aa2061",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We see that, at $(-1,0)$ and $(1,0)$, the gradient is indeed orthogonal to the first-order feasible directions. $\\lhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9524570d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\newcommand{\\bSigma}{\\boldsymbol{\\Sigma}}$ $\\newcommand{\\bmu}{\\boldsymbol{\\mu}}$ $\\newcommand{\\blambda}{\\boldsymbol{\\lambda}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45608f8c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\newcommand{\\bSigma}{\\boldsymbol{\\Sigma}}$ $\\newcommand{\\bmu}{\\boldsymbol{\\mu}}$ $\\newcommand{\\bsigma}{\\boldsymbol{\\sigma}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9769e67",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Gradient descent and its convergence analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8ad579",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We implement gradient descent in Python. We assume that a function `f` and its gradient `grad_f` are provided. We first code the basic steepest descent step with a step size $\\alpha =$ `alpha`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5135b84",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def desc_update(grad_f, x, alpha):\n",
    "    return x - alpha*grad_f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8906f193",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def gd(f, grad_f, x0, alpha=1e-3, niters=int(1e6)):\n",
    "    \n",
    "    xk = x0\n",
    "    for _ in range(niters):\n",
    "        xk = desc_update(grad_f, xk, alpha)\n",
    "\n",
    "    return xk, f(xk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede9817b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We illustrate on a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ed36af",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def f(x): \n",
    "    return (x-1)**2 + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ac5b7a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "xgrid = np.linspace(-5,5,100)\n",
    "plt.plot(xgrid, f(xgrid))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d38f2ff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def grad_f(x):\n",
    "    return 2*(x-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a483d5",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gd(f, grad_f, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287978a7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We found a global minmizer in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbac76c9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The next example shows that a different local minimizer may be reached depending on the starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174345d8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def f(x): \n",
    "    return 4 * (x-1)**2 * (x+1)**2 - 2*(x-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b8c95e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "xgrid = np.linspace(-2,2,100)\n",
    "plt.plot(xgrid, f(xgrid), label='f')\n",
    "plt.ylim((-1,10))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1bf3df",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**CLICK ON TARGET:** If we start gradient descent from $-2$, where will it converge? $\\ddagger$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96123aae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def grad_f(x): \n",
    "    return 8 * (x-1) * (x+1)**2 + 8 * (x-1)**2 * (x+1) - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bbb8dd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "xgrid = np.linspace(-2,2,100)\n",
    "plt.plot(xgrid, f(xgrid), label='f')\n",
    "plt.plot(xgrid, grad_f(xgrid), label='grad_f')\n",
    "plt.ylim((-10,10))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41297166",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "gd(f, grad_f, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dfc24c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gd(f, grad_f, -2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f5ae1a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "In the final example, we end up at a stationary point that is not a local minimizer. Here both the first and second derivatives are zero. This is known as a [saddle point](https://en.wikipedia.org/wiki/Saddle_point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076898c5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952306d4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "xgrid = np.linspace(-2,2,100)\n",
    "plt.plot(xgrid, f(xgrid), label='f')\n",
    "plt.ylim((-10,10))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89af41f7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def grad_f(x):\n",
    "    return 3 * x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca578c2b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "xgrid = np.linspace(-2,2,100)\n",
    "plt.plot(xgrid, f(xgrid), label='f')\n",
    "plt.plot(xgrid, grad_f(xgrid), label='grad_f')\n",
    "plt.ylim((-10,10))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890af034",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "gd(f, grad_f, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6189f21d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gd(f, grad_f, -2, niters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02ab985",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f1aaa5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We revisit our first simple single-variable example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6d5cb8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def f(x): \n",
    "    return (x-1)**2 + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e002e0df",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "xgrid = np.linspace(-5,5,100)\n",
    "plt.plot(xgrid, f(xgrid))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4b326f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Recall that the first derivative is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62b37cc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def grad_f(x):\n",
    "    return 2*(x-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5635a19",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "So the second derivative is $f''(x) = 2$. Hence, this $f$ is $L$-smooth and $m$-strongly convex with $L = m = 2$. The theory we developed suggests taking step size $\\alpha_t = \\alpha = 1/L = 1/2$. It also implies that\n",
    "\n",
    "$$\n",
    "f(x^1) - f(x^*)\n",
    "\\leq \\left(1 - \\frac{m}{L}\\right) [f(x^0) - f(x^*)]\n",
    "= 0.\n",
    "$$\n",
    "\n",
    "We converge in one step! And that holds for any starting point $x^0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d4fffc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Let's try this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a8ed17",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gd(f, grad_f, 0, alpha=0.5, niters=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2d3920",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Let's try a different starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c05fa61",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gd(f, grad_f, 100, alpha=0.5, niters=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ac3c45",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb09711",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\newcommand{\\bSigma}{\\boldsymbol{\\Sigma}}$ $\\newcommand{\\bmu}{\\boldsymbol{\\mu}}$ $\\newcommand{\\bsigma}{\\boldsymbol{\\sigma}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0edde87",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Application to logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a293048a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We return to logistic regression, which we alluded to in the motivating example of this chapter.\n",
    "\n",
    "The input data is of the form $\\{(\\boldsymbol{\\alpha}_i, b_i) : i=1,\\ldots, n\\}$ where $\\boldsymbol{\\alpha}_i = (\\alpha_{i,1}, \\ldots, \\alpha_{i,d}) \\in \\mathbb{R}^d$ are the features and $b_i \\in \\{0,1\\}$ is the label. As before we use a matrix representation: $A \\in \\mathbb{R}^{n \\times d}$ has rows $\\boldsymbol{\\alpha}_i^T$, $i = 1,\\ldots, n$ and $\\mathbf{b} = (b_1, \\ldots, b_n) \\in \\{0,1\\}^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bae542",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We summarize the logistic regression approach. Our goal is to find a function of the features that approximates the probability of the label $1$. For this purpose, we model the [log-odds](https://en.wikipedia.org/wiki/Logit) (or logit function) of the probability of label $1$ as a linear function of the features $\\boldsymbol{\\alpha}  \\in \\mathbb{R}^d$\n",
    "\n",
    "$$\n",
    "\\log \\frac{p(\\mathbf{x}; \\boldsymbol{\\alpha})}{1-p(\\mathbf{x}; \\boldsymbol{\\alpha})}\n",
    "= \\boldsymbol{\\alpha}^T \\mathbf{x}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{x} \\in \\mathbb{R}^d$ is the vector of coefficients (i.e., parameters). Inverting this expression gives\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}; \\boldsymbol{\\alpha})\n",
    "= \\sigma(\\boldsymbol{\\alpha}^T \\mathbf{x})\n",
    "$$\n",
    "\n",
    "where the [sigmoid](https://en.wikipedia.org/wiki/Logistic_function) function is\n",
    "\n",
    "$$\n",
    "\\sigma(z)\n",
    "= \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "for $z \\in \\mathbb{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f89c521",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We plot the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb406fd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "outputs": [],
   "source": [
    "def sigmoid(z): \n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08917dd7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "grid = np.linspace(-5, 5, 100)\n",
    "plt.plot(grid,sigmoid(grid),'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04518d8c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We seek to maximize the probability of observing the data (also known as [likelihood function](https://en.wikipedia.org/wiki/Likelihood_function)) assuming the labels are independent given the features, which is given by\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{x}; A, \\mathbf{b})\n",
    "= \\prod_{i=1}^n p(\\boldsymbol{\\alpha}_i; \\mathbf{x})^{b_i} \n",
    "(1- p(\\boldsymbol{\\alpha}_i; \\mathbf{x}))^{1-b_i}\n",
    "$$\n",
    "\n",
    "Taking a logarithm, multiplying by $-1/n$ and substituting the sigmoid function, we want to minimize the [cross-entropy loss](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression)\n",
    "\n",
    "$$\n",
    "\\ell(\\mathbf{x}; A, \\mathbf{b})\n",
    "= \\frac{1}{n} \\sum_{i=1}^n \\left\\{- b_i \\log(\\sigma(\\boldsymbol{\\alpha}_i^T \\mathbf{x}))\n",
    "- (1-b_i) \\log(1- \\sigma(\\boldsymbol{\\alpha}_i^T \\mathbf{x}))\\right\\}.\n",
    "$$\n",
    "\n",
    "We used standard properties of the logarithm: for $x, y > 0$, $\\log(xy) = \\log x + \\log y$ and $\\log(x^y) = y \\log x$. \n",
    "\n",
    "Hence, we want to solve the minimization problem\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{x} \\in \\mathbb{R}^d} \\ell(\\mathbf{x}; A, \\mathbf{b}).\n",
    "$$\n",
    "\n",
    "We are implicitly using here that the logarithm is a strictly increasing function and therefore does not change the global maximum of a function. Multiplying by $-1$ changes the global maximum into a global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46660e67",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To use gradient descent, we need the gradient of $\\ell$. We use the *Chain Rule* and first compute the derivative of $\\sigma$ which is\n",
    "\n",
    "$$\n",
    "\\sigma'(z)\n",
    "= \\frac{e^{-z}}{(1 + e^{-z})^2}\n",
    "= \\frac{1}{1 + e^{-z}}\\left(1 - \\frac{1}{1 + e^{-z}}\\right)\n",
    "= \\sigma(z) (1 - \\sigma(z)).\n",
    "$$\n",
    "\n",
    "The latter expression is known as the [logistic differential equation](https://en.wikipedia.org/wiki/Logistic_function#Logistic_differential_equation). It arises in a variety of applications, including the modeling of [population dynamics](https://towardsdatascience.com/covid-19-infection-in-italy-mathematical-models-and-predictions-7784b4d7dd8d). Here it will be a convenient way to compute the gradient. \n",
    "\n",
    "Observe that, for $\\boldsymbol{\\alpha} = (\\alpha_{1}, \\ldots, \\alpha_{d})  \\in \\mathbb{R}^d$, by the *Chain Rule*\n",
    "\n",
    "$$\n",
    "\\nabla\\sigma(\\boldsymbol{\\alpha}^T \\mathbf{x})\n",
    "= \\sigma'(\\boldsymbol{\\alpha}^T \\mathbf{x}) \\nabla (\\boldsymbol{\\alpha}^T \\mathbf{x})\n",
    "= \\sigma'(\\boldsymbol{\\alpha}^T \\mathbf{x}) \\boldsymbol{\\alpha}\n",
    "$$\n",
    "\n",
    "where, throughout, the gradient is with respect to $\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8074555b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Alternatively, we can obtain the same formula by applying the single-variable *Chain Rule*\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial x_j} \\sigma(\\boldsymbol{\\alpha}^T \\mathbf{x})\n",
    "&= \\sigma'(\\boldsymbol{\\alpha}^T \\mathbf{x}) \\frac{\\partial}{\\partial x_j}(\\boldsymbol{\\alpha}^T \\mathbf{x})\\\\\n",
    "&= \\sigma'(\\boldsymbol{\\alpha}^T \\mathbf{x}) \\frac{\\partial}{\\partial x_j}\\left(\\alpha_{j} x_{j} + \\sum_{\\ell=1, \\ell \\neq j}^d \\alpha_{\\ell} x_{\\ell}\\right)\\\\\n",
    "&= \\sigma(\\boldsymbol{\\alpha}^T \\mathbf{x}) (1 - \\sigma(\\boldsymbol{\\alpha}^T \\mathbf{x}))\\, \\alpha_{j}\n",
    "\\end{align*}\n",
    "\n",
    "so that\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla\\sigma(\\boldsymbol{\\alpha}^T \\mathbf{x})\n",
    "&= \\left(\\sigma(\\boldsymbol{\\alpha}^T \\mathbf{x}) (1 - \\sigma(\\boldsymbol{\\alpha}^T \\mathbf{x}))\\, \\alpha_{1}, \\ldots, \\sigma(\\boldsymbol{\\alpha}^T \\mathbf{x}) (1 - \\sigma(\\boldsymbol{\\alpha}^T \\mathbf{x}))\\, \\alpha_{d}\\right)\\\\\n",
    "&= \\sigma(\\boldsymbol{\\alpha}^T \\mathbf{x}) (1 - \\sigma(\\boldsymbol{\\alpha}^T \\mathbf{x}))\\, (\\alpha_{1}, \\ldots, \\alpha_{d})\\\\\n",
    "&= \\sigma(\\boldsymbol{\\alpha}^T \\mathbf{x}) (1 - \\sigma(\\boldsymbol{\\alpha}^T \\mathbf{x}))\\, \\boldsymbol{\\alpha}.\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ef9e5d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "By another application of the *Chain Rule*, since $\\frac{\\mathrm{d}}{\\mathrm{d} z} \\log z = \\frac{1}{z}$,\n",
    "\n",
    "\\begin{align*}\n",
    "&\\nabla\\ell(\\mathbf{x}; A, \\mathbf{b})\\\\\n",
    "&= \\nabla\\left[\\frac{1}{n} \\sum_{i=1}^n \\left\\{- b_i \\log(\\sigma(\\boldsymbol{\\alpha_i}^T \\mathbf{x}))\n",
    "- (1-b_i) \\log(1- \\sigma(\\boldsymbol{\\alpha_i}^T \\mathbf{x}))\\right\\}\\right]\\\\\n",
    "&= - \\frac{1}{n} \\sum_{i=1}^n \\frac{b_i}{\\sigma(\\boldsymbol{\\alpha}_i^T \\mathbf{x})} \\nabla\\sigma(\\boldsymbol{\\alpha}_i^T \\mathbf{x})\n",
    "- \\frac{1}{n} \\sum_{i=1}^n \\frac{1-b_i}{1- \\sigma(\\boldsymbol{\\alpha}_i^T \\mathbf{x})} \\nabla(1 - \\sigma(\\boldsymbol{\\alpha}_i^T \\mathbf{x}))\\\\\n",
    "&= - \\frac{1}{n} \\sum_{i=1}^n \\frac{b_i}{\\sigma(\\boldsymbol{\\alpha}_i^T \\mathbf{x})} \\nabla\\sigma(\\boldsymbol{\\alpha}_i^T \\mathbf{x})\n",
    "+ \\frac{1}{n} \\sum_{i=1}^n \\frac{1-b_i}{1- \\sigma(\\boldsymbol{\\alpha}_i^T \\mathbf{x})} \\nabla\\sigma(\\boldsymbol{\\alpha}_i^T \\mathbf{x}).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52dd11e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Using the expression for the gradient of the sigmoid functions, this is\n",
    "\n",
    "\\begin{align*}\n",
    "&= - \\frac{1}{n} \\sum_{i=1}^n \\frac{b_i}{\\sigma(\\boldsymbol{\\alpha}_i^T \\mathbf{x})} \\sigma(\\boldsymbol{\\alpha}_i^T \\mathbf{x}) (1 - \\sigma(\\boldsymbol{\\alpha}_i^T \\mathbf{x})) \\,\\boldsymbol{\\alpha}_i\\\\\n",
    "&\\quad\\quad + \\frac{1}{n} \\sum_{i=1}^n \\frac{1-b_i}{1- \\sigma(\\boldsymbol{\\alpha}_i^T \\mathbf{x})} \\sigma(\\boldsymbol{\\alpha}_i^T \\mathbf{x}) (1 - \\sigma(\\boldsymbol{\\alpha}_i^T \\mathbf{x})) \\,\\boldsymbol{\\alpha}_i\\\\\n",
    "&= - \\frac{1}{n} \\sum_{i=1}^n \\left(\n",
    "b_i (1 - \\sigma(\\boldsymbol{\\alpha}_i^T \\mathbf{x})) - (1-b_i)\\sigma(\\boldsymbol{\\alpha}_i^T \\mathbf{x}) \n",
    "\\right)\\,\\boldsymbol{\\alpha}_i\\\\\n",
    "&= - \\frac{1}{n} \\sum_{i=1}^n (\n",
    "b_i - \\sigma(\\boldsymbol{\\alpha}_i^T \\mathbf{x}) \n",
    ") \\,\\boldsymbol{\\alpha}_i.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07118e7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To implement this formula below, it will be useful to re-write it in terms of the matrix representation $A \\in \\mathbb{R}^{n \\times d}$ (which has rows $\\boldsymbol{\\alpha}_i^T$, $i = 1,\\ldots, n$) and $\\mathbf{b} = (b_1, \\ldots, b_n) \\in \\{0,1\\}^n$. Let $\\bsigma : \\mathbb{R}^n \\to \\mathbb{R}$ be the vector-valued function that applies the sigmoid $\\sigma$ entry-wise, i.e., $\\bsigma(\\mathbf{z}) = (\\sigma(z_1),\\ldots,\\sigma(z_n))$ where $\\mathbf{z} = (z_1,\\ldots,z_n)$. Thinking of $\\sum_{i=1}^n (b_i - \\sigma(\\boldsymbol{\\alpha}_i^T \\mathbf{x})\\,\\boldsymbol{\\alpha}_i$ as a linear combination of the columns of $A^T$ with coefficients being the entries of the vector $\\mathbf{b} - \\bsigma(A \\mathbf{x})$, we that \n",
    "\n",
    "$$\n",
    "\\nabla\\ell(\\mathbf{x}; A, \\mathbf{b})\n",
    "= - \\frac{1}{n} \\sum_{i=1}^n (\n",
    "b_i - \\sigma(\\boldsymbol{\\alpha}_i^T \\mathbf{x}) \n",
    ") \\,\\boldsymbol{\\alpha}_i\n",
    "= -\\frac{1}{n} A^T [\\mathbf{b} - \\bsigma(A \\mathbf{x})].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95f7c11",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We turn to the Hessian. By symmetry, we can think of the $j$-th column of the Hessian as the gradient of the partial derivative with respect to $x_j$. Hence we start by computing the gradient of the $j$-th entry of the summands in the gradient of $\\ell$. We note that, for $\\boldsymbol{\\alpha} = (\\alpha_{1}, \\ldots, \\alpha_{d})  \\in \\mathbb{R}^d$,\n",
    "\n",
    "$$\n",
    "\\nabla [(b - \\sigma(\\boldsymbol{\\alpha}^T \\mathbf{x}))\\, \\alpha_{j}] \n",
    "= - \\nabla [\\sigma(\\boldsymbol{\\alpha}^T \\mathbf{x})] \\, \\alpha_{j} \n",
    "=  - \\sigma(\\boldsymbol{\\alpha}^T \\mathbf{x}) (1 - \\sigma(\\boldsymbol{\\alpha}^T \\mathbf{x}))\\, \\boldsymbol{\\alpha}\\alpha_{j}.\n",
    "$$\n",
    "\n",
    "Thus, using the fact that $\\boldsymbol{\\alpha} \\alpha_{j}$ is the $j$-th column of the matrix $\\boldsymbol{\\alpha} \\boldsymbol{\\alpha}^T$, we get\n",
    "\n",
    "$$\n",
    "\\mathbf{H}_{\\ell}(\\mathbf{x}; A, \\mathbf{b})\n",
    "= \\frac{1}{n} \\sum_{i=1}^n \\sigma(\\boldsymbol{\\alpha}_i^T \\mathbf{x}) (1 - \\sigma(\\boldsymbol{\\alpha}_i^T \\mathbf{x}))\\, \\boldsymbol{\\alpha}_i \\boldsymbol{\\alpha}_i^T\n",
    "$$\n",
    "\n",
    "where $\\mathbf{H}_{\\ell}(\\mathbf{x}; A, \\mathbf{b})$ indicates the Hessian with respect to the $\\mathbf{x}$ variables, for fixed $A, \\mathbf{b}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135de345",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "For step size $\\beta$, one step of gradient descent is therefore\n",
    "\n",
    "$$\n",
    "\\mathbf{x}^{t+1}\n",
    "= \\mathbf{x}^{t} +\\beta \\frac{1}{n} \\sum_{i=1}^n (\n",
    "b_i - \\sigma(\\boldsymbol{\\alpha}_i^T \\mathbf{x}^t) \n",
    ") \\,\\boldsymbol{\\alpha}_i.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a43f19",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** Before implementing GD for logistic regression, we return to our proof of convergence for smooth functions using a special case. We illustrate it on a random dataset. The functions $\\hat{f}$, $\\mathcal{L}$ and $\\frac{\\partial}{\\partial x}\\mathcal{L}$ are defined next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c28cb4d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def fhat(x,a):\n",
    "    return 1 / ( 1 + np.exp(-np.outer(x,a)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ff8efe",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def loss(x,a,b): \n",
    "    return np.mean(-b*np.log(fhat(x,a)) - (1 - b)*np.log(1 - fhat(x,a)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c717cccc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def grad(x,a,b):\n",
    "    return -np.mean((b - fhat(x,a))*a, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552bd61b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "seed = 535\n",
    "rng = np.random.default_rng(seed)\n",
    "n = 10000\n",
    "a = 2*rng.uniform(0,1,n) - 1\n",
    "b = rng.integers(2, size=n)\n",
    "x = np.linspace(-1,1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f2f889",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.plot(x, loss(x,a,b), label='loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d51d005",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We plot next the upper and lower bounds in the *Quadratic Bound for Smooth Functions* around $x = x_0$. It turns out we can take $L=1$ because all features are uniformly random between $-1$ and $1$. Observe that minimizing the upper quadratic bound leads to a decrease in $\\mathcal{L}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd037ec",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x0 = -0.3\n",
    "x = np.linspace(x0-0.05,x0+0.05,100)\n",
    "upper = loss(x0,a,b) + (x - x0)*grad(x0,a,b) + (1/2)*(x - x0)**2 # upper approximation\n",
    "lower = loss(x0,a,b) + (x - x0)*grad(x0,a,b) - (1/2)*(x - x0)**2 # lower approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8426cfb",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.plot(x, loss(x,a,b), label='loss')\n",
    "plt.plot(x, upper, label='upper')\n",
    "plt.plot(x, lower, label='lower')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d84f811",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9384313b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We modify our implementation of gradient descent to take a dataset as input. Recall that to run gradient descent, we first implement a function computing a descent update. It takes as input a function `grad_fn` computing the gradient itself, as well as a current iterate and a step size. We now also feed a dataset as additional input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a79b09f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def desc_update_for_logreg(grad_fn, A, b, curr_x, beta):\n",
    "    gradient = grad_fn(curr_x, A, b)\n",
    "    return curr_x - beta*gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245f31bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We are ready to implement GD. Our function takes as input a function `loss_fn` computing the objective, a function `grad_fn` computing the gradient, the dataset `A` and `b`, and an initial guess `init_x`. Optional parameters are the step size and the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec76ff09",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def gd_for_logreg(loss_fn, grad_fn, A, b, init_x, beta=1e-3, niters=int(1e5)):\n",
    "    \n",
    "    # initialization\n",
    "    curr_x = init_x\n",
    "    \n",
    "    # until the maximum number of iterations\n",
    "    for iter in range(niters):\n",
    "        curr_x = desc_update_for_logreg(grad_fn, A, b, curr_x, beta)\n",
    "    \n",
    "    return curr_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9552784",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To implement `loss_fn` and `grad_fn`, we define the sigmoid as above. Below, `pred_fn` is $\\bsigma(A \\mathbf{x})$. Here we write the loss function as\n",
    "\n",
    "\\begin{align*}\n",
    "\\ell(\\mathbf{x}; A, \\mathbf{b})\n",
    "&= \\frac{1}{n} \\sum_{i=1}^n \\left\\{- b_i \\log(\\sigma(\\boldsymbol{\\alpha_i}^T \\mathbf{x}))\n",
    "- (1-b_i) \\log(1- \\sigma(\\boldsymbol{\\alpha_i}^T \\mathbf{x}))\\right\\}\\\\\n",
    "&= \\mathrm{mean}\\left(-\\mathbf{b} \\odot \\mathbf{log}(\\bsigma(A \\mathbf{x})) - (\\mathbf{1} - \\mathbf{b}) \\odot \\mathbf{log}(\\mathbf{1} - \\bsigma(A \\mathbf{x}))\\right),\n",
    "\\end{align*}\n",
    "\n",
    "where $\\odot$ is the Hadamard product, or element-wise product (for example $\\mathbf{u} \\odot \\mathbf{v} = (u_1 v_1, \\ldots,u_n v_n)^T$), the logarithm (denoted in bold) is applied element-wise and $\\mathrm{mean}(\\mathbf{z})$ is the mean of the entries of $\\mathbf{z}$ (i.e., $\\mathrm{mean}(\\mathbf{z}) = n^{-1} \\sum_{i=1}^n z_i$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e9c2b8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def pred_fn(x, A): \n",
    "    return sigmoid(A @ x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bca385",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def loss_fn(x, A, b): \n",
    "    return np.mean(-b*np.log(pred_fn(x, A)) - (1 - b)*np.log(1 - pred_fn(x, A)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bf334e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def grad_fn(x, A, b):\n",
    "    return -A.T @ (b - pred_fn(x, A))/len(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da01929",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We can choosed a step size based on the smoothness of the objective as above. Recall that [`numpy.linalg.norm`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html) computes the Frobenius norm by default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50195f3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def stepsize_for_logreg(A, b):\n",
    "    L = LA.norm(A)**2 /len(b)\n",
    "    return 1/L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b29e786",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We start with a simple dataset from UC Berkeley's [DS100](http://www.ds100.org) course. The file `lebron.csv` is available [here](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/tree/main/utils/datasets). Quoting a previous version of the course's textbook:\n",
    "\n",
    "> In basketball, players score by shooting a ball through a hoop. One such player, LeBron James, is widely considered one of the best basketball players ever for his incredible ability to score. LeBron plays in the National Basketball Association (NBA), the United States's premier basketball league. We've collected a dataset of all of LeBron's attempts in the 2017 NBA Playoff Games using the NBA statistics website (https://stats.nba.com/).\n",
    "\n",
    "We first load the data and look at its summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf30b8d3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('lebron.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61adbdea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33a4c3e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The two columns we will be interested in are `shot_distance` (LeBron's distance from the basket when the shot was attempted (ft)) and `shot_made` (0 if the shot missed, 1 if the shot went in). As the summary table above indicates, the average distance was `10.6953` and the frequency of shots made was `0.565104`. We extract those two columns and display them on a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39256740",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "feature = data['shot_distance']\n",
    "label = data['shot_made']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f52de6a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.scatter(feature, label, alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f6409b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "As you can see, this kind of data is hard to vizualize because of the superposition of points with the same $x$ and $y$-values. One trick is to jiggle the $y$'s a little bit by adding Gaussian noise. We do this next and plot again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bf411b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "label_jitter = label + 0.05*rng.normal(0,1,len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ac7059",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.scatter(feature, label_jitter, alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cd553e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We apply GD to logistic regression. We first construct the data matrices $A$ and $\\mathbf{b}$. To allow an affine function of the features, we add a column of $1$'s as we have done before.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7f798d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A = np.stack((np.ones(len(label)),feature),axis=-1)\n",
    "b = label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5f2cd8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We run GD starting from $(0,0)$ with a step size computed from the smoothness of the objective as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0d0254",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "stepsize = stepsize_for_logreg(A, b)\n",
    "print(stepsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d87137",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "init_x = np.zeros(A.shape[1])\n",
    "best_x = gd_for_logreg(loss_fn, grad_fn, A, b, init_x, beta=stepsize)\n",
    "print(best_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d87b08",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Finally we plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ee2c85",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "grid = np.linspace(np.min(feature), np.max(feature), 100)\n",
    "feature_grid = np.stack((np.ones(len(grid)),grid),axis=-1)\n",
    "predict_grid = sigmoid(feature_grid @ best_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac19d2b6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.scatter(feature, label_jitter, alpha=0.2)\n",
    "plt.plot(grid,predict_grid,'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f463aa8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**Stochastic gradient descent** In [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) (SGD), a variant of gradient descent, we pick a sample $I_t$ uniformly at random in $\\{1,\\ldots,n\\}$ and update as follows\n",
    "\n",
    "$$\n",
    "\\mathbf{x}^{t+1}\n",
    "= \\mathbf{x}^{t} +\\beta \\, (\n",
    "b_{I_t} - \\sigma(\\boldsymbol{\\alpha}_{I_t}^T \\mathbf{x}^t) \n",
    ") \\, \\boldsymbol{\\alpha}_{I_t}.\n",
    "$$\n",
    "\n",
    "For the mini-batch version of SGD, we pick a random sub-sample $\\mathcal{B}_t \\subseteq \\{1,\\ldots,n\\}$ of size $B$\n",
    "\n",
    "$$\n",
    "\\mathbf{x}^{t+1}\n",
    "= \\mathbf{x}^{t} +\\beta \\frac{1}{B} \\sum_{i\\in \\mathcal{B}_t} (\n",
    "b_i - \\sigma(\\boldsymbol{\\alpha}_i^T \\mathbf{x}^t) \n",
    ") \\,\\boldsymbol{\\alpha}_i.\n",
    "$$\n",
    "\n",
    "The key observation about the two stochastic updates above is that, in expectation, they perform a step of gradient descent. That turns out to be enough and it has computational advantages.\n",
    "\n",
    "The only modification needed to the code is to pick a random mini-batch which can be fed to the descent update sub-routine as dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e83fca0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def sgd_for_logreg(loss_fn, grad_fn, A, b, \n",
    "                   init_x, beta=1e-3, niters=int(1e5), batch=40):\n",
    "    \n",
    "    # initialization\n",
    "    curr_x = init_x\n",
    "    \n",
    "    # until the maximum number of iterations\n",
    "    nsamples = len(b)\n",
    "    for _ in range(niters):\n",
    "        I = rng.integers(nsamples, size=batch)\n",
    "        curr_x = desc_update_for_logreg(\n",
    "            grad_fn, A[I,:], b[I], curr_x, beta)\n",
    "    \n",
    "    return curr_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bb5d3e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**South African Heart Disease dataset** We analyze a dataset from [[ESL](https://web.stanford.edu/~hastie/ElemStatLearn/)], which can be downloaded [here](https://web.stanford.edu/~hastie/ElemStatLearn/data.html). Quoting [[ESL](https://web.stanford.edu/~hastie/ElemStatLearn/), Section 4.4.2] \n",
    "\n",
    "> The data [...] are a subset of the Coronary Risk-Factor Study (CORIS) baseline survey, carried out in three rural areas of the Western Cape, South Africa (Rousseauw et al., 1983). The aim of the study was to establish the intensity of ischemic heart disease risk factors in that high-incidence region. The data represent white males between 15 and 64, and the response variable is the presence or absence of myocardial infarction (MI) at the time of the survey (the overall prevalence of MI was 5.1% in this region). There are 160 cases in our data set, and a sample of 302 controls. These data are described in more detail in Hastie and Tibshirani (1987).\n",
    "\n",
    "We load the data, which we slightly reformatted and look at a summary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5047d865",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('SAHeart.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ad0e26",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Our goal to predict `chd`, which stands for coronary heart disease, based on the other variables (which are briefly described [here](https://web.stanford.edu/~hastie/ElemStatLearn/datasets/SAheart.info.txt)). We use logistic regression again. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306dfa79",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We first construct the data matrices. We only use three of the predictors, as the convergence is quite slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9b55fd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "feature = data[['tobacco', 'ldl', 'age']].to_numpy()\n",
    "print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e9b910",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "label = data['chd'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85805f7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "A = np.concatenate((np.ones((len(label),1)),feature),axis=1)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5479b8d9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "b = label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7d6cc0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We use the same functions `loss_fn` and `grad_fn`, which were written for general logistic regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7141f9",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "init_x = np.zeros(A.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910cc134",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "stepsize = stepsize_for_logreg(A, b)\n",
    "print(stepsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023064bc",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "best_x = gd_for_logreg(loss_fn, grad_fn, A, b, \n",
    "                       init_x, beta=stepsize, niters=int(1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3192d88a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(best_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8923c6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The outcome is harder to vizualize. To get a sense of how accurate the result is, we compare our predictions to the true labels. By prediction, let us say that we mean that we predict label $1$ whenever $\\sigma(\\boldsymbol{\\alpha}^T \\mathbf{x}) > 1/2$. We try this on the training set. (A better approach would be to split the data into training and testing sets, but we will not do this here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b7766f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def logis_acc(x, A, b):\n",
    "    return np.sum((pred_fn(x, A) > 0.5) == b)/len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dc36b1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "logis_acc(best_x, A, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c979984",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We also try mini-batch stochastic gradient descent (SGD). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e84f7de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "init_x = np.zeros(A.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdace68",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "best_x = sgd_for_logreg(loss_fn, grad_fn, A, b, \n",
    "                        init_x, beta=stepsize, niters=int(1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ff5930",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(best_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edc70d4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "logis_acc(best_x, A, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a811e19c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "### Airline customer satisfaction dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c911b3c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We return to our original motivation, the [airline customer satisfaction](https://www.kaggle.com/datasets/sjleshrac/airlines-customer-satisfaction) dataset. We first load the dataset. We will need the column names later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d24ed2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('customer_airline_satisfaction.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43782d6b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "column_names = data.columns.tolist()\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374e5b74",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Our goal will be to predict the first column, `Satisfied`, from the rest of the columns. For this, we transform our data into Numpy arrays. We also standardize the columns by subtracting their mean and dividing by their standard deviation. This will allow to compare the influence of different features on the prediction. And we add a column of 1s to to account for the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3053a286",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y = data['Satisfied'].to_numpy()\n",
    "X = data.drop(columns=['Satisfied']).to_numpy()\n",
    "means = np.mean(X, axis=0)\n",
    "stds = np.std(X, axis=0)\n",
    "X_standardized = (X - means) / stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255c66da",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "A = np.concatenate((np.ones((len(y),1)),X_standardized),axis=1)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d22f7e2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "b = y\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13c835c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We use the functions `loss_fn` and `grad_fn` which were written for general logistic regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87068739",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "init_x = np.zeros(A.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01239a8",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "best_x = gd_for_logreg(loss_fn, grad_fn, A, b, \n",
    "                       init_x, beta=1e-3, niters=int(1e3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecfd523",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(best_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8075a1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To interpret the results, we plot the coefficients in decreasing order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69747fb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Exclude the intercept for plotting\n",
    "coefficients = best_x[1:]\n",
    "features = column_names[1:]\n",
    "\n",
    "# Sort the coefficients and corresponding feature names\n",
    "sorted_indices = np.argsort(coefficients)\n",
    "sorted_coefficients = coefficients[sorted_indices]\n",
    "sorted_features = np.array(features)[sorted_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cc35a1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Create the horizontal bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(sorted_features, sorted_coefficients, color='skyblue')\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.title('Logistic Regression Coefficients')\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dee3a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We see from the first ten bars or so that, as might be expected, higher ratings on various aspects of the flight generally contribute to a higher predicted likelihood of satisfaction (with one exception being `Gate location` whose coefficient is negative but may not be [statistically significant](https://en.wikipedia.org/wiki/Statistical_significance)). `Inflight entertainment` seems particularly influential. `Age` also shows the same pattern, something we had noticed in the introductory section through a different analysis. On the other hand, departure delay and arrival delay contribute to a lower predicted likelihood of satisfaction, again an expected pattern. The most negative influence however appears to come from `Class_Eco`."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
