<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MMiDS 6.4: Self-Assessment Quiz</title>
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@600&family=Open+Sans&display=swap"
        rel="stylesheet">
    <style>
        body {
            font-family: 'Arial', sans-serif;
            background-color: #f5f5f5;
            color: #333;
            margin: 0;
            padding: 0;
            display: flex;
            justify-content: center;
            align-items: center;
            min-height: 100vh;
        }

        #quiz-container {
            background-color: #fff;
            border-radius: 10px;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            padding: 30px;
            max-width: 800px;
            width: 100%;
            margin: 20px;
        }

        h1 {
            font-family: 'Montserrat', sans-serif;
            color: #673ab7;
            text-align: center;
            margin-bottom: 30px;
        }

        .question {
            background-color: #f5f0ff;
            border: 1px solid #d1c4e9;
            border-radius: 5px;
            padding: 20px;
            margin-bottom: 30px;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        }

        .question p {
            margin: 0 0 15px;
            font-weight: bold;
        }

        .options {
            list-style-type: none;
            padding: 0;
        }

        .options li {
            margin-bottom: 0px;
        }

        .options li label {
            display: block;
            padding: 12px;
            background-color: #f5f0ff;
            color: #333;
            border-radius: 5px;
            cursor: pointer;
            transition: background-color 0.3s;
        }

        .options li label:hover {
            background-color: #e0d6ff;
        }

        .options li input[type="radio"] {
            accent-color: #673ab7;
        }

        .feedback {
            margin-top: 15px;
            display: none;
            padding: 12px;
            border-radius: 5px;
        }

        .feedback.correct {
            background-color: #e0f0e5;
            color: #155724;
            display: block;
        }

        .feedback.incorrect {
            background-color: #f8e1e3;
            color: #721c24;
            display: block;
        }

        img.graph-image {
            display: block;
            margin-left: auto;
            margin-right: auto;
            width: 50%;
            margin-bottom: 20px;
            margin-top: 20px;
        }

        @media (max-width: 600px) {
            #quiz-container {
                padding: 20px;
                margin: 20px 10px;
            }

            h1 {
                font-size: 24px;
                margin-bottom: 20px;
            }

            .question {
                padding: 15px;
                margin-bottom: 20px;
            }

            img.graph-image {
                width: 70%;
                margin-bottom: 15px;
                margin-top: 15px;
            }
        }
    </style>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
    <div id="quiz-container">
        <h1>MMiDS 6.4: Self-Assessment Quiz</h1>

        <div class="question">
            <p>In the mixture of multivariate Bernoullis model, the joint distribution is given by:</p>
            <ul class="options">
                <li><label><input type="radio" name="q0" value="0"> a) \(P[X = x] = \prod_{k=1}^K P[C = k, X =
                        x]\)</label></li>
                <li><label><input type="radio" name="q0" value="1"> b) \(P[X = x] = \sum_{k=1}^K P[X = x|C = k] P[C =
                        k]\)</label></li>
                <li><label><input type="radio" name="q0" value="2"> c) \(P[X = x] = \prod_{k=1}^K P[X = x|C = k] P[C =
                        k]\)</label></li>
                <li><label><input type="radio" name="q0" value="3"> d) \(P[X = x] = \sum_{k=1}^K P[C = k, X =
                        x]\)</label></li>
            </ul>
            <div class="feedback" id="feedback0"></div>
        </div>

        <div class="question">
            <p>The majorization-minimization principle states that:</p>
            <ul class="options">
                <li><label><input type="radio" name="q1" value="0"> a) If \(U_x\) majorizes \(f\) at \(x\), then a
                        global minimum \(x'\) of \(U_x\) satisfies \(f(x') \geq f(x)\).</label></li>
                <li><label><input type="radio" name="q1" value="1"> b) If \(U_x\) majorizes \(f\) at \(x\), then a
                        global minimum \(x'\) of \(U_x\) satisfies \(f(x') \leq f(x)\).</label></li>
                <li><label><input type="radio" name="q1" value="2"> c) If \(U_x\) minorizes \(f\) at \(x\), then a
                        global minimum \(x'\) of \(U_x\) satisfies \(f(x') \geq f(x)\).</label></li>
                <li><label><input type="radio" name="q1" value="3"> d) If \(U_x\) minorizes \(f\) at \(x\), then a
                        global minimum \(x'\) of \(U_x\) satisfies \(f(x') \leq f(x)\).</label></li>
            </ul>
            <div class="feedback" id="feedback1"></div>
        </div>

        <div class="question">
            <p>In the EM algorithm for mixtures of multivariate Bernoullis, the E-step involves:</p>
            <ul class="options">
                <li><label><input type="radio" name="q2" value="0"> a) Updating the parameters \(\pi_k\) and
                        \(p_{k,m}\)</label></li>
                <li><label><input type="radio" name="q2" value="1"> b) Computing the responsibilities
                        \(r_{k,i}^t\)</label></li>
                <li><label><input type="radio" name="q2" value="2"> c) Minimizing the negative log-likelihood</label>
                </li>
                <li><label><input type="radio" name="q2" value="3"> d) Applying the log-sum-exp trick</label></li>
            </ul>
            <div class="feedback" id="feedback2"></div>
        </div>

        <div class="question">
            <p>In the EM algorithm for mixtures of multivariate Bernoullis, the M-step involves:</p>
            <ul class="options">
                <li><label><input type="radio" name="q3" value="0"> a) Updating the parameters \(\pi_k\) and
                        \(p_{k,m}\)</label></li>
                <li><label><input type="radio" name="q3" value="1"> b) Computing the responsibilities
                        \(r_{k,i}^t\)</label></li>
                <li><label><input type="radio" name="q3" value="2"> c) Minimizing the negative log-likelihood</label>
                </li>
                <li><label><input type="radio" name="q3" value="3"> d) Applying the log-sum-exp trick</label></li>
            </ul>
            <div class="feedback" id="feedback3"></div>
        </div>

        <div class="question">
            <p>The mixture of multivariate Bernoullis model is represented by the following graphical model:</p>
            <ul class="options">
                <li><label><input type="radio" name="q4" value="0"> a)
                        <pre><code>G = nx.DiGraph()
G.add_node("X", shape="circle", style="filled", fillcolor="gray")
G.add_node("C", shape="circle", style="filled", fillcolor="white")
G.add_edge("C", "X")</code></pre>
                    </label></li>
                <li><label><input type="radio" name="q4" value="1"> b)
                        <pre><code>G = nx.DiGraph()
G.add_node("X", shape="circle", style="filled", fillcolor="white")
G.add_node("C", shape="circle", style="filled", fillcolor="gray")
G.add_edge("C", "X")</code></pre>
                    </label></li>
                <li><label><input type="radio" name="q4" value="2"> c)
                        <pre><code>G = nx.DiGraph()
G.add_node("X", shape="circle", style="filled", fillcolor="gray")
G.add_node("C", shape="circle", style="filled", fillcolor="gray")
G.add_edge("C", "X")</code></pre>
                    </label></li>
                <li><label><input type="radio" name="q4" value="3"> d)
                        <pre><code>G = nx.DiGraph()
G.add_node("X", shape="circle", style="filled", fillcolor="white")
G.add_node("C", shape="circle", style="filled", fillcolor="white")
G.add_edge("C", "X")</code></pre>
                    </label></li>
            </ul>
            <div class="feedback" id="feedback4"></div>
        </div>

        <div class="question">
            <p>In the context of the EM algorithm, what is the purpose of the majorization-minimization principle?</p>
            <ul class="options">
                <li><label><input type="radio" name="q5" value="0"> a) To find the global minimum of the log-likelihood
                        function directly.</label></li>
                <li><label><input type="radio" name="q5" value="1"> b) To iteratively improve a lower bound on the
                        log-likelihood function.</label></li>
                <li><label><input type="radio" name="q5" value="2"> c) To avoid getting stuck in local minima of the
                        log-likelihood function.</label></li>
                <li><label><input type="radio" name="q5" value="3"> d) To simplify the computation of the
                        responsibilities in the E-step.</label></li>
            </ul>
            <div class="feedback" id="feedback5"></div>
        </div>

        <div class="question">
            <p>In a mixture of multivariate Bernoullis, what do the parameters \(p_{k,m}\) represent?</p>
            <ul class="options">
                <li><label><input type="radio" name="q6" value="0"> a) The probability of observing class \(k\) given
                        feature \(m\).</label></li>
                <li><label><input type="radio" name="q6" value="1"> b) The probability of observing feature \(m\) given
                        class \(k\).</label></li>
                <li><label><input type="radio" name="q6" value="2"> c) The prior probability of class \(k\).</label>
                </li>
                <li><label><input type="radio" name="q6" value="3"> d) The mixing weight of class \(k\).</label></li>
            </ul>
            <div class="feedback" id="feedback6"></div>
        </div>

        <div class="question">
            <p>In the context of clustering, what is the interpretation of the responsibilities computed in the E-step
                of the EM algorithm?</p>
            <ul class="options">
                <li><label><input type="radio" name="q7" value="0"> a) They represent the distance of each data point to
                        the cluster centers.</label></li>
                <li><label><input type="radio" name="q7" value="1"> b) They indicate the probability of each data point
                        belonging to each cluster.</label></li>
                <li><label><input type="radio" name="q7" value="2"> c) They determine the optimal number of
                        clusters.</label></li>
                <li><label><input type="radio" name="q7" value="3"> d) They are used to initialize the cluster centers
                        in the M-step.</label></li>
            </ul>
            <div class="feedback" id="feedback7"></div>
        </div>

        <div class="question">
            <p>What is the main difference between the Naive Bayes model and the mixture of multivariate Bernoullis
                model?</p>
            <ul class="options">
                <li><label><input type="radio" name="q8" value="0"> a) The Naive Bayes model assumes that the features
                        are independent given the class, while the mixture of multivariate Bernoullis model does
                        not.</label></li>
                <li><label><input type="radio" name="q8" value="1"> b) The Naive Bayes model is a generative model,
                        while the mixture of multivariate Bernoullis model is a discriminative model.</label></li>
                <li><label><input type="radio" name="q8" value="2"> c) The Naive Bayes model requires knowing the true
                        labels of the data points, while the mixture of multivariate Bernoullis model does not.</label>
                </li>
                <li><label><input type="radio" name="q8" value="3"> d) The Naive Bayes model can only handle binary
                        features, while the mixture of multivariate Bernoullis model can handle categorical
                        features.</label></li>
            </ul>
            <div class="feedback" id="feedback8"></div>
        </div>

    </div>

    <script>
        document.querySelectorAll('.options').forEach((optionsList, index) => {
            const correctOptions = [1, 1, 1, 0, 1, 1, 1, 1, 2];

            optionsList.addEventListener('change', (event) => {
                const selectedOption = parseInt(event.target.value);
                const feedbackDiv = document.getElementById('feedback' + index);
                let feedbackText = '';

                if (selectedOption === correctOptions[index]) {
                    feedbackText = "Excellent! " + getCorrectFeedback(index);
                    feedbackDiv.classList.add('correct');
                    feedbackDiv.classList.remove('incorrect');
                } else {
                    feedbackText = "Try again.";
                    feedbackDiv.classList.add('incorrect');
                    feedbackDiv.classList.remove('correct');
                }

                feedbackDiv.innerHTML = feedbackText;

                MathJax.typesetPromise([feedbackDiv]);
            });
        });

        function getCorrectFeedback(index) {
            switch (index) {
                case 0:
                    return "The text states, '\\(P[X = x] = \\sum_{k=1}^K P[C = k, X = x] = \\sum_{k=1}^K P[X = x|C = k] P[C = k]\\)'.";
                case 1:
                    return "The Majorization-Minimization Lemma in the text states, 'Let \\(f: \\mathbb{R}^d \\to \\mathbb{R}\\) and suppose \\(U_x\\) majorizes \\(f\\) at \\(x\\). Let \\(x'\\) be a global minimizer of \\(U_x(\\tilde{\\theta})\\) as a function of \\(\\tilde{\\theta}\\), provided it exists. Then \\(f(x') \\leq f(x)\\)'.";
                case 2:
                    return "In the summary of the EM algorithm, the E-step is described as 'compute \\(r_{k,i}^t\\) for all \\(i \\in [n], k \\in [K], m \\in [M]\\)' where \\(r_{k,i}^t\\) are the responsibilities.";
                case 3:
                    return "In the summary of the EM algorithm, the M-step is described as updating the parameters: '\\(\\pi_k^{t+1} = \\frac{\\eta_k^t}{n}\\) and \\(p_{k,m}^{t+1} = \\frac{\\eta_{k,m}^t}{\\eta_k^t}\\)'.";
                case 4:
                    return "The text states, 'Mathematically, that corresponds to applying the law of total probability as we did previously. Further, we let the vertex for \\(X\\) be shaded to indicate that it is observed, while the vertex for \\(Y\\) is not shaded to indicate that it is not.'.";
                case 5:
                    return "The text explains that the EM algorithm is an instantiation of the majorization-minimization principle. It introduces a surrogate function \\(Q_n(\\tilde{\\theta}|\\theta)\\) that majorizes the negative log-likelihood (NLL) at \\(\\theta\\). The EM algorithm then iteratively minimizes this surrogate function, which indirectly leads to an increase in the log-likelihood.";
                case 6:
                    return "The text defines \\(p_{k,m}\\) as \\(P[X_m = 1 | C = k]\\), which is the probability of observing feature \\(m\\) (taking the value 1) given that the class is \\(k\\).";
                case 7:
                    return "The text refers to responsibilities as 'our estimate – under the current parameter – of the probability that the sample comes from cluster \\(k\\)'.";
                case 8:
                    return "The text explicitly states that the main difference is that in the mixture of multivariate Bernoullis, 'the true labels of the samples are not observed.'.";
                default:
                    return "";
            }
        }

    </script>

</body>

</html>