<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MMiDS 8.4: Self-Assessment Quiz</title>
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@600&family=Open+Sans&display=swap"
        rel="stylesheet">
    <style>
        body {
            font-family: 'Arial', sans-serif;
            background-color: #f5f5f5;
            color: #333;
            margin: 0;
            padding: 0;
            display: flex;
            justify-content: center;
            align-items: center;
            min-height: 100vh;
        }

        #quiz-container {
            background-color: #fff;
            border-radius: 10px;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            padding: 30px;
            max-width: 800px;
            width: 100%;
            margin: 20px;
        }

        h1 {
            font-family: 'Montserrat', sans-serif;
            color: #673ab7;
            text-align: center;
            margin-bottom: 30px;
        }

        .question {
            background-color: #f5f0ff;
            border: 1px solid #d1c4e9;
            border-radius: 5px;
            padding: 20px;
            margin-bottom: 30px;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        }

        .question p {
            margin: 0 0 15px;
            font-weight: bold;
        }

        .options {
            list-style-type: none;
            padding: 0;
        }

        .options li {
            margin-bottom: 0px;
        }

        .options li label {
            display: block;
            padding: 12px;
            background-color: #f5f0ff;
            color: #333;
            border-radius: 5px;
            cursor: pointer;
            transition: background-color 0.3s;
        }

        .options li label:hover {
            background-color: #e0d6ff;
        }

        .options li input[type="radio"] {
            accent-color: #673ab7;
        }

        .feedback {
            margin-top: 15px;
            display: none;
            padding: 12px;
            border-radius: 5px;
        }

        .feedback.correct {
            background-color: #e0f0e5;
            color: #155724;
            display: block;
        }

        .feedback.incorrect {
            background-color: #f8e1e3;
            color: #721c24;
            display: block;
        }

        img.graph-image {
            display: block;
            margin-left: auto;
            margin-right: auto;
            width: 50%;
            margin-bottom: 20px;
            margin-top: 20px;
        }

        @media (max-width: 600px) {
            #quiz-container {
                padding: 20px;
                margin: 20px 10px;
            }

            h1 {
                font-size: 24px;
                margin-bottom: 20px;
            }

            .question {
                padding: 15px;
                margin-bottom: 20px;
            }

            img.graph-image {
                width: 70%;
                margin-bottom: 15px;
                margin-top: 15px;
            }
        }
    </style>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
    <div id="quiz-container">
        <h1>MMiDS 8.4: Self-Assessment Quiz</h1>

        <div class="question">
            <p>In stochastic gradient descent (SGD), how is the gradient estimated at each iteration?</p>
            <ul class="options">
                <li><label><input type="radio" name="q0" value="0"> a) By computing the gradient over the entire dataset.</label></li>
                <li><label><input type="radio" name="q0" value="1"> b) By using the gradient from the previous iteration.</label></li>
                <li><label><input type="radio" name="q0" value="2"> c) By randomly selecting a subset of sample and computing their gradient.</label></li>
                <li><label><input type="radio" name="q0" value="3"> d) By averaging the gradients of all samples in the dataset.</label></li>
            </ul>
            <div class="feedback" id="feedback0"></div>
        </div>

        <div class="question">
            <p>What is the key advantage of using mini-batch SGD over standard SGD?</p>
            <ul class="options">
                <li><label><input type="radio" name="q1" value="0"> a) It guarantees faster convergence to the optimal solution.</label></li>
                <li><label><input type="radio" name="q1" value="1"> b) It reduces the variance of the gradient estimate at each iteration.</label></li>
                <li><label><input type="radio" name="q1" value="2"> c) It eliminates the need for computing gradients altogether.</label></li>
                <li><label><input type="radio" name="q1" value="3"> d) It increases the computational cost per iteration.</label></li>
            </ul>
            <div class="feedback" id="feedback1"></div>
        </div>

        <div class="question">
            <p>Which of the following statements is true about the expected update step in stochastic gradient descent?</p>
            <ul class="options">
                <li><label><input type="radio" name="q2" value="0"> a) It is always equal to the full gradient descent update.</label></li>
                <li><label><input type="radio" name="q2" value="1"> b) It is always in the opposite direction of the full gradient descent update.</label></li>
                <li><label><input type="radio" name="q2" value="2"> c) It is, on average, equivalent to the full gradient descent update.</label></li>
                <li><label><input type="radio" name="q2" value="3"> d) It has no relationship to the full gradient descent update.</label></li>
            </ul>
            <div class="feedback" id="feedback2"></div>
        </div>

        <div class="question">
            <p>In multinomial logistic regression, what is the role of the softmax function (\(\gamma\))?</p>
            <ul class="options">
                <li><label><input type="radio" name="q3" value="0"> a) To compute the gradient of the loss function.</label></li>
                <li><label><input type="radio" name="q3" value="1"> b) To normalize the input features.</label></li>
                <li><label><input type="radio" name="q3" value="2"> c) To transform scores into a probability distribution over labels.</label></li>
                <li><label><input type="radio" name="q3" value="3"> d) To update the model parameters during gradient descent.</label></li>
            </ul>
            <div class="feedback" id="feedback3"></div>
        </div>

        <div class="question">
            <p>What is the Kullback-Leibler (KL) divergence used for in multinomial logistic regression?</p>
            <ul class="options">
                <li><label><input type="radio" name="q4" value="0"> a) To measure the distance between the predicted probabilities and the true labels.</label></li>
                <li><label><input type="radio" name="q4" value="1"> b) To normalize the input features.</label></li>
                <li><label><input type="radio" name="q4" value="2"> c) To update the model parameters during gradient descent.</label></li>
                <li><label><input type="radio" name="q4" value="3"> d) To compute the gradient of the loss function.</label></li>
            </ul>
            <div class="feedback" id="feedback4"></div>
        </div>

        <div class="question">
            <p>Which of the following is NOT a component of the forward pass in the backpropagation algorithm for multinomial logistic regression?</p>
            <ul class="options">
                <li><label><input type="radio" name="q5" value="0"> a) Computing the affine transformation of the input data.</label></li>
                <li><label><input type="radio" name="q5" value="1"> b) Applying the softmax function to obtain probabilities.</label></li>
                <li><label><input type="radio" name="q5" value="2"> c) Calculating the gradient of the loss function.</label></li>
                <li><label><input type="radio" name="q5" value="3"> d) Initializing the input data.</label></li>
            </ul>
            <div class="feedback" id="feedback5"></div>
        </div>

        <div class="question">
            <p>In the analysis of the MNIST dataset, what is the purpose of the <code>Flatten</code> layer in PyTorch?</p>
            <ul class="options">
                <li><label><input type="radio" name="q6" value="0"> a) To reduce the dimensionality of the input data.</label></li>
                <li><label><input type="radio" name="q6" value="1"> b) To reshape a multi-dimensional input (like an image) into a vector.</label></li>
                <li><label><input type="radio" name="q6" value="2"> c) To normalize the input data.</label></li>
                <li><label><input type="radio" name="q6" value="3"> d) To apply an activation function to the input data.</label></li>
            </ul>
            <div class="feedback" id="feedback6"></div>
        </div>

        <div class="question">
            <p>What is the purpose of the <code>zero_grad()</code> method in PyTorch optimizers?</p>
            <ul class="options">
                <li><label><input type="radio" name="q7" value="0"> a) To reset the gradients to zero before backpropagation.</label></li>
                <li><label><input type="radio" name="q7" value="1"> b) To initialize the model parameters.</label></li>
                <li><label><input type="radio" name="q7" value="2"> c) To evaluate the model's performance on a test dataset.</label></li>
                <li><label><input type="radio" name="q7" value="3"> d) To update the model parameters based on the computed gradients.</label></li>
            </ul>
            <div class="feedback" id="feedback7"></div>
        </div>

    </div>

    <script>
        document.querySelectorAll('.options').forEach((optionsList, index) => {
            const correctOptions = [2, 1, 2, 2, 0, 2, 1, 0];

            optionsList.addEventListener('change', (event) => {
                const selectedOption = parseInt(event.target.value);
                const feedbackDiv = document.getElementById('feedback' + index);
                let feedbackText = '';

                if (selectedOption === correctOptions[index]) {
                    feedbackText = "Correct! " + getCorrectFeedback(index);
                    feedbackDiv.classList.add('correct');
                    feedbackDiv.classList.remove('incorrect');
                } else {
                    feedbackText = "Try again.";
                    feedbackDiv.classList.add('incorrect');
                    feedbackDiv.classList.remove('correct');
                }

                feedbackDiv.innerHTML = feedbackText;

                MathJax.typesetPromise([feedbackDiv]);
            });
        });

        function getCorrectFeedback(index) {
            switch (index) {
                case 0:
                    return "The text states that in SGD, 'we pick a sample uniformly at random in \\(\\{1, ..., n\\}\\) and update as follows \\(w^{t+1} = w^t - \\alpha_t \\nabla f_{x_{I_t}, y_{I_t}}(w^t)\\).'";
                case 1:
                    return "The text implies that mini-batch SGD reduces the variance of the gradient estimate compared to standard SGD, which only uses a single sample.";
                case 2:
                    return "The text proves a lemma stating that 'in expectation, they [stochastic updates] perform a step of gradient descent.'";
                case 3:
                    return "The text defines the softmax function and states that it is used to 'transform these scores into a probability distribution over the labels.'";
                case 4:
                    return "The text introduces the KL divergence as a 'notion of distance between probability measures' and uses it to define the loss function in multinomial logistic regression.";
                case 5:
                    return "The text describes the forward pass as involving initialization, computing the affine transformation, and applying the softmax function. Calculating the gradient is part of the backward pass.";
                case 6:
                    return "The text mentions using <code>torch.nn.Flatten</code> to turn an image into a vector, which is a common preprocessing step in image classification tasks.";
                case 7:
                    return "In PyTorch, gradients accumulate by default, so <code>zero_grad()</code> is used to reset them before computing new gradients in the next iteration.";
                default:
                    return "";
            }
        }
    </script>

</body>

</html>
