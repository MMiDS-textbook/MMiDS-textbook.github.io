{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f4f7bd6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell",
     "colab-keep"
    ]
   },
   "source": [
    "***\n",
    "\n",
    "*Course:* [Math 535](https://people.math.wisc.edu/~roch/mmids/) - Mathematical Methods in Data Science (MMiDS)  \n",
    "*Chapter:* 5-Random walks on graphs and Markov chains  \n",
    "*Author:* [Sebastien Roch](https://people.math.wisc.edu/~roch/), Department of Mathematics, University of Wisconsin-Madison  \n",
    "*Updated:* Jan 7, 2024   \n",
    "*Copyright:* &copy; 2024 Sebastien Roch\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd71c592",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# IF RUNNING ON GOOGLE COLAB, UNCOMMENT THE FOLLOWING CODE CELL\n",
    "# When prompted, upload: \n",
    "#     * mmids.py\n",
    "#     * mathworld-adjacency.csv\n",
    "#     * mathworld-titles.csv\n",
    "# from your local file system\n",
    "# Files at: https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/tree/main/utils\n",
    "# Alternative instructions: https://colab.research.google.com/notebooks/io.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afc12dc",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell",
     "colab-uncomment"
    ]
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bff4c6c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# PYTHON 3\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "from numpy.random import default_rng\n",
    "rng = default_rng(535)\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import mmids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae139b52",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Motivating example: ranking webpages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6690af94",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "A common task in network analysis is to identify \"central\" vertices in a graph. Centrality is a vague concept. It can be defined in many different ways depending on the context and the type of network. Quoting from [Wikipedia](https://en.wikipedia.org/wiki/Centrality):\n",
    "\n",
    "> In graph theory and network analysis, indicators of centrality assign numbers or rankings to nodes within a graph corresponding to their network position. Applications include identifying the most influential person(s) in a social network, key infrastructure nodes in the Internet or urban networks, super-spreaders of disease, and brain networks. [...] Centrality indices are answers to the question \"What characterizes an important vertex?\" The answer is given in terms of a real-valued function on the vertices of a graph, where the values produced are expected to provide a ranking which identifies the most important nodes. The word \"importance\" has a wide number of meanings, leading to many different definitions of centrality. \n",
    "\n",
    "In an undirected graph, a natural approach is to look at the degree of a vertex as a measure of its importance (also referred to as degree centrality). But it is hardly the only one. One could for instance look at the average distance to all other nodes (its reciprocal is the [closeness centrality](https://en.wikipedia.org/wiki/Closeness_centrality)) or at the number of shortest paths between pairs of vertices going through the vertex (known as [betweenness centrality](https://en.wikipedia.org/wiki/Betweenness_centrality)). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002e9b1b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "What if the graph is directed? Things are somewhat more complicated there. For instance, there is now the in-degree as well as the out-degree. \n",
    "\n",
    "Let us look at a particular example of practical importance, the World Wide Web (from now on, the Web). In this case, the vertices are webpages and a directed edge from $u$ to $v$ indicates a hyperlink from page $u$ to page $v$. The Web is much too large to analyze here. Instead, we will consider a tiny (but still interesting!) subset of it, the pages of [Wolfram's MathWorld](https://mathworld.wolfram.com), a wonderful mathematics resource. \n",
    "\n",
    "Each page of MathWorld concerns a particular mathematical concept, e.g., [scale-free network](https://mathworld.wolfram.com/Scale-FreeNetwork.html). A definition and notable properties are described. Importantly for us, in a section entitled \"SEE ALSO\", other related mathematical concepts are listed with a link to their MathWorld page. In the case of scale-free networks, the [small world network](https://mathworld.wolfram.com/SmallWorldNetwork.html) topic is referenced, among others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b10af31",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The resulting directed graph is available through the [NetSet](https://netset.telecom-paris.fr/index.html) datasets and can be downloaded [here](https://netset.telecom-paris.fr/pages/mathworld.html). We load it now. For convenience, we have reformatted it into the files `mathworld-adjacency.csv` and `mathworld-titles.csv`, which are available on the [GitHub of the book](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/tree/main/utils/datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a102155a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_edges = pd.read_csv('mathworld-adjacency.csv')\n",
    "df_edges.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5d396a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "It consists in a list of directed edges. For example, the first one is an edge from vertex `0` to vertex `2`. The second one is from `1` to `47` and so on. \n",
    "\n",
    "There is a total of $49069$ edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2277546b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_edges.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952e5b84",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The second file contains the titles of the pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893a51cf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_titles = pd.read_csv('mathworld-titles.csv')\n",
    "df_titles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626bf932",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "So the first edge above is from `Alexander's Horned Sphere` to `Antoine's Horned Sphere`. That is, the [latter](https://mathworld.wolfram.com/AntoinesHornedSphere.html) is listed in the SEE ALSO section of the [former](https://mathworld.wolfram.com/AlexandersHornedSphere.html). \n",
    "\n",
    "There are $12362$ topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4ff4f8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_titles.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152f0548",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We construct the graph by adding the edges one by one. We first convert `df_edges` into a Numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5202a9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "edgelist = df_edges[['from','to']].to_numpy()\n",
    "print(edgelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2f45ca",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n = 12362\n",
    "G = nx.empty_graph(n, create_using=nx.DiGraph)\n",
    "for i in range(edgelist.shape[0]):\n",
    "    G.add_edge(edgelist[i,0], edgelist[i,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db77fb68",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "G.in_degree(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002376fe",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "while that of `Antoine's Horned Sphere` is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d543aac3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "G.in_degree(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1ac82a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "suggesting that the former is more central than the latter, at least in the sense that it is referenced more often.\n",
    "\n",
    "But is that the right measure? Consider the following: `Antoine's Horned Sphere` receives only one reference, but it is from a seemingly relatively important vertex, `Alexander's Horned Sphere`. How can one take this into account in quantifying its importance in the network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00fa7eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We will come back to this question later in this chapter. To hint at things to come, it will turn out that \"exploring the graph at random\" provides a powerful perspective on centrality.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef63d84a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Elements of finite Markov chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60207fa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**EXAMPLE:** **(Random Walk on the Petersen Graph)** Let $G = (V,E)$ be the Petersen graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d1d179",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "G_petersen = nx.petersen_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198b1932",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nx.draw_networkx(G_petersen, pos=nx.circular_layout(G_petersen), labels={i: i+1 for i in range(10)}, \n",
    "                 node_size=600, node_color='black', font_size=16, font_color='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8664048b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Each vertex $i$ has degree $3$, that is, it has three neighbors which we denote $v_{i,1}, v_{i,2}, v_{i,3}$ in some arbitrary order. For instance, denoting the vertices by $1,\\ldots, 10$ as above, vertex $9$ has neighbors $v_{9,1} = 4, v_{9,2} = 6, v_{9,3} = 7$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7269931",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We consider the following random walk on $G$. We start at $X_0 = 1$. Then, for each $t\\geq 0$, we let $X_{t+1}$ be a uniformly chosen neighbor of $X_t$, independently of the previous history. That is, we jump at random from neighbor to neighbor. Formally, fix $X_0 = 1$ and let $(Z_t)_{t \\geq 0}$ be an i.i.d. sequence of random variables taking values in $\\{1,2,3\\}$ satisfying\n",
    "\n",
    "$$\n",
    "\\mathbb{P}[Z_t = 1] = \\mathbb{P}[Z_t = 2] = \\mathbb{P}[Z_t = 3] = 1/3.\n",
    "$$\n",
    "\n",
    "Then define, for all $t \\geq 0$,\n",
    "$\n",
    "X_{t+1}\n",
    "= f(X_t, Z_t)\n",
    "= v_{i,Z_t}\n",
    "$\n",
    "if $X_t = v_i$.\n",
    "\n",
    "By an argument similar to the previous example, $(X_t)_{t \\geq 0}$ is a Markov chain.\n",
    "Also as in the previous example, one can pick $X_0$ according to an initial distribution, independently from the sequence $(Z_t)_{t \\geq 0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de76d2ce",
   "metadata": {
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\lhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c72c6c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**EXAMPLE:** **(Random Walk on the Petersen Graph, continued)** Consider again the random walk on the Petersen graph $G = (V,E)$. We number the vertices $1, 2,\\ldots, 10$. To compute the transition matrix, we list for each vertex its neighbors and put the value $1/3$ in the corresponding columns. For instance, vertex $1$ has neighbors $2$, $5$ and $6$, so row $1$ has $1/3$ in columns $2$, $5$, and $6$. And so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66138f46",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nx.draw_networkx(G_petersen, pos=nx.circular_layout(G_petersen), labels={i: i+1 for i in range(10)}, \n",
    "                 node_size=600, node_color='black', font_size=16, font_color='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45683895",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We get:\n",
    "\n",
    "$$\n",
    "P = \\begin{pmatrix}\n",
    "0 & 1/3 & 0 & 0 & 1/3 & 1/3 & 0 & 0 & 0 & 0\\\\\n",
    "1/3 & 0 & 1/3 & 0 & 0 & 0 & 1/3 & 0 & 0 & 0\\\\\n",
    "0 & 1/3 & 0 & 1/3 & 0 & 0 & 0 & 1/3 & 0 & 0\\\\\n",
    "0 & 0 & 1/3 & 0 & 1/3 & 0 & 0 & 0 & 1/3 & 0\\\\\n",
    "1/3 & 0 & 0 & 1/3 & 0 & 0 & 0 & 0 & 0 & 1/3\\\\\n",
    "1/3 & 0 & 0 & 0 & 0 & 0 & 0 & 1/3 & 1/3 & 0\\\\\n",
    "0 & 1/3 & 0 & 0 & 0 & 0 & 0 & 0 & 1/3 & 1/3\\\\\n",
    "0 & 0 & 1/3 & 0 & 0 & 1/3 & 0 & 0 & 0 & 1/3\\\\\n",
    "0 & 0 & 0 & 1/3 & 0 & 1/3 & 1/3 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 1/3 & 0 & 1/3 & 1/3 & 0 & 0\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8da679",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We have already encountered a matrix that encodes the neighbors of each vertex, the adjacency matrix. Here we can recover the transition matrix by multiplying the adjacency matrix by $1/3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccec4f4c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A_petersen = nx.adjacency_matrix(G_petersen).toarray()\n",
    "P_petersen = (1/3) * A_petersen\n",
    "print(P_petersen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08b7474",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\lhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2bbeb2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**EXAMPLE:** **(Robot Vacuum, continued)** Returning to our *Robot Vacuum Example*, the transition graph of the chain can be obtained by thinking of $P$ as the weighted adjacency matrix of the transition graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5c5dce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "P_robot = np.array([\n",
    "[0, 0.8, 0, 0.2, 0, 0, 0, 0, 0],\n",
    "[0.3, 0, 0.2, 0, 0, 0.5, 0, 0, 0],\n",
    "[0, 0.6, 0, 0, 0, 0.4, 0, 0, 0],\n",
    "[0.1, 0.1, 0, 0, 0.8, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0.25, 0, 0, 0.75, 0, 0],\n",
    "[0, 0.15, 0.15, 0, 0, 0, 0, 0.35, 0.35],\n",
    "[0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "[0, 0, 0, 0, 0.3, 0.4, 0.2, 0, 0.1],\n",
    "[0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "]\n",
    ")\n",
    "print(P_robot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e874b2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We define a graph from its adjancency matrix. See [`networkx.from_numpy_array()`](https://networkx.org/documentation/stable/reference/generated/networkx.convert_matrix.from_numpy_array.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77186680",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "G_robot = nx.from_numpy_array(P_robot, create_using=nx.DiGraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13673fcf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Drawing edge weights on a directed graph in a readable fashion is not straighforward. We will not do this here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b0ad1c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "n_robot = P_robot.shape[0]\n",
    "nx.draw_networkx(G_robot, pos=nx.circular_layout(G_robot), \n",
    "                 labels={i: i+1 for i in range(n_robot)}, \n",
    "                 node_size=600, node_color='black', font_size=16, font_color='white', \n",
    "                 connectionstyle='arc3, rad = 0.2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4589e6e8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\lhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e68743",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** Once we have specified a transition matrix (and an initial distribution), we can simulate the corresponding Markov chain. This is useful to compute (approximately) probabilities of complex events through the law of large numbers. Here is some code to generate one sample path up to some given time $T$. We assume that the state space is $[n]$. We use [`rng.choice`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.choice.html) to generate each transition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14057133",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from numpy.random import default_rng\n",
    "rng = default_rng(535)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22523108",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def SamplePath(mu, P, T):\n",
    "    n = mu.shape[0] # size of state space\n",
    "    X = np.zeros(T+1) # initialization of sampe path\n",
    "    for i in range(T+1):\n",
    "        if i == 0: # initial distribution\n",
    "            X[i] = rng.choice(a=np.arange(start=1,stop=n+1),p=mu)\n",
    "        else: # next state is chosen from current state row\n",
    "            X[i] = rng.choice(a=np.arange(start=1,stop=n+1),p=P[int(X[i-1]-1),:])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a71072",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Let's try with our *Robot Vacuum*. We take the initial distribution to be the uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f763fa42",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mu = np.ones(n_robot) / n_robot\n",
    "SamplePath(mu, P_robot, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfd50ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "For example, we can use a simulation to approximate the expected number of times that room $9$ is visited up to time $10$. To do this, we run the simulation a large number of times (say $1000$) and count the average number of visits to $9$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c895b698",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "z = 9 # state of interest\n",
    "N_samples = 1000 # number of repetitions\n",
    "visits_to_z = np.zeros(N_samples) # initialization of number of visits\n",
    "\n",
    "for i in range(N_samples):\n",
    "    visits_to_z[i] = np.count_nonzero(SamplePath(mu, P_robot, 10) == z)\n",
    "\n",
    "print(np.mean(visits_to_z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa5ec30",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\lhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4f2237",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\newcommand{\\P}{\\mathbb{P}}$\n",
    "$\\newcommand{\\E}{\\mathbb{E}}$\n",
    "$\\newcommand{\\S}{\\mathcal{S}}$\n",
    "$\\newcommand{\\indep}{\\perp\\!\\!\\!\\perp}$   \n",
    "$\\newcommand{\\bmu}{\\boldsymbol{\\mu}}$\n",
    "$\\newcommand{\\bpi}{\\boldsymbol{\\pi}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdabcfca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Limit behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472e299c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**EXAMPLE:** **(Robot Vacuum, continued)** Going back to the *Robot Vacuum Example*, recall the transition graph above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d436199",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "P_robot = np.array([\n",
    "[0, 0.8, 0, 0.2, 0, 0, 0, 0, 0],\n",
    "[0.3, 0, 0.2, 0, 0, 0.5, 0, 0, 0],\n",
    "[0, 0.6, 0, 0, 0, 0.4, 0, 0, 0],\n",
    "[0.1, 0.1, 0, 0, 0.8, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0.25, 0, 0, 0.75, 0, 0],\n",
    "[0, 0.15, 0.15, 0, 0, 0, 0, 0.35, 0.35],\n",
    "[0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "[0, 0, 0, 0, 0.3, 0.4, 0.2, 0, 0.1],\n",
    "[0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "]\n",
    ")\n",
    "G_robot = nx.from_numpy_array(P_robot, create_using=nx.DiGraph)\n",
    "n_robot = P_robot.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7245983d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nx.draw_networkx(G_robot, pos=nx.circular_layout(G_robot), \n",
    "                 labels={i: i+1 for i in range(n_robot)}, \n",
    "                 node_size=600, node_color='black', font_size=16, font_color='white', \n",
    "                 connectionstyle='arc3, rad = 0.2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a03aa1c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "While there is no direct edge from $4$ to $3$, we do have $4 \\to 3$ through the path $(4,2), (2,3)$. Do we have $3 \\to 4$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac75ca9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\lhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c82e8d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**EXAMPLE:** **(Two Sinks)** Consider the following random walk on a digraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0babdf6",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "G_sinks = nx.DiGraph()\n",
    "\n",
    "for i in range(5):\n",
    "    G_sinks.add_node(i)\n",
    "\n",
    "G_sinks.add_edge(0, 0, weight=1/3)\n",
    "G_sinks.add_edge(0, 1, weight=1/3)\n",
    "G_sinks.add_edge(1, 1, weight=1/3)\n",
    "G_sinks.add_edge(1, 2, weight=1/3)\n",
    "G_sinks.add_edge(2, 2, weight=1)\n",
    "G_sinks.add_edge(3, 3, weight=1)\n",
    "G_sinks.add_edge(0, 4, weight=1/3)\n",
    "G_sinks.add_edge(1, 4, weight=1/3)\n",
    "G_sinks.add_edge(4, 3, weight=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467d148a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nx.draw_networkx(G_sinks, pos=nx.circular_layout(G_sinks), \n",
    "                 labels={i: i+1 for i in range(5)}, \n",
    "                 node_size=600, node_color='black', font_size=16, font_color='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea18b975",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Here we have $1 \\to 4$ (Why?). The *Communication Lemma* implies that, when started at $1$, $(X_t)_{t \\geq 0}$ visits $4$ with positive probability. But that probability is not one. Indeed we also have $1 \\to 3$ (Why?), so there is a positive probability of visiting $3$ as well. But if we do so before visiting $4$, we stay at $3$ forever hence cannot subsequently reach $4$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc43f86",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "In fact, intuitively, if we run this chain long enough we will either get stuck at $3$ or get stuck at $4$. These give rise to different stationary distributions. The transition probability is the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013e8b77",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "P_sinks = nx.adjacency_matrix(G_sinks).toarray()\n",
    "print(P_sinks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96171b8f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "It is easy to check that $\\bpi = (0,0,1,0,0)$ and $\\bpi' = (0,0,0,1,0)$ are both stationary distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b88485",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pi = np.array([0.,0.,1.,0.,0.])\n",
    "pi_prime = np.array([0.,0.,0.,1.,0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4026291",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "P_sinks.T @ pi.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b3c49b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "P_sinks.T @ pi_prime.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a542fcd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "In fact, there are infinitely many stationary distributions in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf9189d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\lhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f83fc65",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**EXAMPLE:** **(Robot Vacuum and Two Sinks, continued)** Because irreducibility is ultimately a graph-theoretic property, it is easy to check using `NetworkX`. For this, we use the function [`is_strongly_connected()`](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.components.is_strongly_connected.html). Consider again the *Robot Vacuum Example*. This one turns out to be irreducible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f3d249",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(nx.is_strongly_connected(G_robot))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774ed92d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The *Two Sinks Example*, on the other hand, is not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a418ca",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(nx.is_strongly_connected(G_sinks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca6d23b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\lhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a95086",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** In general, computing stationary distributions is not as straigthforward as in the simple example we considered above. We conclude this section with some numerical recipes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9d32c5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Going back to the *Robot Vacuum*, finding a solution to $\\bpi P =\\bpi$ in this case is not obvious. One way to do this is to note that, taking transposes, this condition is equivalent to $P^T \\bpi^T = \\bpi^T$. That is, $\\bpi^T$ is an eigenvector of $P^T$ with eigenvalue $1$. (Or, as we noted previously, the row vector $\\bpi$ is a left eigenvector of $P$ with eigenvalue $1$.) It must also satisfy $\\bpi \\geq 0$ with at least one entry non-zero. Here, we use NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7743c60",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "w, v = LA.eig(P_robot.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f24ac5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The first eigenvalue is approximately $1$, as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7138a982",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1accc3a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The corresponding eigenvector is approximately non-negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3a9c44",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(v[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c407c61a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To obtain a stationary distribution, we remove the imaginary part and normalize it to sum to $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a00250a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pi_robot = np.real(v[:,0]) / np.sum(np.real(v[:,0]))\n",
    "print(pi_robot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6383dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Alternatively, we can solve the linear system\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n \\pi_i p_{i,j} = \\pi_j, \\qquad \\forall j \\in [n].\n",
    "$$\n",
    "\n",
    "It turns out that the last equation is a linear combination over the other equations (see *Exercise 3.48*), so we remove it and replace it instead with the condition $\\sum_{i=1}^n \\pi_i = 1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30493c97",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The left-hand side of the resulting linear system is (after taking the transpose to work with column vectors):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4901e52",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A = np.copy(P_robot.T) - np.diag(np.ones(n_robot))\n",
    "A[n_robot-1,:] = np.ones(n_robot)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e9452d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The right-hand side of the resulting linear system is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abf0834",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "b = np.concatenate((np.zeros(n_robot-1),[1.]))\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a032ada",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We solve the linear system using [`numpy.linalg.solve()`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2f4100",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pi_robot_solve = LA.solve(A,b)\n",
    "print(pi_robot_solve)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eac68fe",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "This last approach is known as \"Replace an Equation\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5e14b1",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\lhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f41480b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** The *Convergence to Equilibrium Theorem* implies that we can use power iteration to compute the unique stationary diistribution in the irreducible case. We revisit the *Robot Vaccum Example*. We initialize with the uniform distribution, then repeatedly multiply by $P$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2d5d49",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mu = np.ones(n_robot)/n_robot\n",
    "print(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd03d09",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mu = mu @ P_robot\n",
    "print(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367091a5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mu = mu @ P_robot\n",
    "print(mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7640b69",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We repeat, say, $10$ more times and compare to the truth `pi_robot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefd7859",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    mu = mu @ P_robot\n",
    "print(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0763e96e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(pi_robot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49a97ca",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We see that a small number of iterations sufficed to get an accurate answer. In general, the speed of convergence depends on the eigenvalues of $P$ that are strictly smaller than $1$ in absolute value. We will derive this type of result in a special case in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f6f1ba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We can also check the *Ergodic Theorem* through simulation. We generate a long sample path and compare the state visit frequencies to `pi_robot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3328f4ab",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mu = np.ones(n_robot) / n_robot\n",
    "path_length = 10000\n",
    "visit_freq = np.zeros(n_robot) # initialization of number of visits\n",
    "\n",
    "path = mmids.SamplePath(mu, P_robot, path_length)\n",
    "for i in range(n_robot):\n",
    "    visit_freq[i] = np.count_nonzero(path == i+1)/(path_length+1)\n",
    "\n",
    "print(visit_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c933eb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(pi_robot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f49df55",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\lhd$"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
