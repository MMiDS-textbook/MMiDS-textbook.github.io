{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78f9f1c1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell",
     "colab-keep"
    ]
   },
   "source": [
    "***\n",
    "\n",
    "*Course:* [Math 535](https://people.math.wisc.edu/~roch/mmids/) - Mathematical Methods in Data Science (MMiDS)  \n",
    "*Chapter:* 2-Least squares   \n",
    "*Author:* [Sebastien Roch](https://people.math.wisc.edu/~roch/), Department of Mathematics, University of Wisconsin-Madison  \n",
    "*Updated:* Feb 7, 2024   \n",
    "*Copyright:* &copy; 2024 Sebastien Roch\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bc20f4",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# You will need the files:\n",
    "#     * mmids.py\n",
    "#     * advertising.csv \n",
    "# from https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/tree/main/utils\n",
    "#\n",
    "# IF RUNNING ON GOOGLE COLAB (RECOMMENDED):\n",
    "# \"Upload to session storage\" from the Files tab on the left\n",
    "# Alternative instructions: https://colab.research.google.com/notebooks/io.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9beb731",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# PYTHON 3\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import mmids\n",
    "seed = 535\n",
    "rng = np.random.default_rng(seed)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c06f69",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Motivating example: predicting sales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3e77c9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**Figure:** Helpful map of ML by scitkit-learn ([Source](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html))\n",
    "\n",
    "![ml-cheat-sheet](https://scikit-learn.org/stable/_static/ml_map.png)\n",
    "\n",
    "$\\bowtie$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e444201f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The following dataset is from the excellent textbook [[ISLP]](https://www.statlearning.com/). Quoting [ISLP, Section 2.1]:\n",
    "\n",
    "> Suppose that we are statistical consultants hired by a client to provide advice on how to improve sales of a particular product. The `Advertising` data set consists of the `sales` of that product in 200 different markets, along with advertising budgets for the product in each of those markets for three different media: `TV`, `radio`, and `newspaper`. [...] It is not possible for our client to directly increase sales of the product. On the other hand, they can control the advertising expenditure in each of the three media. Therefore, if we determine that there is an association between advertising and sales, then we can instruct our client to adjust advertising budgets, thereby indirectly increasing sales. In other words, our goal is to develop an accurate model that can be used to predict sales on the basis of the three media budgets.\n",
    "\n",
    "This a [regression](https://en.wikipedia.org/wiki/Regression_analysis) problem. That is, we want to estimate the relationship between an outcome variable and one or more predictors (or features). We load the data, show its first few lines and some statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9c7831",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('advertising.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3880e820",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cd6bad",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909d4cda",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We will focus for now on the TV budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cb9f9f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TV = df['TV'].to_numpy()\n",
    "sales = df['sales'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea95fb7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We make a scatterplot showing the realtion between those two quantities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81f2fe1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.scatter(TV, sales)\n",
    "plt.xlabel('TV')\n",
    "plt.ylabel('sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa317b42",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "There does seem to be a relationship between the two. Roughly a higher TV budget is linked to higher sales, although the correspondence is not perfect. To express the relationship more quantitatively, we seek a function $f$ such that\n",
    "\n",
    "$$\n",
    "y \\approx f(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "where $\\mathbf{x}$ denotes a sample TV budget and $y$ is the corresponding observed sales. We might posit for instance that there exists a true $f$ and that each observation is disrupted by some noise $\\varepsilon$\n",
    "\n",
    "$$\n",
    "y = f(\\mathbf{x}) + \\varepsilon.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f1ee77",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "A natural way to estimate such an $f$ from data is [$k$-nearest-neighbors ($k$-NN) regression](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm#k-NN_regression). Let the data be of the form $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$, where in our case $\\mathbf{x}_i$ is the TV budget of the $i$-th sample and $y_i$ is the corresponding sales. For each $\\mathbf{x}$ (not necessarily in the data), we do the following:\n",
    "\n",
    "- find the $k$ nearest $\\mathbf{x}_i$'s to $\\mathbf{x}$\n",
    "\n",
    "- take an average of the corresponding $y_i$'s. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a570a8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We implement this method in Python. We use the function [`numpy.argsort`](https://numpy.org/doc/stable/reference/generated/numpy.argsort.html) to sort an array and the function [`numpy.absolute`](https://numpy.org/doc/stable/reference/generated/numpy.absolute.html) to compute the absolute deviation. Our quick implementation here assumes that the $\\mathbf{x}_i$'s are scalars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0c1643",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def knnregression(x,y,k,xnew):\n",
    "    n = len(x)\n",
    "    closest = np.argsort([np.absolute(x[i]-xnew) for i in range(n)])\n",
    "    return np.mean(y[closest[0:k]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bb903b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "For $k=3$ and a grid of $1000$ points, we get the following approximation $\\hat{f}$. Here the function [`numpy.linspace`](https://numpy.org/doc/stable/reference/generated/numpy.linspace.html) creates an array of equally spaced points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d632d5a6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "k = 3\n",
    "xgrid = np.linspace(TV.min(), TV.max(), num=1000)\n",
    "yhat = [knnregression(TV,sales,k,xnew) for xnew in xgrid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd82575",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.scatter(TV, sales, alpha=0.5)\n",
    "plt.xlabel('TV')\n",
    "plt.ylabel('sales')\n",
    "plt.plot(xgrid, yhat, 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0be59d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "A higher $k$ gives something smoother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b174b5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "k = 10\n",
    "xgrid = np.linspace(TV.min(), TV.max(), num=1000)\n",
    "yhat = [knnregression(TV,sales,k,xnew) for xnew in xgrid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84eb9a8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.scatter(TV, sales, alpha=0.5)\n",
    "plt.xlabel('TV')\n",
    "plt.ylabel('sales')\n",
    "plt.plot(xgrid, yhat, 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c83abc1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "One downside of $k$-NN regression is that it does not give an easily interpretable relationship: if I increase my TV budget by $\\Delta$ dollars, how is it expected to affect the sales? Another issue arises in high dimension where the counter-intuitive phenomena we discussed in a previous section can have a significant impact. Recall in particular the *High-dimensional Cube Theorem*. If we have $d$ predictors -- where $d$ is large -- and our data is distributed uniformly in a bounded region, then any given $\\mathbf{x}$ will be far from any of our data points. In that case, the $y$-values of the closest $\\mathbf{x}_i$'s may not be predictive. This is referred to as the [Curse of Dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0396585",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "One way out is to make stronger assumptions on the function $f$. For instance, we can assume that the true relationship is (approximately) affine, that is,\n",
    "\n",
    "$$\n",
    "y \\approx \\beta_0 + \\beta_1 x.\n",
    "$$\n",
    "\n",
    "Or if we have $d$ predictors:\n",
    "\n",
    "$$\n",
    "y \\approx \\beta_0 + \\sum_{j=1}^d \\beta_j x_j.\n",
    "$$\n",
    "\n",
    "How do we estimate appropriate intercept and coefficients? The standard approach is to minimize the sum of the squared errors\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n \\left(y_i - \\left\\{\\beta_0 + \\sum_{j=1}^d \\beta_j (\\mathbf{x}_{i})_j\\right\\}\\right)^2,\n",
    "$$\n",
    "\n",
    "where $(\\mathbf{x}_{i})_j$ is the $j$-th entry of input vector $\\mathbf{x}_i$ and $y_i$ is the corresponding $y$-value. This is called [multiple linear regression](https://en.wikipedia.org/wiki/Linear_regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4bbea8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "It is a [least-squares problem](https://en.wikipedia.org/wiki/Least_squares). We re-write it in a more convenient matrix form and combine $\\beta_0$ with the other $\\beta_i$'s by adding a dummy predictor to each sample. Let\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \n",
    "\\begin{pmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_n\n",
    "\\end{pmatrix},\n",
    "\\quad\\quad\n",
    "A =\n",
    "\\begin{pmatrix}\n",
    "1 & \\mathbf{x}_1^T \\\\\n",
    "1 & \\mathbf{x}_2^T \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "1 & \\mathbf{x}_n^T\n",
    "\\end{pmatrix}\n",
    "\\quad\\text{and}\\quad\n",
    "\\boldsymbol{\\beta} = \n",
    "\\begin{pmatrix}\n",
    "\\beta_0 \\\\\n",
    "\\beta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_d\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Then observe that\n",
    "\n",
    "\\begin{align*}\n",
    "\\|\\mathbf{y} \n",
    "- A \\boldsymbol{\\beta}\\|^2\n",
    "&= \\sum_{i=1}^n \\left(y_i - (A \\boldsymbol{\\beta})_i\\right)^2\\\\\n",
    "&= \\sum_{i=1}^n \\left(y_i - \\left\\{\\beta_0 + \\sum_{j=1}^d \\beta_j (\\mathbf{x}_{i})_j\\right\\}\\right)^2.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7034277a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The linear least-squares problem is then formulated as\n",
    "\n",
    "$$\n",
    "\\min_{\\boldsymbol{\\beta}} \n",
    "\\|\\mathbf{y} \n",
    "- A \\boldsymbol{\\beta}\\|^2.\n",
    "$$\n",
    "\n",
    "In words, we are looking for a linear combination of the columns of $A$ that is closest to $\\mathbf{y}$ in Euclidean distance. Indeed, minimizing the squared Euclidean distance is equivalent to minimizing its square root, as the latter in an increasing function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5126cff5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "One could solve this optimization problem through calculus (and we will come back to this approach later in the course), but understanding the geometric and algebraic structure of the problem turns out to provide powerful insights into its solution -- and that of many of problems in data science. It will also be an opportunity to review some basic linear-algebraic concepts along the way. \n",
    "\n",
    "We will come back to the `Advertising` dataset later in the chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09464da0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Background: review of vector spaces and matrix inverses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d553f713",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** The plane $P$ made of all points $(x,y,z) \\in \\mathbb{R}^3$ that satisfy $z = x+y$ is a linear subspace. Indeed, $0 = 0 + 0$ so $(0,0,0) \\in P$. And, for any $\\mathbf{u}_1 = (x_1, y_1, z_1)$ and $\\mathbf{u}_2 = (x_2, y_2, z_2)$ such that $z_1 = x_1 + y_1$ and $z_2 = x_2 + y_2$ and for any $\\alpha \\in \\mathbb{R}$, we have\n",
    "\n",
    "$$\n",
    "\\alpha z_1 + z_2 = \\alpha (x_1 + y_1) + (x_2 + y_2) = (\\alpha x_1 + x_2) + (\\alpha y_1 + y_2).\n",
    "$$\n",
    "\n",
    "That is, $\\alpha \\mathbf{u}_1 + \\mathbf{u}_2$ satisfies the condition defining $P$ and therefore is itself in $P$. Note also that $P$ passes through the origin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2917286c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "In this example, the linear subspace $P$ can be described alternatively as the collection of every vector of the form $(x, y, x+y)$.\n",
    "\n",
    "We use [`plot_surface`](https://matplotlib.org/stable/api/_as_gen/mpl_toolkits.mplot3d.axes3d.Axes3D.html#mpl_toolkits.mplot3d.axes3d.Axes3D.plot_surface) to plot it over a grid of points created using [`numpy.meshgrid`](https://numpy.org/doc/stable/reference/generated/numpy.meshgrid.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419e7e57",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0,1,num=101)\n",
    "y = np.linspace(0,1,num=101)\n",
    "X, Y = np.meshgrid(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7249b7ba",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9d81da",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0823a8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Z = X + Y\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e2413c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(X, Y, Z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7f757b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806f2334",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** In Numpy, one can compute the rank of a matrix using the function [`numpy.linalg.matrix_rank`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.matrix_rank.html). We will see later in the course how to compute it using the singular value decomposition (which is how `LA.matrix_rank` does it).\n",
    "\n",
    "Let's try the example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d29e3b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "w1 = np.array([1., 0., 1.])\n",
    "w2 = np.array([0., 1., 1.])\n",
    "w3 = np.array([1., -1., 0.])\n",
    "A = np.stack((w1, w2, w3), axis=-1)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce28945",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We compute the rank of `A`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb425dfb",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "LA.matrix_rank(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5e943e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We take only the first two columns of `A` this time to form `B`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecf6d9a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "B = np.stack((w1, w2),axis=-1)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374fe8b7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "LA.matrix_rank(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b030971",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "In Numpy, `@` is used for matrix product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87378c8b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "C = np.array([[1., 0., 1.],[0., 1., -1.]])\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101842b1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "LA.matrix_rank(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5005605b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(B @ C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef129cd9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469a5fc9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## The geometry of least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f42fee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** To solve a linear system in Numpy, use [`numpy.linalg.solve`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html). As an example, we consider the overdetermined system with\n",
    "\n",
    "$$\n",
    "A = \\begin{pmatrix}\n",
    "1 & 0\\\\\n",
    "0 & 1\\\\\n",
    "1 & 1\n",
    "\\end{pmatrix}\n",
    "\\quad\n",
    "\\text{and}\n",
    "\\quad\n",
    "\\mathbf{b} = \\begin{pmatrix}\n",
    "0\\\\\n",
    "0\\\\\n",
    "2\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "We use [`numpy.ndarray.T`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.T.html) for the transpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46677a13",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "w1 = np.array([1., 0., 1.])\n",
    "w2 = np.array([0., 1., 1.])\n",
    "A = np.stack((w1, w2),axis=-1)\n",
    "b = np.array([0., 0., 2.])\n",
    "x = LA.solve(A.T @ A, A.T @ b)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f34c95",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We can also use [`numpy.linalg.lstsq`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html) directly on the overdetermined system to compute the least-square solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8520134",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = LA.lstsq(A, b, rcond=None)[0]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf65348a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3a2d2f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## QR decomposition and Householder transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df27f550",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We implement the Gram-Schmidt algorithm in Python. For reasons that will become clear in the next subsection, we output both the $\\mathbf{q}_j$'s and $r_{ij}$'s, each in matrix form. Here we use [`numpy.dot`](https://numpy.org/doc/stable/reference/generated/numpy.dot.html) to compute inner products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da61a56",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def gramschmidt(A):\n",
    "    (n,m) = A.shape\n",
    "    Q = np.zeros((n,m))\n",
    "    R = np.zeros((m,m))\n",
    "    for j in range(m):\n",
    "        v = np.copy(A[:,j])\n",
    "        for i in range(j):\n",
    "            R[i,j] = np.dot(Q[:,i], A[:,j])\n",
    "            v -= R[i,j]*Q[:,i]\n",
    "        R[j,j] = LA.norm(v)\n",
    "        Q[:,j] = v/R[j,j]\n",
    "    return Q, R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13beaa90",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Let's try a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f426345",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "w1 = np.array([1., 0., 1.])\n",
    "w2 = np.array([0., 1., 1.])\n",
    "A = np.stack((w1, w2),axis=-1)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3e5e22",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Q, R = gramschmidt(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e708825",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4fde5c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c2faad",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6905fd72",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We implement back substitution in Python. In our naive implementation, we assume that the diagonal entries are not zero, which will suffice for our purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc8d577",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def backsubs(R,b):\n",
    "    m = b.shape[0]\n",
    "    x = np.zeros(m)\n",
    "    for i in reversed(range(m)):\n",
    "        x[i] = (b[i] - np.dot(R[i,i+1:m],x[i+1:m]))/R[i,i]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79484d14",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Forward substitution is implemented similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b6c02b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def forwardsubs(L,b):\n",
    "    m = b.shape[0]\n",
    "    x = np.zeros(m)\n",
    "    for i in range(m):\n",
    "        x[i] = (b[i] - np.dot(L[i,0:i],x[0:i]))/L[i,i]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07970df7",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d8ef89",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We implement the QR approach to least squares and return to our simple overdetermined system example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad823a9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def ls_by_qr(A, b):\n",
    "    Q, R = gramschmidt(A)\n",
    "    return backsubs(R, Q.T @ b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14881827",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "w1 = np.array([1., 0., 1.])\n",
    "w2 = np.array([0., 1., 1.])\n",
    "A = np.stack((w1, w2),axis=-1)\n",
    "b = np.array([0., 0., 2.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bd003f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = ls_by_qr(A, b)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0b0c29",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329be1b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We implement the procedure above in Python. We will need the following function. For $\\alpha \\in \\mathbb{R}$, let the sign of $\\alpha$ be\n",
    "\n",
    "$$\n",
    "\\mathrm{sign}(\\alpha)\n",
    "= \n",
    "\\begin{cases}\n",
    "1 & \\text{if $\\alpha > 0$}\\\\\n",
    "0 & \\text{if $\\alpha = 0$}\\\\\n",
    "-1 & \\text{if $\\alpha < 0$}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "For example, in Python, using the function [`numpy.sign`](https://numpy.org/doc/stable/reference/generated/numpy.sign.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674f6cfd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.sign(-10), np.sign(20), np.sign(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad76e77",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The following function constructs the upper triangular matrix $R$ by iteratively modifying the relevant block of $A$. On the other hand, computing the matrix $Q$ actually requires extra computational work that is often not needed. We saw that, in the context of the least-squares problem, we really only need to compute $Q^T \\mathbf{b}$ for some input vector $\\mathbf{b}$. This can be done at the same time that $R$ is constructed, as follows. The key point to note is that $Q^T \\mathbf{b} = H_m \\cdots H_1 \\mathbf{b}$.\n",
    "\n",
    "See [here](https://numpy.org/doc/stable/reference/generated/numpy.copy.html) for an explanation of `numpy.copy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babe8aa1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def householder(A, b):\n",
    "    n, m = A.shape\n",
    "    R = np.copy(A)\n",
    "    Qtb = np.copy(b)\n",
    "    for k in range(m):\n",
    "    \n",
    "        # computing z\n",
    "        y = R[k:n,k]\n",
    "        e1 = np.zeros(n-k)\n",
    "        e1[0] = 1\n",
    "        z = np.sign(y[0]) * LA.norm(y) * e1 + y\n",
    "        z = z / LA.norm(z)\n",
    "        \n",
    "        # updating R\n",
    "        R[k:n,k:m] = R[k:n,k:m] - 2 * np.outer(z, z) @ R[k:n,k:m]\n",
    "        \n",
    "        # updating Qtb\n",
    "        Qtb[k:n] = Qtb[k:n] - 2 * np.outer(z, z) @ Qtb[k:n]\n",
    "    \n",
    "    return R[0:m,0:m], Qtb[0:m]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27efe72d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "In `householder`, we use both reflections defined above. We will not prove this here, but the particular choice made has good numerical properties. Quoting [TB, Lecture 10]:\n",
    "\n",
    "> Mathematically, either choice of sign is satisfactory. However, this is a case where numerical stability -- insensitivity to rounding errors -- dictates that one choice should be taken rather than the other. For numerical stability, it is desirable to reflect $\\mathbf{x}$ to the vector $z \\|\\mathbf{x}\\| \\mathbf{e}_1$ that is not too close to $\\mathbf{x}$ itself. [...] Suppose that [in the figure above] the angle between $H^+$ and the $\\mathbf{e}_1$ axis is very small. Then the vector $\\mathbf{v} = \\|\\mathbf{x}\\| \\mathbf{e}_1 - \\mathbf{x}$ is much smaller than $\\mathbf{x}$ or $\\|\\mathbf{x}\\| \\mathbf{e}_1$. Thus the calculation of $\\mathbf{v}$ represents a subtraction of nearby quantities and will tend to suffer from cancellation errors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5d4436",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We return to our overdetermined system example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ff6725",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "w1 = np.array([1., 0., 1.])\n",
    "w2 = np.array([0., 1., 1.])\n",
    "A = np.stack((w1, w2),axis=-1)\n",
    "b = np.array([0., 0., 2.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bef2a6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "R, Qtb = householder(A, b)\n",
    "x = backsubs(R, Qtb)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eb7067",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "One advantage of the Householder approach is that it produces a matrix $Q$ with very good orthogonality, i.e., $Q^T Q \\approx I$. We give a quick example below comparing Gram-Schmidt and Householder. (The choice of matrix $A$ will become clearer when we discuss the singular value decomposition later in the chapter.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7310490",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n = 50\n",
    "U, W = LA.qr(rng.normal(0,1,(n,n)))\n",
    "V, W = LA.qr(rng.normal(0,1,(n,n)))\n",
    "S = np.diag((1/2) ** np.arange(1,n+1))\n",
    "A = U @ S @ V.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32585b2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Qgs, Rgs = gramschmidt(A)\n",
    "print(LA.norm(A - Qgs @ Rgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609e06d9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(LA.norm(Qgs.T @ Qgs - np.identity(n)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a4bd4b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "As you can see above, the $Q$ and $R$ factors produced by Gram-Schmidt do have the property that $QR \\approx A$. However, $Q$ is far from orthogonal. (Recall that `LA.norm` computes the Frobenius norm introduced previously.)\n",
    "\n",
    "On the other hand, Householder reflections perform much better in that respect as we show next. Here we use the implementation of Householder transformations in [`numpy.linalg.qr`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.qr.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f41bb9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Qhh, Rhh = LA.qr(A)\n",
    "print(LA.norm(A - Qhh @ Rhh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9d9dbb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(LA.norm(Qhh.T @ Qhh - np.identity(n)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2128b9fe",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaa485d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Application to regression analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979f3da1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We test our least-squares method on simulated data. This has the advantage that we know the truth.\n",
    "\n",
    "Suppose the truth is a linear function of one variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d62309d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n, b0, b1 = 100, -1, 1\n",
    "x = np.linspace(0,10,num=n)\n",
    "y = b0 + b1*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62d3144",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.scatter(x,y,alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd253c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "A perfect straight line is little too easy. So let's add some noise. That is, to each $y_i$ we add an independent random variable $\\varepsilon_i$ with a standard Normal distribution (mean $0$, variance $1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8df810",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y += rng.normal(0,1,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5add327",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.scatter(x,y,alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ced6615",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We form the matrix $A$ and use our least-squares code to solve for $\\boldsymbol{\\hat\\beta}$. The function `ls_by_qr`, which we implemented previously, is in [mmids.py](https://raw.githubusercontent.com/MMiDS-textbook/MMiDS-textbook.github.io/main/utils/mmids.py), which is available on the [GitHub of the notes](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/tree/main). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70568a1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A = np.stack((np.ones(n),x),axis=-1)\n",
    "coeff = mmids.ls_by_qr(A,y)\n",
    "print(coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dba56c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.scatter(x,y,alpha=0.5)\n",
    "plt.plot(x,coeff[0]+coeff[1]*x,'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4363d063",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb96f2f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** Suppose the truth is in fact a degree-two polynomial of one variable with Gaussian noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90c1ad0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n, b0, b1, b2 = 100, 0, 0, 1\n",
    "x = np.linspace(0,10,num=n)\n",
    "y = b0 + b1 * x + b2 * x**2 + 10*rng.normal(0,1,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6bbfd0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.scatter(x,y,alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40da5c7e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We form the matrix $A$ and use our least-squares code to solve for $\\boldsymbol{\\hat\\beta}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c772ee37",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A = np.stack((np.ones(n), x, x**2), axis=-1)\n",
    "coeff = mmids.ls_by_qr(A,y)\n",
    "print(coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdf87f3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.scatter(x,y,alpha=0.5)\n",
    "plt.plot(x, coeff[0] + coeff[1] * x + coeff[2] * x**2, 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a10125",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6404caa0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "### Overfitting in polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f18af95",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We return to the `Advertising` dataset from the [[ISLP]](https://www.statlearning.com/) textbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcac1d1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('advertising.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccbb851",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We will focus for now on the TV budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccab03d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "TV = df['TV'].to_numpy()\n",
    "sales = df['sales'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f6d7dd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.scatter(TV, sales)\n",
    "plt.xlabel('TV')\n",
    "plt.ylabel('sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e281bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We form the matrix $A$ and use our least-squares code to solve for $\\boldsymbol{\\beta}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bded4a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n = np.size(TV)\n",
    "A = np.stack((np.ones(n),TV),axis=-1)\n",
    "coeff = mmids.ls_by_qr(A,sales)\n",
    "print(coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc492c3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "TVgrid = np.linspace(TV.min(), TV.max(), num=100)\n",
    "plt.scatter(TV,sales,alpha=0.5)\n",
    "plt.plot(TVgrid,coeff[0]+coeff[1]*TVgrid,'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36c421a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "A degree-two polynomial might be a better fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bab024d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A = np.stack((np.ones(n), TV, TV**2), axis=-1)\n",
    "coeff = mmids.ls_by_qr(A,sales)\n",
    "print(coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab874c54",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.scatter(TV,sales,alpha=0.5)\n",
    "plt.plot(TVgrid, coeff[0] + coeff[1] * TVgrid + coeff[2] * TVgrid**2,'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f139f5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The fit looks slightly better than the linear one. This is not entirely surprising though given that the linear model is a subset of the quadratic one. But, in adding more parameters, one must worry about [overfitting](https://en.wikipedia.org/wiki/Overfitting#cite_note-1). To quote Wikipedia:\n",
    "\n",
    ">In statistics, overfitting is \"the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably\".[[1](https://en.wikipedia.org/wiki/Overfitting#cite_note-1)] An overfitted model is a statistical model that contains more parameters than can be justified by the data.[[2](https://en.wikipedia.org/wiki/Overfitting#cite_note-CDS-2)] The essence of overfitting is to have unknowingly extracted some of the residual variation (i.e. the noise) as if that variation represented underlying model structure.[[3](https://en.wikipedia.org/wiki/Overfitting#cite_note-BA2002-3)]\n",
    "\n",
    "To illustrate, let's see what happens with a degree-$20$ polynomial fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f324eb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "deg = 20\n",
    "A = np.stack([TV**i for i in range(deg+1)], axis=-1)\n",
    "coeff = mmids.ls_by_qr(A,sales)\n",
    "print(coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9141d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "saleshat = np.sum([coeff[i] * TVgrid**i for i in range(deg+1)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630015db",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.scatter(TV,sales,alpha=0.5)\n",
    "plt.plot(TVgrid, saleshat, 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1cf23e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We could use [cross-validation](https://www.textbook.ds100.org/ch/15/bias_cv.html) to choose a suitable degree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695bc0f4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We return to the linear case, but with the full set of predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5ecb3e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "radio = df['radio'].to_numpy()\n",
    "newspaper = df['newspaper'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700b551a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, sharex=False, sharey=True)\n",
    "ax1.scatter(radio,sales,alpha=0.5)\n",
    "ax1.set_title('radio')\n",
    "ax2.scatter(newspaper,sales,alpha=0.5)\n",
    "ax2.set_title('newspaper')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e07f29b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "A = np.stack((np.ones(n), TV, radio, newspaper), axis=-1)\n",
    "coeff = mmids.ls_by_qr(A,sales)\n",
    "print(coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de2f91c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Newspaper advertising (the last coefficient) seems to have a much weaker effect on sales per dollar spent. Next, we briefly sketch one way to assess the [statistical significance](https://en.wikipedia.org/wiki/Statistical_significance) of such a conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8212c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Our coefficients are estimated from a sample. There is intrinsic variability in our sampling procedure. We would like to understand how our estimated coefficients compare to the true coefficients. This is set up beautifully in [[Data8](https://www.inferentialthinking.com/chapters/13/2/Bootstrap.html), Section 13.2]:\n",
    "\n",
    "> A data scientist is using the data in a random sample to estimate an unknown parameter. She uses the sample to calculate the value of a statistic that she will use as her estimate. Once she has calculated the observed value of her statistic, she could just present it as her estimate and go on her merry way. But she's a data scientist. She knows that her random sample is just one of numerous possible random samples, and thus her estimate is just one of numerous plausible estimates. By how much could those estimates vary? To answer this, it appears as though she needs to draw another sample from the population, and compute a new estimate based on the new sample. But she doesn't have the resources to go back to the population and draw another sample. It looks as though the data scientist is stuck. Fortunately, a brilliant idea called *the bootstrap* can help her out. Since it is not feasible to generate new samples from the population, the bootstrap generates new random samples by a method called *resampling*: the new samples are drawn at random *from the original sample*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0788ee12",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Without going into full details (see [[DS100](http://www.textbook.ds100.org/ch/17/inf_pred_gen_boot.html), Section 17.3] for more), it works as follows. Let $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$ be our data. We assume that our sample is representative of the population and we simulate our sampling procedure by resampling from the sample. That is, we take a random sample with replacement $\\mathcal{X}_{\\mathrm{boot},1} = \\{(\\mathbf{x}_i, y_i)\\,:\\,i \\in I\\}$ where $I$ is a [multi-set](https://en.wikipedia.org/wiki/Multiset) of elements from $[n]$ of size $n$. We recompute our estimated coefficients on $\\mathcal{X}_{\\mathrm{boot},1}$. Then we repeat independently for a desired number of replicates $\\mathcal{X}_{\\mathrm{boot},1}, \\ldots, \\mathcal{X}_{\\mathrm{boot},r}$. Plotting a histogram of the resulting coefficients gives some idea of the variability of our estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afc0bcb",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We implement the bootstrap for linear regression in Python next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10b77a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def linregboot(A, b, replicates = np.int32(10000)):\n",
    "    n,m = A.shape\n",
    "    coeff_boot = np.zeros((m,replicates))\n",
    "    for i in range(replicates):\n",
    "        resample = rng.integers(0,n,n)\n",
    "        Aboot = A[resample,:]\n",
    "        bboot = b[resample]\n",
    "        coeff_boot[:,i] = mmids.ls_by_qr(Aboot,bboot)\n",
    "    return coeff_boot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e1f98a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "First, let's use a simple example from the lecture with a known ground truth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28149153",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n, b0, b1 = 100, -1, 1\n",
    "x = np.linspace(0,10,num=n)\n",
    "y = b0 + b1*x + rng.normal(0,1,n)\n",
    "A = np.stack((np.ones(n),x),axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc92d6b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The estimated coefficients are the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41d1f3a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "coeff = mmids.ls_by_qr(A,y)\n",
    "print(coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422bcfc8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Now we apply the bootstrap and plot histograms of the two coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad517c86",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "coeff_boot = linregboot(A,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40057570",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.hist(coeff_boot[0,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed24646",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.hist(coeff_boot[1,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55509cd3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We see in the histograms that the true coefficient values $-1$ and $1$ fall within the likely range. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e84efe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We return to the `Advertising` dataset and apply the bootstrap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb42cbf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n = np.size(TV)\n",
    "A = np.stack((np.ones(n), TV, radio, newspaper), axis=-1)\n",
    "coeff = mmids.ls_by_qr(A,sales)\n",
    "print(coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ded152c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "coeff_boot = linregboot(A,sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8958763e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Plotting a histogram of the coefficients corresponding to newspaper advertising shows that $0$ is a plausible value, while it is not for TV advertising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221f10d8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.hist(coeff_boot[1,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fc4e72",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.hist(coeff_boot[3,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c553ae55",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\newcommand{\\bfbeta}{\\boldsymbol{\\beta}}$ $\\newcommand{\\bflambda}{\\boldsymbol{\\lambda}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e64d2ce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## More advanced material: orthogonality in high dimension; linear independence lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a4aec5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We implement the algorithm above. In our naive implementation, we assume that $B$ is positive definite, and therefore that all steps are well-defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d41792",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def cholesky(B):\n",
    "    n = B.shape[0] \n",
    "    L = np.zeros((n, n))\n",
    "    for j in range(n):\n",
    "        L[j,0:j] = mmids.forwardsubs(L[0:j,0:j],B[j,0:j])\n",
    "        L[j,j] = np.sqrt(B[j,j] - LA.norm(L[j,0:j])**2)\n",
    "    return L "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698207b2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Here is a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192e93a3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "B = np.array([[2., 1.],[1., 2.]])\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2becc70d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "L = cholesky(B)\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbcd51b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We can check that it produces the right factorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa7eaff",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(L @ L.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8218913d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63596725",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We implement this algorithm below. In our naive implementation, we assume that $A$ has full column rank, and therefore that all steps are well-defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce835af9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def ls_by_chol(A, b):\n",
    "    L = cholesky(A.T @ A)\n",
    "    z = mmids.forwardsubs(L, A.T @ b)\n",
    "    return mmids.backsubs(L.T, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835238dd",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
