{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea2e1ae7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell",
     "colab-keep"
    ]
   },
   "source": [
    "***\n",
    "\n",
    "*Course:* [Math 535](https://people.math.wisc.edu/~roch/mmids/) - Mathematical Methods in Data Science (MMiDS)  \n",
    "*Chapter:* 3-Optimization theory and algorithms  \n",
    "*Author:* [Sebastien Roch](https://people.math.wisc.edu/~roch/), Department of Mathematics, University of Wisconsin-Madison  \n",
    "*Updated:* July 15, 2024   \n",
    "*Copyright:* &copy; 2024 Sebastien Roch\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f15d0e6",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# You will need the files:\n",
    "#     * mmids.py\n",
    "#     * customer_airline_satisfaction.csv\n",
    "#     * advertising.csv \n",
    "#     * SAHeart.csv \n",
    "# from https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/tree/main/utils\n",
    "#\n",
    "# IF RUNNING ON GOOGLE COLAB (RECOMMENDED):\n",
    "# \"Upload to session storage\" from the Files tab on the left\n",
    "# Alternative instructions: https://colab.research.google.com/notebooks/io.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f0395f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch\n",
    "import mmids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8650e0",
   "metadata": {
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\newcommand{\\bmu}{\\boldsymbol{\\mu}}$ \n",
    "$\\newcommand{\\bSigma}{\\boldsymbol{\\Sigma}}$\n",
    "$\\newcommand{\\bfbeta}{\\boldsymbol{\\beta}}$ \n",
    "$\\newcommand{\\bflambda}{\\boldsymbol{\\lambda}}$\n",
    "$\\newcommand{\\bgamma}{\\boldsymbol{\\gamma}}$ \n",
    "$\\newcommand{\\bsigma}{{\\boldsymbol{\\sigma}}}$\n",
    "$\\newcommand{\\bpi}{\\boldsymbol{\\pi}}$\n",
    "$\\newcommand{\\btheta}{{\\boldsymbol{\\theta}}}$ \n",
    "$\\newcommand{\\bphi}{\\boldsymbol{\\phi}}$ \n",
    "$\\newcommand{\\balpha}{\\boldsymbol{\\alpha}}$\n",
    "$\\newcommand{\\blambda}{\\boldsymbol{\\lambda}}$\n",
    "$\\renewcommand{\\P}{\\mathbb{P}}$\n",
    "$\\newcommand{\\E}{\\mathbb{E}}$\n",
    "$\\newcommand{\\indep}{\\perp\\!\\!\\!\\perp} \\newcommand{\\bx}{\\mathbf{x}}$ \n",
    "$\\newcommand{\\bp}{\\mathbf{p}}$\n",
    "$\\renewcommand{\\bx}{\\mathbf{x}}$\n",
    "$\\newcommand{\\bX}{\\mathbf{X}}$ \n",
    "$\\newcommand{\\by}{\\mathbf{y}}$ \n",
    "$\\newcommand{\\bY}{\\mathbf{Y}}$ \n",
    "$\\newcommand{\\bz}{\\mathbf{z}}$\n",
    "$\\newcommand{\\bZ}{\\mathbf{Z}}$ \n",
    "$\\newcommand{\\bw}{\\mathbf{w}}$ \n",
    "$\\newcommand{\\bW}{\\mathbf{W}}$\n",
    "$\\newcommand{\\bv}{\\mathbf{v}}$ \n",
    "$\\newcommand{\\bV}{\\mathbf{V}}$\n",
    "$\\newcommand{\\bfg}{\\mathbf{g}}$ \n",
    "$\\newcommand{\\bfh}{\\mathbf{h}}$ \n",
    "$\\newcommand{\\horz}{\\rule[.5ex]{2.5ex}{0.5pt}}$\n",
    "$\\renewcommand{\\S}{\\mathcal{S}}$ \n",
    "$\\newcommand{\\X}{\\mathcal{X}}$ \n",
    "$\\newcommand{\\var}{\\mathrm{Var}}$ \n",
    "$\\newcommand{\\pa}{\\mathrm{pa}}$\n",
    "$\\newcommand{\\Z}{\\mathcal{Z}}$ \n",
    "$\\newcommand{\\bh}{\\mathbf{h}}$ \n",
    "$\\newcommand{\\bb}{\\mathbf{b}}$ \n",
    "$\\newcommand{\\bc}{\\mathbf{c}}$ \n",
    "$\\newcommand{\\cE}{\\mathcal{E}}$ \n",
    "$\\newcommand{\\cP}{\\mathcal{P}}$\n",
    "$\\newcommand{\\bbeta}{\\boldsymbol{\\beta}}$\n",
    "$\\newcommand{\\bLambda}{\\boldsymbol{\\Lambda}}$\n",
    "$\\newcommand{\\cov}{\\mathrm{Cov}}$\n",
    "$\\newcommand{\\bfk}{\\mathbf{k}}$\n",
    "$\\newcommand{\\idx}[1]{}$\n",
    "$\\newcommand{\\xdi}{}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6822c45e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Motivating example:  analyzing customer satisfaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957589de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We now turn to classification$\\idx{classification}\\xdi$.\n",
    "\n",
    "Quoting [Wikipedia](https://en.wikipedia.org/wiki/Statistical_classification):\n",
    "\n",
    "> In machine learning and statistics, classification is the problem of identifying to which of a set of categories (sub-populations) a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known. Examples are assigning a given email to the \"spam\" or \"non-spam\" class, and assigning a diagnosis to a given patient based on observed characteristics of the patient (sex, blood pressure, presence or absence of certain symptoms, etc.). Classification is an example of pattern recognition. In the terminology of machine learning, classification is considered an instance of supervised learning, i.e., learning where a training set of correctly identified observations is available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c9ca26",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We will illustrate this problem on an [airline customer satisfaction](https://www.kaggle.com/datasets/sjleshrac/airlines-customer-satisfaction) dataset available on [Kaggle](https://www.kaggle.com), an excellent source of data and community contributed analyses. The background is the following: \n",
    "\n",
    "> The dataset consists of the details of customers who have already flown with them. The feedback of the customers on various context and their flight data has been consolidated. The main purpose of this dataset is to predict whether a future customer would be satisfied with their service given the details of the other parameters values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf94f5f0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We first load the data and convert it to an appropriate matrix representation. We (or, more precisely, ChatGPT) pre-processed the original file to remove rows with missing data or 0 ratings, convert categorical variables into one-hot encodings, and keep only a subset of the rows and columns. You can see the details of the pre-processing in this [chat history](https://chatgpt.com/share/c5070b9c-f33f-4a37-a793-fde0d7cb7b06). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c53ec41",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('customer_airline_satisfaction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af1fb16",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "This is a large dataset. Here are the first five rows and first 6 colums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410dc6ef",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(data.iloc[:5, :6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c752b5de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "It has 100,000 rows and 24 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02328d8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef86109d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The column names are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1151a408",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(data.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88538d6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The first column indicates whether a customer was satisfied (with `1` meaning satisfied). The next 6 columns give some information about the customers, e.g., their age or whether they are members of a loyalty program with the airline. The following three columns give information about the flight, with names that should be self-explanatory: `Flight Distance`, `Departure Delay in Minutes`, and `Arrival Delay in Minutes`. The remaining columns give the customers' ratings, between `1` and `5`, of various feature, e.g., `Baggage handling`, `Checkin service`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a593c1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Our goal will be to predict the first column, `Satisfied`, from the rest of the columns. For this, we transform our data into Numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f43da5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y = data['Satisfied'].to_numpy()\n",
    "X = data.drop(columns=['Satisfied']).to_numpy()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5891423c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92536643",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Some features may affect satisfication more than others. Let us look at age for instance. The following code extracts the `Age` column from `X` (i.e., column $0$) and computes the fraction of satisfied customers in several age bins.\n",
    "\n",
    "Explanation by ChatGPT (who wrote the code):\n",
    "\n",
    "1. [`numpy.digitize`](https://numpy.org/doc/stable/reference/generated/numpy.digitize.html) bins the age data into the specified age bins. The `-1` adjustment is to match zero-based indexing.\n",
    "2. [`numpy.bincount`](https://numpy.org/doc/stable/reference/generated/numpy.bincount.html) counts the occurrences of each bin index. The `minlength` parameter ensures that the resulting array length matches the number of age bins (`age_labels`). This is important if some bins have zero counts, ensuring the counts array covers all bins.\n",
    "3. `freq_satisfied = counts_satisfied / counts_all` calculates the satisfaction frequency for each age group by dividing the counts of satisfied customers by the total counts in each age group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8d3708",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "age_col_index = 0\n",
    "age_data = X[:, age_col_index]\n",
    "age_bins = [0, 18, 25, 35, 45, 55, 65, 100]\n",
    "age_labels = ['0-17', '18-24', '25-34', '35-44', '45-54', '55-64', '65+']\n",
    "age_bin_indices = np.digitize(age_data, bins=age_bins) - 1\n",
    "counts_all = np.bincount(age_bin_indices, minlength=len(age_labels))\n",
    "counts_satisfied = np.bincount(age_bin_indices[y == 1], minlength=len(age_labels))\n",
    "freq_satisfied = counts_satisfied / counts_all\n",
    "age_group_labels = np.array(age_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8624eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The results are plotted using matplotlib's [`matplotlib.pyplot.bar`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.bar.html) function. We see in particular that younger people tend to be more dissatisfied. Of course, this might be because they cannot afford the most expensive services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2a30aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.bar(age_group_labels, freq_satisfied, color='lightblue', edgecolor='black')\n",
    "plt.xlabel('Age Group'), plt.ylabel('Frequency of Satisfied Customers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b194b2ab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The input data is now of the form $\\{(\\mathbf{x}_i, y_i) : i=1,\\ldots, n\\}$ where $\\mathbf{x}_i \\in \\mathbb{R}^d$ are the features and $y_i \\in \\{0,1\\}$ is the label. Above we use the matrix representation $X \\in \\mathbb{R}^{d \\times n}$ with columns $\\mathbf{x}_i$, $i = 1,\\ldots, n$ and $\\mathbf{y} = (y_1, \\ldots, y_n)^T \\in \\{0,1\\}^n$. \n",
    "\n",
    "Our goal: \n",
    "\n",
    "> learn a classifier from the examples $\\{(\\mathbf{x}_i, y_i) : i=1,\\ldots, n\\}$, that is, a function $\\hat{f} : \\mathbb{R}^d \\to \\mathbb{R}$ such that $\\hat{f}(\\mathbf{x}_i) \\approx y_i$.\n",
    "\n",
    "We may want to enforce that the output is in $\\{0,1\\}$ as well. This problem is referred to as [binary classification](https://en.wikipedia.org/wiki/Binary_classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfcfc60",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "A natural approach to this type of [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning)$\\idx{supervised learning}\\xdi$ problem is to define two objects:\n",
    "\n",
    "1. **Family of classifiers:** A class $\\widehat{\\mathcal{F}}$ of classifiers from which to pick $\\hat{f}$.\n",
    "\n",
    "2. **Loss function:** A loss function $\\ell(\\hat{f}, (\\mathbf{x},y))$ which quantifies how good of a fit $\\hat{f}(\\mathbf{x})$ is to $y$.\n",
    "\n",
    "Our goal is then to solve\n",
    "\n",
    "$$\n",
    "\\min_{\\hat{f} \\in \\widehat{\\mathcal{F}}} \\frac{1}{n} \\sum_{i=1}^n \\ell(\\hat{f}, (\\mathbf{x}_i, y_i)),\n",
    "$$\n",
    "\n",
    "that is, we seek to find a classifier among $\\widehat{\\mathcal{F}}$ that minimizes the average loss over the examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43d4aee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "For instance, in [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression), we consider linear classifiers of the form\n",
    "\n",
    "$$\n",
    "\\hat{f}(\\mathbf{x})\n",
    "= \\sigma(\\mathbf{x}^T \\boldsymbol{\\theta})\n",
    "\\qquad\n",
    "\\text{with}\n",
    "\\qquad\n",
    "\\sigma(t) = \\frac{1}{1 + e^{-t}}\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\theta} \\in \\mathbb{R}^d$ is a parameter vector. And we use the [cross-entropy loss](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression)\n",
    "\n",
    "$$\n",
    "\\ell(\\hat{f}, (\\mathbf{x}, y))\n",
    "= -  y \\log(\\sigma(\\mathbf{x}^T \\boldsymbol{\\theta}))\n",
    "- (1-y) \\log(1- \\sigma(\\mathbf{x}^T \\boldsymbol{\\theta})).\n",
    "$$\n",
    "\n",
    "In parametric form, the problem boils down to\n",
    "\n",
    "$$\n",
    "\\min_{\\boldsymbol{\\theta} \\in \\mathbb{R}^d}\n",
    "- \\frac{1}{n} \\sum_{i=1}^n y_i \\log(\\sigma(\\mathbf{x}_i^T \\boldsymbol{\\theta}))\n",
    "- \\frac{1}{n} \\sum_{i=1}^n (1-y_i) \\log(1- \\sigma(\\mathbf{x}_i^T \\boldsymbol{\\theta})).\n",
    "$$\n",
    "\n",
    "To obtain a prediction in $\\{0,1\\}$ here, we could cutoff $\\hat{f}(\\mathbf{x})$ at a threshold $\\tau \\in [0,1]$, that is, return $\\mathbf{1}\\{\\hat{f}(\\mathbf{x}) > \\tau\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69854e61",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We will explain in a later chapter where this choice comes from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f905cd",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The purpose of this chapter is to develop some of the mathematical theory and algorithms needed to solve this type of optimization formulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d435b404",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**CHAT & LEARN** Ask your favorite AI chatbot to help you explore the following hypothesis about this dataset: \n",
    "\n",
    "> Younger people tend to be more dissatisfied because they cannot afford the best services.\n",
    "\n",
    "For example, consider the relationship between age, satisfaction, and class (e.g., economy, business, etc.). Specifically:\n",
    "\n",
    "1. Compare the distribution of class types among different age groups.\n",
    "\n",
    "2. Compare the satisfaction levels within each class type for different age groups.\n",
    "\n",
    "([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb)) $\\ddagger$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d27c383",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Optimality conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8826bc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** Consider $f(x) = e^x$. Then $f'(x) = f''(x) = e^x$. Suppose we are interested in approximating $f$ in the interval $[0,1]$. We take $a=0$ and $b=1$ in *Taylor's Theorem*. The linear term is \n",
    "\n",
    "$$\n",
    "f(a) + (x-a) f'(a) = 1 + x e^0 = 1 + x.\n",
    "$$\n",
    "\n",
    "Then for any $x \\in [0,1]$\n",
    "\n",
    "$$\n",
    "f(x) = 1 + x + \\frac{1}{2}x^2 e^{\\xi_x}\n",
    "$$\n",
    "\n",
    "where $\\xi_x \\in (0,1)$ depends on $x$. We get a uniform bound on the error over $[0,1]$ by replacing $\\xi_x$ with its worst possible value over $[0,1]$ \n",
    "\n",
    "$$\n",
    "|f(x) - (1+x)| \\leq \\frac{1}{2}x^2 e^{\\xi_x} \\leq \\frac{e}{2} x^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eabea3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0,1,100)\n",
    "y = np.exp(x)\n",
    "taylor = 1 + x\n",
    "err = (np.exp(1)/2) * x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2d8ab7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "If we plot the upper and lower bounds, we see that $f$ indeed falls within them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73f626f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(x,y,label='f')\n",
    "plt.plot(x,taylor,label='taylor')\n",
    "plt.plot(x,taylor-err,linestyle=':',color='green',label='lower')\n",
    "plt.plot(x,taylor+err,linestyle='--',color='green',label='upper')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14063633",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246c0e10",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** Let $f(x) = x^3$. Then $f'(x) = 3 x^2$ and $f''(x) = 6 x$ so that $f'(0) = 0$ and $f''(0) \\geq 0$. Hence $x=0$ is a stationary point. But $x=0$ is not a local minimizer. Indeed $f(0) = 0$ but, for any $\\delta > 0$, $f(-\\delta) < 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75d25fc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-2,2,100)\n",
    "y = x**3\n",
    "\n",
    "plt.plot(x,y, c='k')\n",
    "plt.ylim(-5,5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74b852d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2c22bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** Returning to the previous example, the points satisfying $h_1(\\mathbf{x}) = 0$ sit on the circle of radius $1$ around the origin. We have already seen that \n",
    "\n",
    "$$\n",
    "\\nabla h_1(\\mathbf{x})\n",
    "= \\left(\n",
    "\\frac{\\partial h_1(\\mathbf{x})}{\\partial x_1},\n",
    "\\frac{\\partial h_1(\\mathbf{x})}{\\partial x_2}\n",
    "\\right)\n",
    "= (- 2 x_1, - 2 x_2).\n",
    "$$\n",
    "\n",
    "Here is code illustrating the theorem (with help from ChatGPT). We first compute the function $h_1$ at a grid of points using [`numpy.meshgrid`](https://numpy.org/doc/stable/reference/generated/numpy.meshgrid.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b25bb5b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def h1(x1, x2):\n",
    "    return 1 - x1**2 - x2**2\n",
    "\n",
    "x1, x2 = np.linspace(-1.5, 1.5, 400), np.linspace(-1.5, 1.5, 400)\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "H1 = h1(X1, X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6e1f70",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We use [`matplotlib.pyplot.contour`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.contour.html) to plot the constraint set as a [contour line](https://en.wikipedia.org/wiki/Contour_line) (for the constant value $0$) of $h_1$. Gradients of $h_1$ are plotted at a collection of `points` with the [`matplotlib.pyplot.quiver`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.quiver.html) function, which is used for plotting vectors as arrows. We see that the directions of first-order feasible directions are orthogonal to the arrows, and therefore are tangent to the constraint set. \n",
    "\n",
    "At those same `points`, we also plot the gradient of $f$, which recall is\n",
    "\n",
    "$$\n",
    "\\nabla f(\\mathbf{x})\n",
    "= \\left(\n",
    "\\frac{\\partial f(\\mathbf{x})}{\\partial x_1},\n",
    "\\frac{\\partial f(\\mathbf{x})}{\\partial x_2}\n",
    "\\right)\n",
    "= (4 x_1, 6 x_2).\n",
    "$$\n",
    "\n",
    "We make all gradients into unit vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a5b346",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.contour(X1, X2, H1, levels=[0], colors='b')\n",
    "points = [(0.5, np.sqrt(3)/2), (-0.5, np.sqrt(3)/2), (0.5, -np.sqrt(3)/2),\n",
    "    (-0.5, -np.sqrt(3)/2), (1, 0), (-1, 0), (0, 1), (0, -1)]\n",
    "for x1, x2 in points:\n",
    "    plt.quiver(x1, x2, -x1/np.sqrt(x1**2 + x2**2), -x2/np.sqrt(x1**2 + x2**2), \n",
    "               scale=10, color='r')\n",
    "    plt.quiver(x1, x2, 4*x1/np.sqrt(16 * x1**2 + 36 * x2**2), \n",
    "               6*x2/np.sqrt(16 * x1**2 + 36 * x2**2), scale=10, color='lime')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85edf131",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We see that, at $(-1,0)$ and $(1,0)$, the gradient is indeed orthogonal to the first-order feasible directions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd39ffd",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e56c4a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Gradient descent and its convergence analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b89203",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "At each iteration of gradient descent, we take a step in the direction of the negative of the gradient, that is,\n",
    "\n",
    "$$\n",
    "\\mathbf{x}^{t+1}\n",
    "= \\mathbf{x}^t - \\alpha_t \\nabla f(\\mathbf{x}^t),\n",
    "\\quad t=0,1,2\\ldots\n",
    "$$\n",
    "\n",
    "for a sequence of step sizes $\\alpha_t > 0$. Choosing the right step size (also known as steplength or learning rate) is a large subject in itself. We will only consider the case of fixed step size here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2af9e6",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**CHAT & LEARN** Ask your favorite AI chatbot about the different approaches for selecting a step size in gradient descent methods. $\\ddagger$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b987f5cb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "In general, we will not be able to guarantee that a global minimizer is reached in the limit, even if one exists. Our goal for now is more modest: to find a point where the gradient of $f$ approximately vanishes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f29fd73",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We implement gradient descent$\\idx{gradient descent}\\xdi$ in Python. We assume that a function `f` and its gradient `grad_f` are provided. We first code the basic steepest descent step with a step size$\\idx{step size}\\xdi$ $\\idx{learning rate}\\xdi$ $\\alpha =$ `alpha`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12caab46",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def desc_update(grad_f, x, alpha):\n",
    "    return x - alpha*grad_f(x)\n",
    "\n",
    "def gd(f, grad_f, x0, alpha=1e-3, niters=int(1e6)):\n",
    "    \n",
    "    xk = x0\n",
    "    for _ in range(niters):\n",
    "        xk = desc_update(grad_f, xk, alpha)\n",
    "\n",
    "    return xk, f(xk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f254982",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We illustrate on a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af76e09",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def f(x): \n",
    "    return (x-1)**2 + 10\n",
    "\n",
    "def grad_f(x):\n",
    "    return 2*(x-1)\n",
    "\n",
    "xgrid = np.linspace(-5,5,100)\n",
    "plt.plot(xgrid, f(xgrid), label='f')\n",
    "plt.plot(xgrid, grad_f(xgrid), label='grad_f')\n",
    "plt.ylim((-20,50)), plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b0adec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "gd(f, grad_f, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b54a67c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We found a global minmizer in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c528cd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The next example shows that a different local minimizer may be reached depending on the starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fac3318",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def f(x): \n",
    "    return 4 * (x-1)**2 * (x+1)**2 - 2*(x-1)\n",
    "\n",
    "def grad_f(x): \n",
    "    return 8 * (x-1) * (x+1)**2 + 8 * (x-1)**2 * (x+1) - 2\n",
    "\n",
    "xgrid = np.linspace(-2,2,100)\n",
    "plt.plot(xgrid, f(xgrid), label='f')\n",
    "plt.plot(xgrid, grad_f(xgrid), label='grad_f')\n",
    "plt.ylim((-10,10)), plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1755e4dc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "gd(f, grad_f, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d848e6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gd(f, grad_f, -2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afeff14d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**TRY IT!** In this last example, does changing the step size affect the outcome? ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb)) $\\ddagger$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d63fe2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "In the final example, we end up at a stationary point that is not a local minimizer. Here both the first and second derivatives are zero. This is known as a [saddle point](https://en.wikipedia.org/wiki/Saddle_point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a0349e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**3\n",
    "\n",
    "def grad_f(x):\n",
    "    return 3 * x**2\n",
    "\n",
    "xgrid = np.linspace(-2,2,100)\n",
    "plt.plot(xgrid, f(xgrid), label='f')\n",
    "plt.plot(xgrid, grad_f(xgrid), label='grad_f')\n",
    "plt.ylim((-10,10)), plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399461c7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "gd(f, grad_f, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af66f1f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gd(f, grad_f, -2, niters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237edb04",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9f0dfe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We revisit our first simple single-variable example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0198749a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def f(x): \n",
    "    return (x-1)**2 + 10\n",
    "\n",
    "def grad_f(x):\n",
    "    return 2*(x-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a55ead",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The second derivative is $f''(x) = 2$. Hence, this $f$ is $L$-smooth and $m$-strongly convex with $L = m = 2$. The theory we developed suggests taking step size $\\alpha_t = \\alpha = 1/L = 1/2$. It also implies that\n",
    "\n",
    "$$\n",
    "f(x^1) - f(x^*)\n",
    "\\leq \\left(1 - \\frac{m}{L}\\right) [f(x^0) - f(x^*)]\n",
    "= 0.\n",
    "$$\n",
    "\n",
    "We converge in one step! And that holds for any starting point $x^0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e023a68b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Let's try this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1993ca",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gd(f, grad_f, 0, alpha=0.5, niters=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccc9a4d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Let's try a different starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b844c6b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gd(f, grad_f, 100, alpha=0.5, niters=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989bb467",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3c49c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Application: logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1dfbb4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We plot the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbfc168",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "outputs": [],
   "source": [
    "def sigmoid(z): \n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "grid = np.linspace(-5, 5, 100)\n",
    "plt.plot(grid, sigmoid(grid), c='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe09f2b3",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634657b3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We modify our implementation of gradient descent to take a dataset as input. Recall that to run gradient descent, we first implement a function computing a descent update. It takes as input a function `grad_fn` computing the gradient itself, as well as a current iterate and a step size. We now also feed a dataset as additional input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ddd71f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def desc_update_for_logreg(grad_fn, A, b, curr_x, beta):\n",
    "    gradient = grad_fn(curr_x, A, b)\n",
    "    return curr_x - beta*gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431c532e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We are ready to implement GD. Our function takes as input a function `loss_fn` computing the objective, a function `grad_fn` computing the gradient, the dataset `A` and `b`, and an initial guess `init_x`. Optional parameters are the step size and the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca488cd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def gd_for_logreg(loss_fn, grad_fn, A, b, init_x, beta=1e-3, niters=int(1e5)):\n",
    "    curr_x = init_x\n",
    "    \n",
    "    for iter in range(niters):\n",
    "        curr_x = desc_update_for_logreg(grad_fn, A, b, curr_x, beta)\n",
    "    \n",
    "    return curr_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4666eceb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To implement `loss_fn` and `grad_fn`, we define the sigmoid as above. Below, `pred_fn` is $\\bsigma(A \\mathbf{x})$. Here we write the loss function as\n",
    "\n",
    "\\begin{align*}\n",
    "\\ell(\\mathbf{x}; A, \\mathbf{b})\n",
    "&= \\frac{1}{n} \\sum_{i=1}^n \\left\\{- b_i \\log(\\sigma(\\boldsymbol{\\alpha_i}^T \\mathbf{x}))\n",
    "- (1-b_i) \\log(1- \\sigma(\\boldsymbol{\\alpha_i}^T \\mathbf{x}))\\right\\}\\\\\n",
    "&= \\mathrm{mean}\\left(-\\mathbf{b} \\odot \\mathbf{log}(\\bsigma(A \\mathbf{x})) - (\\mathbf{1} - \\mathbf{b}) \\odot \\mathbf{log}(\\mathbf{1} - \\bsigma(A \\mathbf{x}))\\right),\n",
    "\\end{align*}\n",
    "\n",
    "where $\\odot$ is the Hadamard product, or element-wise product (for example $\\mathbf{u} \\odot \\mathbf{v} = (u_1 v_1, \\ldots,u_n v_n)^T$), the logarithm (denoted in bold) is applied element-wise and $\\mathrm{mean}(\\mathbf{z})$ is the mean of the entries of $\\mathbf{z}$ (i.e., $\\mathrm{mean}(\\mathbf{z}) = n^{-1} \\sum_{i=1}^n z_i$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3a00b2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def pred_fn(x, A): \n",
    "    return sigmoid(A @ x)\n",
    "\n",
    "def loss_fn(x, A, b): \n",
    "    return np.mean(-b*np.log(pred_fn(x, A)) - (1 - b)*np.log(1 - pred_fn(x, A)))\n",
    "\n",
    "def grad_fn(x, A, b):\n",
    "    return -A.T @ (b - pred_fn(x, A))/len(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad404dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We can choose a step size based on the smoothness of the objective as above. Recall that [`numpy.linalg.norm`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html) computes the Frobenius norm by default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465df7ea",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def stepsize_for_logreg(A, b):\n",
    "    L = LA.norm(A)**2 / (4 * len(b))\n",
    "    return 1/L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeec4e0c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We return to our original motivation, the [airline customer satisfaction](https://www.kaggle.com/datasets/sjleshrac/airlines-customer-satisfaction) dataset. We first load the dataset. We will need the column names later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0e8dbb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('customer_airline_satisfaction.csv')\n",
    "column_names = data.columns.tolist()\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c957e763",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Our goal will be to predict the first column, `Satisfied`, from the rest of the columns. For this, we transform our data into Numpy arrays. We also standardize the columns by subtracting their mean and dividing by their standard deviation. This will allow to compare the influence of different features on the prediction. And we add a column of 1s to to account for the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aff1e9b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y = data['Satisfied'].to_numpy()\n",
    "X = data.drop(columns=['Satisfied']).to_numpy()\n",
    "\n",
    "means = np.mean(X, axis=0)\n",
    "stds = np.std(X, axis=0)\n",
    "X_standardized = (X - means) / stds\n",
    "\n",
    "A = np.concatenate((np.ones((len(y),1)), X_standardized), axis=1)\n",
    "b = y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ecfb99",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We use the functions `loss_fn` and `grad_fn` which were written for general logistic regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd30fbfc",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "init_x = np.zeros(A.shape[1])\n",
    "best_x = gd_for_logreg(loss_fn, grad_fn, A, b, init_x, beta=1e-3, niters=int(1e3))\n",
    "print(best_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5f376b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To interpret the results, we plot the coefficients in decreasing order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce33e73",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "coefficients, features = best_x[1:], column_names[1:]\n",
    "\n",
    "sorted_indices = np.argsort(coefficients)\n",
    "sorted_coefficients = coefficients[sorted_indices]\n",
    "sorted_features = np.array(features)[sorted_indices]\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.barh(sorted_features, sorted_coefficients, color='lightblue', edgecolor='black')\n",
    "plt.xlabel('Coefficient Value'), plt.title('Logistic Regression Coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caef6495",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We see from the first ten bars or so that, as might be expected, higher ratings on various aspects of the flight generally contribute to a higher predicted likelihood of satisfaction (with one exception being `Gate location` whose coefficient is negative but may not be [statistically significant](https://en.wikipedia.org/wiki/Statistical_significance)). `Inflight entertainment` seems particularly influential. `Age` also shows the same pattern, something we had noticed in the introductory section through a different analysis. On the other hand, departure delay and arrival delay contribute to a lower predicted likelihood of satisfaction, again an expected pattern. The most negative influence however appears to come from `Class_Eco`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5538f68",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**CHAT & LEARN** There are faster methods for logistic regression. Ask your favorite AI chatbot for an explanation and implementation of the iteratively reweighted least squares method. Try it on this dataset. ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb)) $\\ddagger$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ca0c35",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**TRY IT!** One can attempt to predict whether a new customer, whose feature vector is $\\boldsymbol{\\alpha}$, will be satisfied by using the prediction function $p(\\mathbf{x}; \\boldsymbol{\\alpha}) = \\sigma(\\boldsymbol{\\alpha}^T \\mathbf{x})$, where $\\mathbf{x}$ is the fitted coefficients. Say a customer is predicted to be satisfied if $\\sigma(\\boldsymbol{\\alpha}^T \\mathbf{x}) > 0.5$. Implement this predictor and compute its accuracy on this dataset. ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2f500f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**CHAT & LEARN** Because of the issue of overfitting, computing the accuracy of a predictor on a dataset used to estimate the coefficients is problematic. Ask your favorite AI chatbot about scikit-learn's `train_test_split` function and how it helps resolve this issue. Implement it on this dataset. ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb)) $\\ddagger$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcec0fb5",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
