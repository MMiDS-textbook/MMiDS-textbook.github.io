
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>6.7. Online supplementary material &#8212; MMiDS Textbook</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=87e54e7c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-P5E8DW088F"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-P5E8DW088F');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-P5E8DW088F');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chap06_prob/supp/roch-mmids-prob-supp';</script>
    <link rel="canonical" href="https://mmids-textbook.github.io/chap06_prob/supp/roch-mmids-prob-supp.html" />
    <link rel="icon" href="https://mmids-textbook.github.io/favicon-32x32.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="7. Random walks on graphs and Markov chains" href="../../chap07_rwmc/00_intro/roch-mmids-rwmc-intro.html" />
    <link rel="prev" title="6.6. Exercises" href="../exercises/roch-mmids-prob-exercises.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/mmids-cover-small.jpg" class="logo__image only-light" alt="MMiDS Textbook - Home"/>
    <script>document.write(`<img src="../../_static/mmids-cover-small.jpg" class="logo__image only-dark" alt="MMiDS Textbook - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../index.html">
                    <b>MATHEMATICAL METHODS in DATA SCIENCE</b>
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap01_intro/00_intro/roch-mmids-intro-intro.html">1. Introduction: a first data science problem</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap01_intro/01_motiv/roch-mmids-intro-motiv.html">1.1. Motivating example: identifying penguin species</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap01_intro/02_review/roch-mmids-intro-review.html">1.2. Background: quick refresher of matrix algebra, differential calculus, and elementary probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap01_intro/03_clustering/roch-mmids-intro-clustering.html">1.3. Clustering: an objective, an algorithm and a guarantee</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap01_intro/04_highdim/roch-mmids-intro-highdim.html">1.4. Some observations about high-dimensional data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap01_intro/exercises/roch-mmids-intro-exercises.html">1.5. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap01_intro/supp/roch-mmids-intro-supp.html">1.6. Online supplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap02_ls/00_intro/roch-mmids-ls-intro.html">2. Least squares: geometric, algebraic, and numerical aspects</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/01_motiv/roch-mmids-ls-motiv.html">2.1. Motivating example: predicting sales</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/02_spaces/roch-mmids-ls-spaces.html">2.2. Background: review of vector spaces and matrix inverses</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/03_orthog/roch-mmids-ls-orthog.html">2.3. Geometry of least squares: the orthogonal projection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/04_qr/roch-mmids-ls-qr.html">2.4. QR decomposition and Householder transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/05_regression/roch-mmids-ls-regression.html">2.5. Application: regression analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/exercises/roch-mmids-ls-exercises.html">2.6. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/supp/roch-mmids-ls-supp.html">2.7. Online supplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap03_opt/00_intro/roch-mmids-opt-intro.html">3. Optimization theory and algorithms</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap03_opt/01_motiv/roch-mmids-opt-motiv.html">3.1. Motivating example:  analyzing customer satisfaction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap03_opt/02_several/roch-mmids-opt-several.html">3.2. Background: review of differentiable functions of several variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap03_opt/03_optimality/roch-mmids-opt-optimality.html">3.3. Optimality conditions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap03_opt/04_convexity/roch-mmids-opt-convexity.html">3.4. Convexity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap03_opt/05_gd/roch-mmids-opt-gd.html">3.5. Gradient descent and its convergence analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap03_opt/06_logistic/roch-mmids-opt-logistic.html">3.6. Application: logistic regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap03_opt/exercises/roch-mmids-opt-exercises.html">3.7. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap03_opt/supp/roch-mmids-opt-supp.html">3.8. Online supplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap04_svd/00_intro/roch-mmids-svd-intro.html">4. Singular value decomposition</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_svd/01_motiv/roch-mmids-svd-motiv.html">4.1. Motivating example: visualizing viral evolution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_svd/02_spectral/roch-mmids-svd-spectral.html">4.2. Background: review of matrix rank  and spectral decomposition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_svd/03_svd/roch-mmids-svd-svd.html">4.3. Approximating subspaces and the SVD</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_svd/04_power/roch-mmids-svd-power.html">4.4. Power iteration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_svd/05_pca/roch-mmids-svd-pca.html">4.5. Application: principal components analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_svd/06_further/roch-mmids-svd-further.html">4.6. Further applications of the SVD: low-rank approximations and ridge regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_svd/exercises/roch-mmids-svd-exercises.html">4.7. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_svd/supp/roch-mmids-svd-supp.html">4.8. Online supplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap05_specgraph/00_intro/roch-mmids-specgraph-intro.html">5. Spectral graph theory</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap05_specgraph/01_motiv/roch-mmids-specgraph-motiv.html">5.1. Motivating example: uncovering social groups</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap05_specgraph/02_graph/roch-mmids-specgraph-graph.html">5.2. Background: basic concepts in graph theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap05_specgraph/03_extremal/roch-mmids-specgraph-extremal.html">5.3. Variational characterization of eigenvalues</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap05_specgraph/04_laplacian/roch-mmids-specgraph-laplacian.html">5.4. Spectral properties of the Laplacian matrix</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap05_specgraph/05_partitioning/roch-mmids-specgraph-partitioning.html">5.5. Application: graph partitioning via spectral clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap05_specgraph/06_sbm/roch-mmids-specgraph-sbm.html">5.6. Erdős-Rényi random graph and stochastic blockmodel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap05_specgraph/exercises/roch-mmids-specgraph-exercises.html">5.7. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap05_specgraph/supp/roch-mmids-specgraph-supp.html">5.8. Online supplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../00_intro/roch-mmids-prob-intro.html">6. Probabilistic models: from simple to complex</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../01_motiv/roch-mmids-prob-motiv.html">6.1. Motivating example: tracking location</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02_parametric/roch-mmids-prob-parametric.html">6.2. Background: introduction to parametric families and maximum likelihood estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03_joint/roch-mmids-prob-joint.html">6.3. Modeling more complex dependencies 1: using conditional independence</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04_em/roch-mmids-prob-em.html">6.4. Modeling more complex dependencies 2: marginalizing out an unobserved variable</a></li>
<li class="toctree-l2"><a class="reference internal" href="../05_kalman/roch-mmids-prob-kalman.html">6.5. Application: linear-Gaussian models and Kalman filtering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../exercises/roch-mmids-prob-exercises.html">6.6. Exercises</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">6.7. Online supplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap07_rwmc/00_intro/roch-mmids-rwmc-intro.html">7. Random walks on graphs and Markov chains</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap07_rwmc/01_motiv/roch-mmids-rwmc-motiv.html">7.1. Motivating example: discovering mathematical topics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap07_rwmc/02_mcdefs/roch-mmids-rwmc-mcdefs.html">7.2. Background: elements of finite Markov chains</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap07_rwmc/03_stat/roch-mmids-rwmc-stat.html">7.3. Limit behavior 1: stationary distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap07_rwmc/04_mclimit/roch-mmids-rwmc-mclimit.html">7.4. Limit behavior 2: convergence to equilibrium</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap07_rwmc/05_pagerank/roch-mmids-rwmc-pagerank.html">7.5. Application: random walks on graphs and PageRank</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap07_rwmc/06_gibbs/roch-mmids-rwmc-gibbs.html">7.6. Further applications: Gibbs sampling and generating images</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap07_rwmc/exercises/roch-mmids-rwmc-exercises.html">7.7. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap07_rwmc/supp/roch-mmids-rwmc-supp.html">7.8. Online supplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap08_nn/00_intro/roch-mmids-nn-intro.html">8. Neural networks, backpropagation and stochastic gradient descent</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/01_motiv/roch-mmids-nn-motiv.html">8.1. Motivating example:  classifying natural images</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/02_chain/roch-mmids-nn-chain.html">8.2. Background: Jacobian, chain rule, and a brief introduction to automatic differentiation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/03_backprop/roch-mmids-nn-backprop.html">8.3. Building blocks of AI 1: backpropagation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/04_sgd/roch-mmids-nn-sgd.html">8.4. Building blocks of AI 2: stochastic gradient descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/05_nn/roch-mmids-nn-nn.html">8.5. Building blocks of AI 3: neural networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/exercises/roch-mmids-nn-exercises.html">8.6. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/supp/roch-mmids-nn-supp.html">8.7. Online supplementary material</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/issues/new?title=Issue%20on%20page%20%2Fchap06_prob/supp/roch-mmids-prob-supp.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/chap06_prob/supp/roch-mmids-prob-supp.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Online supplementary material</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quizzes-solutions-code-etc">6.7.1. Quizzes, solutions, code, etc.</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#just-the-code">6.7.1.1. Just the code</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#self-assessment-quizzes">6.7.1.2. Self-assessment quizzes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#auto-quizzes">6.7.1.3. Auto-quizzes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#solutions-to-odd-numbered-warm-up-exercises">6.7.1.4. Solutions to odd-numbered warm-up exercises</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-outcomes">6.7.1.5. Learning outcomes</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-sections">6.7.2. Additional sections</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sentiment-analysis">6.7.2.1. Sentiment analysis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kalman-filtering-missing-data">6.7.2.2. Kalman filtering: missing data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cholesky-decomposition">6.7.2.3. Cholesky decomposition</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <p><span class="math notranslate nohighlight">\(\newcommand{\P}{\mathbb{P}}\)</span> <span class="math notranslate nohighlight">\(\newcommand{\E}{\mathbb{E}}\)</span> <span class="math notranslate nohighlight">\(\newcommand{\S}{\mathcal{S}}\)</span> <span class="math notranslate nohighlight">\(\newcommand{\var}{\mathrm{Var}}\)</span> <span class="math notranslate nohighlight">\(\newcommand{\bmu}{\boldsymbol{\mu}}\)</span> <span class="math notranslate nohighlight">\(\newcommand{\bSigma}{\boldsymbol{\Sigma}}\)</span> <span class="math notranslate nohighlight">\(\newcommand{\btheta}{\boldsymbol{\theta}}\)</span> <span class="math notranslate nohighlight">\(\newcommand{\bpi}{\boldsymbol{\pi}}\)</span> <span class="math notranslate nohighlight">\(\newcommand{\indep}{\perp\!\!\!\perp}\)</span> <span class="math notranslate nohighlight">\(\newcommand{\bp}{\mathbf{p}}\)</span> <span class="math notranslate nohighlight">\(\newcommand{\bx}{\mathbf{x}}\)</span> <span class="math notranslate nohighlight">\(\newcommand{\bX}{\mathbf{X}}\)</span> <span class="math notranslate nohighlight">\(\newcommand{\by}{\mathbf{y}}\)</span> <span class="math notranslate nohighlight">\(\newcommand{\bY}{\mathbf{Y}}\)</span> <span class="math notranslate nohighlight">\(\newcommand{\bz}{\mathbf{z}}\)</span> <span class="math notranslate nohighlight">\(\newcommand{\bZ}{\mathbf{Z}}\)</span> <span class="math notranslate nohighlight">\(\newcommand{\bw}{\mathbf{w}}\)</span> <span class="math notranslate nohighlight">\(\newcommand{\bW}{\mathbf{W}}\)</span> <span class="math notranslate nohighlight">\(\newcommand{\bv}{\mathbf{v}}\)</span> <span class="math notranslate nohighlight">\(\newcommand{\bV}{\mathbf{V}}\)</span> <span class="math notranslate nohighlight">\(\newcommand{\bfbeta}{\boldsymbol{\beta}}\)</span> <span class="math notranslate nohighlight">\(\newcommand{\bflambda}{\boldsymbol{\lambda}}\)</span> <span class="math notranslate nohighlight">\(\newcommand{\cov}{\mathrm{Cov}}\)</span></p>
<section id="online-supplementary-material">
<h1><span class="section-number">6.7. </span>Online supplementary material<a class="headerlink" href="#online-supplementary-material" title="Link to this heading">#</a></h1>
<section id="quizzes-solutions-code-etc">
<h2><span class="section-number">6.7.1. </span>Quizzes, solutions, code, etc.<a class="headerlink" href="#quizzes-solutions-code-etc" title="Link to this heading">#</a></h2>
<section id="just-the-code">
<h3><span class="section-number">6.7.1.1. </span>Just the code<a class="headerlink" href="#just-the-code" title="Link to this heading">#</a></h3>
<p>An interactive Jupyter notebook featuring the code in this chapter can be accessed below (Google Colab recommended). You are encouraged to tinker with it. Some suggested computational exercises are scattered throughout. The notebook is also available as a slideshow.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb">Notebook</a> (<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb">Open In Colab</a>)</p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_prob_notebook_slides.slides.html">Slideshow</a></p></li>
</ul>
</section>
<section id="self-assessment-quizzes">
<h3><span class="section-number">6.7.1.2. </span>Self-assessment quizzes<a class="headerlink" href="#self-assessment-quizzes" title="Link to this heading">#</a></h3>
<p>A more extensive web version of the self-assessment quizzes is available by following the links below.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_6_2.html">Section 6.2</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_6_3.html">Section 6.3</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_6_4.html">Section 6.4</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_6_5.html">Section 6.5</a></p></li>
</ul>
</section>
<section id="auto-quizzes">
<h3><span class="section-number">6.7.1.3. </span>Auto-quizzes<a class="headerlink" href="#auto-quizzes" title="Link to this heading">#</a></h3>
<p>Automatically generated quizzes for this chapter can be accessed here (Google Colab recommended).</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-prob-autoquiz.ipynb">Auto-quizzes</a>
(<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-prob-autoquiz.ipynb">Open In Colab</a>)</p></li>
</ul>
</section>
<section id="solutions-to-odd-numbered-warm-up-exercises">
<h3><span class="section-number">6.7.1.4. </span>Solutions to odd-numbered warm-up exercises<a class="headerlink" href="#solutions-to-odd-numbered-warm-up-exercises" title="Link to this heading">#</a></h3>
<p><em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>E6.2.1</strong> The probability of <span class="math notranslate nohighlight">\(Y\)</span> being in the second category is:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}(Y = e_2) = \pi_2 = 0.5.
\]</div>
<p><strong>E6.2.3</strong> Let <span class="math notranslate nohighlight">\(X_1, \ldots, X_n\)</span> be i.i.d. Bernoulli<span class="math notranslate nohighlight">\((q^*)\)</span>. The MLE is <span class="math notranslate nohighlight">\(\hat{q}_{\mathrm{MLE}} = \frac{1}{n} \sum_{i=1}^n X_i\)</span>. By the Law of Large Numbers, as <span class="math notranslate nohighlight">\(n \to \infty\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\hat{q}_{\mathrm{MLE}} \to \mathbb{E}[X_1] = q^*
\]</div>
<p>almost surely.</p>
<p><strong>E6.2.5</strong> The gradient at <span class="math notranslate nohighlight">\(w = 0\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
\nabla_w L_4(0; \{(x_i, y_i)\}_{i=1}^4) = -\sum_{i=1}^4 x_i(y_i - \sigma(0)) = -\frac{1}{2}(1 - 2 + 3 - 4) = 1.
\]</div>
<p>The updated parameter after one step of gradient descent is:</p>
<div class="math notranslate nohighlight">
\[
w' = w - \eta \nabla_w L_4(w; \{(x_i, y_i)\}_{i=1}^4) = 0 - 0.1 \cdot 1 = -0.1.
\]</div>
<p><strong>E6.2.7</strong> We must have <span class="math notranslate nohighlight">\(\sum_{x=-1}^1 P(X=x) = 1\)</span>. This implies</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{Z(\theta)} (h(-1)e^{-\theta} + h(0) + h(1)e^{\theta}) = 1.
\]</div>
<p>Hence,</p>
<div class="math notranslate nohighlight">
\[
Z(\theta) = h(-1)e^{-\theta} + h(0) + h(1)e^{\theta}.
\]</div>
<p><strong>E6.2.9</strong> The empirical frequency for each category is given by:</p>
<div class="math notranslate nohighlight">
\[
\hat{\pi}_i = \frac{N_i}{n},
\]</div>
<p>where <span class="math notranslate nohighlight">\(N_i\)</span> is the number of times category <span class="math notranslate nohighlight">\(i\)</span> appears in the sample. The counts are:</p>
<div class="math notranslate nohighlight">
\[
N_1 = 1, \quad N_2 = 2, \quad N_3 = 1.
\]</div>
<p>Thus, the empirical frequencies are:</p>
<div class="math notranslate nohighlight">
\[
\hat{\pi}_1 = \frac{1}{4} = 0.25, \quad \hat{\pi}_2 = \frac{2}{4} = 0.5, \quad \hat{\pi}_3 = \frac{1}{4} = 0.25.
\]</div>
<p><strong>E6.2.11</strong> The log-likelihood for a multivariate Gaussian distribution is given by:</p>
<div class="math notranslate nohighlight">
\[
\log \mathcal{L}(\mu, \Sigma; X) = -\frac{1}{2} \left[ (X - \mu)^T \Sigma^{-1} (X - \mu) + \log |\Sigma| + 2 \log(2\pi) \right].
\]</div>
<p>First, compute the inverse and determinant of <span class="math notranslate nohighlight">\(\Sigma\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\Sigma^{-1} = \frac{1}{3} \begin{pmatrix} 2 &amp; -1 \\ -1 &amp; 2 \end{pmatrix}, \quad |\Sigma| = 3.
\end{split}\]</div>
<p>Then,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
X - \mu = \begin{pmatrix} 1 \\ 3 \end{pmatrix} - \begin{pmatrix} 1 \\ 2 \end{pmatrix} = \begin{pmatrix} 0 \\ 1 \end{pmatrix}.
\end{split}\]</div>
<p>Thus,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
(X - \mu)^T \Sigma^{-1} (X - \mu) = \begin{pmatrix} 0 &amp; 1 \end{pmatrix} \frac{1}{3} \begin{pmatrix} 2 &amp; -1 \\ -1 &amp; 2 \end{pmatrix} \begin{pmatrix} 0 \\ 1 \end{pmatrix} = \frac{1}{3} \cdot 2 = \frac{2}{3}.
\end{split}\]</div>
<p>So the log-likelihood is:
$<span class="math notranslate nohighlight">\(
\log \mathcal{L} = -\frac{1}{2} \left[ \frac{2}{3} + \log 3 + 2 \log (2\pi) \right] \approx -3.178.
\)</span>$</p>
<p><strong>E6.3.1</strong> $<span class="math notranslate nohighlight">\( P(A|B) = \frac{P(A \cap B)}{P(B)} = \frac{0.2}{0.5} = 0.4. \)</span>$ This follows from the definition of conditional probability.</p>
<p><strong>E6.3.3</strong> $<span class="math notranslate nohighlight">\( \mathbb{E}[X|Y=y] = 1 \cdot 0.3 + 2 \cdot 0.7 = 0.3 + 1.4 = 1.7. \)</span>$ This is the definition of the conditional expectation for discrete random variables.</p>
<p><strong>E6.3.5</strong></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
P[A \mid B \cap C] &amp;= \frac{P[A \cap (B \cap C)]}{P[B \cap C]} \\
&amp;= \frac{P[A \cap B \cap C]}{P[B \cap C]} \\
&amp;= \frac{0.05}{0.1} \\
&amp;= 0.5.
\end{align*}\]</div>
<p><strong>E6.3.7</strong> If <span class="math notranslate nohighlight">\(A\)</span>, <span class="math notranslate nohighlight">\(B\)</span>, and <span class="math notranslate nohighlight">\(C\)</span> are pairwise independent, then they are also mutually independent. Therefore,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
P[A \cap B \cap C] &amp;= P[A] P[B] P[C] \\
&amp;= 0.8 \cdot 0.6 \cdot 0.5 \\
&amp;= 0.24.
\end{align*}\]</div>
<p><strong>E6.3.9</strong></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
(P[X=x, Z=z])_{x,z} &amp;= \sum_{y} (P[X=x, Y = y, Z=z])_{x,z} \\
&amp;= \sum_{y} [(P[X=x])_x \odot (P[Y = y \mid X=x])_x] (P[Z=z \mid Y = y])_z \\
&amp;= [(P[X=x])_x \odot (P[Y = 0 \mid X=x])_x] (P[Z=z \mid Y = 0])_z\\ 
&amp; \quad + [(P[X=x])_x \odot (P[Y = 1 \mid X=x])_x] (P[Z=z \mid Y = 1])_z \\
&amp;= \begin{pmatrix} 0.3 \cdot 0.2 \cdot 0.5 &amp; 0.3 \cdot 0.2 \cdot 0.5 \\ 0.7 \cdot 0.6 \cdot 0.5 &amp; 0.7 \cdot 0.6 \cdot 0.5 \end{pmatrix} + \begin{pmatrix} 0.3 \cdot 0.8 \cdot 0.1 &amp; 0.3 \cdot 0.8 \cdot 0.9 \\ 0.7 \cdot 0.4 \cdot 0.1 &amp; 0.7 \cdot 0.4 \cdot 0.9 \end{pmatrix} \\
&amp;= \begin{pmatrix} 0.03 &amp; 0.03 \\ 0.21 &amp; 0.21 \end{pmatrix} + \begin{pmatrix} 0.024 &amp; 0.216 \\ 0.028 &amp; 0.252 \end{pmatrix} \\
&amp;= \begin{pmatrix} 0.054 &amp; 0.246 \\ 0.238 &amp; 0.462 \end{pmatrix}.
\end{align*}\]</div>
<p><strong>E6.3.11</strong></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\hat{\pi}_1 &amp;= \frac{N_1}{N_1 + N_2} = \frac{50}{50 + 100} = \frac{1}{3}, \\
\hat{p}_{1,1} &amp;= \frac{N_{1,1}}{N_1} = \frac{10}{50} = 0.2, \\
\hat{p}_{2,1} &amp;= \frac{N_{2,1}}{N_2} = \frac{40}{100} = 0.4.
\end{align*}\]</div>
<p><strong>E6.3.13</strong></p>
<div class="math notranslate nohighlight">
\[
P[X=x, Y=y, Z=z] = P[X=x]P[Y=y|X=x]P[Z=z|X=x].
\]</div>
<p><strong>E6.4.1</strong></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
P(X = 1) &amp;= \sum_{i=1}^2 \pi_i p_i \\
&amp;= (0.6)(0.3) + (0.4)(0.8) \\
&amp;= 0.5
\end{align*}\]</div>
<p><strong>E6.4.3</strong> <span class="math notranslate nohighlight">\(P[X = (1, 0)] = \pi_1 p_{1,1} (1 - p_{1,2}) + \pi_2 p_{2,1} (1 - p_{2,2}) = 0.4 \cdot 0.7 \cdot 0.7 + 0.6 \cdot 0.2 \cdot 0.2 = 0.196 + 0.024 = 0.22\)</span>.</p>
<p><strong>E6.4.5</strong> <span class="math notranslate nohighlight">\(r_{1,i} = \frac{\pi_1 p_{1,1} p_{1,2}}{\pi_1 p_{1,1} p_{1,2} + \pi_2 p_{2,1} p_{2,2}} = \frac{0.5 \cdot 0.8 \cdot 0.2}{0.5 \cdot 0.8 \cdot 0.2 + 0.5 \cdot 0.1 \cdot 0.9} = \frac{0.08}{0.08 + 0.045} \approx 0.64\)</span>, <span class="math notranslate nohighlight">\(r_{2,i} = 1 - r_{1,i} \approx 0.36\)</span>.</p>
<p><strong>E6.4.7</strong> <span class="math notranslate nohighlight">\(\pi_1 = \frac{\eta_1}{n} = \frac{r_{1,1} + r_{1,1}}{2} = \frac{0.8 + 0.8}{2} = 0.8\)</span>, <span class="math notranslate nohighlight">\(\pi_2 = 1 - \pi_1 = 0.2\)</span>.</p>
<p><strong>E6.4.9</strong></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
r_{1,2} &amp;= \frac{\pi_1 \prod_{m=1}^3 p_{1,m}^{x_{2,m}} (1 - p_{1,m})^{1-x_{2,m}}}{\sum_{k=1}^2 \pi_k \prod_{m=1}^3 p_{k,m}^{x_{2,m}} (1 - p_{k,m})^{1-x_{2,m}}} \\
&amp;= \frac{(0.4)(0.2)^0(0.8)^1(0.9)^0(0.1)^1}{(0.4)(0.2)^0(0.8)^1(0.9)^0(0.1)^1 + (0.6)(0.8)^0(0.2)^1(0.5)^0(0.5)^1} \\
&amp;= \frac{0.032}{0.032 + 0.06} \\
&amp;= \frac{8}{23}
\end{align*}\]</div>
<p><strong>E6.4.11</strong></p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[X] = \pi_1 \mu_1 + \pi_2 \mu_2 = 0.5 \times (-1) + 0.5 \times 3 = -0.5 + 1.5 = 1.
\]</div>
<p><strong>E6.4.13</strong></p>
<div class="math notranslate nohighlight">
\[
\mathrm{Var}(X) = \pi_1 (\sigma_1^2 + \mu_1^2) + \pi_2 (\sigma_2^2 + \mu_2^2) - \left(\pi_1 \mu_1 + \pi_2 \mu_2\right)^2,
\]</div>
<div class="math notranslate nohighlight">
\[
\mathrm{Var}(X) = 0.4 (1 + 0^2) + 0.6 (2 + 4^2) - (0.4 \times 0 + 0.6 \times 4)^2 = 0.4 \times 1 + 0.6 \times 18 - 2.4^2,
\]</div>
<div class="math notranslate nohighlight">
\[
\mathrm{Var}(X) = 0.4 + 10.8 - 5.76 = 5.44.
\]</div>
<p><strong>E6.5.1</strong> $<span class="math notranslate nohighlight">\( B / B_{11} = B_{22} - B_{12}^T B_{11}^{-1} B_{12} = 3 - 1 \cdot \frac{1}{2} \cdot 1 = \frac{5}{2}, \)</span>$ using the definition of the Schur complement.</p>
<p><strong>E6.5.3</strong> The Schur complement of <span class="math notranslate nohighlight">\(A_{11}\)</span> in <span class="math notranslate nohighlight">\(A\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A/A_{11} = A_{22} - A_{21}A_{11}^{-1}A_{12} = 7 - \begin{pmatrix}
0 &amp; 6 
\end{pmatrix} \begin{pmatrix}
1 &amp; 2 \\
3 &amp; 4 
\end{pmatrix}^{-1} \begin{pmatrix}
0 \\
5 
\end{pmatrix}.
\end{split}\]</div>
<p>First, compute <span class="math notranslate nohighlight">\(A_{11}^{-1}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A_{11}^{-1} = \frac{1}{1 \cdot 4 - 2 \cdot 3} \begin{pmatrix}
4 &amp; -2 \\
-3 &amp; 1
\end{pmatrix} = \begin{pmatrix}
-2 &amp; 1 \\
1.5 &amp; -0.5
\end{pmatrix}.
\end{split}\]</div>
<p>Then,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A/A_{11} = 7 - \begin{pmatrix}
0 &amp; 6 
\end{pmatrix} \begin{pmatrix}
-2 &amp; 1 \\
1.5 &amp; -0.5
\end{pmatrix} \begin{pmatrix}
0 \\
5 
\end{pmatrix} = 7 + (6 \cdot 0.5 \cdot 5) = 22.
\end{split}\]</div>
<p><strong>E6.5.5</strong> The conditional</p>
<p>mean of <span class="math notranslate nohighlight">\(X_1\)</span> given <span class="math notranslate nohighlight">\(X_2 = 3\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\mu_{1|2}(3) = \mu_1 + \Sigma_{12} \Sigma_{22}^{-1} (3 - \mu_2) = 1 + 1 \cdot \frac{1}{3} (3 - 2) = \frac{4}{3},
\]</div>
<p>using the formula for the conditional mean of multivariate Gaussians.</p>
<p><strong>E6.5.7</strong> The conditional distribution of <span class="math notranslate nohighlight">\(X_1\)</span> given <span class="math notranslate nohighlight">\(X_2 = 1\)</span> is Gaussian with mean <span class="math notranslate nohighlight">\(\mu_{1|2} = \mu_1 + \Sigma_{12} \Sigma_{22}^{-1} (1 - \mu_2) = \frac{1}{2}\)</span> and variance <span class="math notranslate nohighlight">\(\Sigma_{1|2} = \Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21} = \frac{7}{2}\)</span>.</p>
<p><strong>E6.5.9</strong> The distribution of <span class="math notranslate nohighlight">\(Y\)</span> is Gaussian with mean vector <span class="math notranslate nohighlight">\(A\mu = \begin{pmatrix} -3 \\ -3 \end{pmatrix}\)</span> and covariance matrix <span class="math notranslate nohighlight">\(A\Sigma A^T = \begin{pmatrix} 8 &amp; 1 \\ 1 &amp; 6 \end{pmatrix}\)</span>.</p>
<p><strong>E6.5.11</strong> The mean of <span class="math notranslate nohighlight">\(Y_t\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbb{E}[Y_t] = \begin{pmatrix} 1 &amp; 0 \end{pmatrix} \mathbb{E}[X_t] = \begin{pmatrix} 1 &amp; 0 \end{pmatrix} \begin{pmatrix} 1 \\ 2 \end{pmatrix} = 1,
\end{split}\]</div>
<p>and the variance of <span class="math notranslate nohighlight">\(Y_t\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathrm{Var}[Y_t] = \begin{pmatrix} 1 &amp; 0 \end{pmatrix} \mathrm{Cov}[X_t] \begin{pmatrix} 1 \\ 0 \end{pmatrix} + 1 = \begin{pmatrix} 1 &amp; 0 \end{pmatrix} \begin{pmatrix} 1 \\ 0 \end{pmatrix} + 1 = 2,
\end{split}\]</div>
<p>using the properties of linear-Gaussian systems.</p>
<p><strong>E6.5.13</strong> The innovation is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
e_t = Y_t - H \mu_{\text{pred}} = 3 - \begin{pmatrix} 1 &amp; 0 \end{pmatrix} \begin{pmatrix} 3 \\ 1 \end{pmatrix} = 3 - 3 = 0.
\end{split}\]</div>
<p><strong>E6.5.15</strong> The updated state estimate is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mu_t = \mu_{\text{pred}} + K_t e_t = \begin{pmatrix} 3 \\ 1 \end{pmatrix} + \begin{pmatrix} \frac{2}{3} \\ \frac{1}{3} \end{pmatrix} \cdot 0 = \begin{pmatrix} 3 \\ 1 \end{pmatrix}.
\end{split}\]</div>
</section>
<section id="learning-outcomes">
<h3><span class="section-number">6.7.1.5. </span>Learning outcomes<a class="headerlink" href="#learning-outcomes" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Define exponential families and give examples of common probability distributions that belong to this family.</p></li>
<li><p>Derive the maximum likelihood estimator for exponential families and explain its properties.</p></li>
<li><p>Prove that, under certain conditions, the maximum likelihood estimator is statistically consistent.</p></li>
<li><p>Formulate generalized linear models using exponential families and express linear and logistic regression as special cases.</p></li>
<li><p>Compute the gradient and Hessian of the negative log-likelihood for generalized linear models.</p></li>
<li><p>Interpret the moment-matching equations for the maximum likelihood estimator in generalized linear models.</p></li>
<li><p>Apply the multiplication rule, the law of total probability, and Bayes’ rule to solve problems involving conditional probabilities.</p></li>
<li><p>Calculate conditional probability mass functions and conditional expectations for discrete random variables.</p></li>
<li><p>Define conditional independence and express it mathematically in terms of conditional probabilities.</p></li>
<li><p>Differentiate between the fork, chain, and collider configurations in graphical models representing conditional independence relations.</p></li>
<li><p>Derive the joint probability distribution for the Naive Bayes model under the assumption of conditional independence.</p></li>
<li><p>Implement maximum likelihood estimation to fit the parameters of the Naive Bayes model.</p></li>
<li><p>Apply the Naive Bayes model for prediction and evaluate its accuracy.</p></li>
<li><p>Implement Laplace smoothing to address the issue of unseen words in the training data when fitting a Naive Bayes model.</p></li>
<li><p>Apply the Naive Bayes model to perform sentiment analysis on a real-world dataset and interpret the results.</p></li>
<li><p>Define mixtures as convex combinations of distributions and express the probability distribution of a mixture model using the law of total probability.</p></li>
<li><p>Identify examples of mixture models, such as mixtures of multinomials and Gaussian mixture models, and recognize their probability density functions.</p></li>
<li><p>Explain the concept of marginalizing out an unobserved random variable in the context of mixture models.</p></li>
<li><p>Formulate the objective function for parameter estimation in mixtures of multivariate Bernoullis using the negative log-likelihood.</p></li>
<li><p>Describe the majorization-minimization principle and its application in the Expectation-Maximization (EM) algorithm.</p></li>
<li><p>Derive the E-step and M-step updates for the EM algorithm in the context of mixtures of multivariate Bernoullis.</p></li>
<li><p>Implement the EM algorithm for mixtures of multivariate Bernoullis and apply it to a real-world dataset, such as clustering handwritten digits from the MNIST dataset.</p></li>
<li><p>Identify and address numerical issues that may arise during the implementation of the EM algorithm, such as underflow, by applying techniques like the log-sum-exp trick.</p></li>
<li><p>Define block matrices and the Schur complement, and demonstrate their properties through examples and proofs.</p></li>
<li><p>Derive the marginal and conditional distributions of multivariate Gaussians using the properties of block matrices and the Schur complement.</p></li>
<li><p>Describe the linear-Gaussian system model and its components, including the state evolution and observation processes.</p></li>
<li><p>Explain the purpose and key steps of the Kalman filter algorithm, including the prediction and update steps.</p></li>
<li><p>Implement the Kalman filter algorithm in code, given the state evolution and observation models, and the initial state distribution.</p></li>
<li><p>Apply the Kalman filter to a location tracking problem, and interpret the results in terms of the estimated object path and the algorithm’s performance.</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\aleph\)</span></p>
</section>
</section>
<section id="additional-sections">
<h2><span class="section-number">6.7.2. </span>Additional sections<a class="headerlink" href="#additional-sections" title="Link to this heading">#</a></h2>
<section id="sentiment-analysis">
<h3><span class="section-number">6.7.2.1. </span>Sentiment analysis<a class="headerlink" href="#sentiment-analysis" title="Link to this heading">#</a></h3>
<p>As an application of the Naive Bayes model, we consider the task of sentiment analysis, which is a classification problem. We use a dataset from <a class="reference external" href="https://data.world/crowdflower/airline-twitter-sentiment">Crowdflower</a>. The full datatset is available <a class="reference external" href="https://www.kaggle.com/crowdflower/twitter-airline-sentiment">here</a>. Quoting <a class="reference external" href="https://data.world/crowdflower/airline-twitter-sentiment">Crowdflower</a>:</p>
<blockquote>
<div><p>A sentiment analysis job about the problems of each major U.S. airline. Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as “late flight” or “rude service”).</p>
</div></blockquote>
<p>We first load a cleaned-up version of the data and look at its summary.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;twitter-sentiment.csv&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;latin-1&#39;</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>time</th>
      <th>user</th>
      <th>sentiment</th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2/24/15 11:35</td>
      <td>cairdin</td>
      <td>neutral</td>
      <td>@VirginAmerica What @dhepburn said.</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2/24/15 11:15</td>
      <td>jnardino</td>
      <td>positive</td>
      <td>@VirginAmerica plus you've added commercials t...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2/24/15 11:15</td>
      <td>yvonnalynn</td>
      <td>neutral</td>
      <td>@VirginAmerica I didn't today... Must mean I n...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2/24/15 11:15</td>
      <td>jnardino</td>
      <td>negative</td>
      <td>@VirginAmerica it's really aggressive to blast...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2/24/15 11:14</td>
      <td>jnardino</td>
      <td>negative</td>
      <td>@VirginAmerica and it's a really big bad thing...</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>14640
</pre></div>
</div>
</div>
</div>
<p>We extract the text information in this dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">corpus</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0                      @VirginAmerica What @dhepburn said.
1        @VirginAmerica plus you&#39;ve added commercials t...
2        @VirginAmerica I didn&#39;t today... Must mean I n...
3        @VirginAmerica it&#39;s really aggressive to blast...
4        @VirginAmerica and it&#39;s a really big bad thing...
                               ...                        
14635    @AmericanAir thank you we got on a different f...
14636    @AmericanAir leaving over 20 minutes Late Flig...
14637    @AmericanAir Please bring American Airlines to...
14638    @AmericanAir you have my money, you change my ...
14639    @AmericanAir we have 8 ppl so we need 2 know h...
Name: text, Length: 14640, dtype: object
</pre></div>
</div>
</div>
</div>
<p>Next, we convert our dataset into a matrix by creating a document-term matrix using <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.fit_transform"><code class="docutils literal notranslate"><span class="pre">sklearn.feature_extraction.text.CountVectorizer</span></code></a>. Quoting <a class="reference external" href="https://en.wikipedia.org/wiki/Document-term_matrix">Wikipedia</a>:</p>
<blockquote>
<div><p>A document-term matrix or term-document matrix is a mathematical matrix that describes the frequency of terms that occur in a collection of documents. In a document-term matrix, rows correspond to documents in the collection and columns correspond to terms.</p>
</div></blockquote>
<p>By default, it first preprocesses the data. In particular, it lower-cases all words and removes punctuation. A more careful pre-procsseing would also include stemming, although we do not do this here. Regarding the latter, quoting <a class="reference external" href="https://en.wikipedia.org/wiki/Stemming">Wikipedia</a>:</p>
<blockquote>
<div><p>In linguistic morphology and information retrieval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form—generally a written word form. […] A computer program or subroutine that stems word may be called a stemming program, stemming algorithm, or stemmer. […] A stemmer for English operating on the stem cat should identify such strings as cats, catlike, and catty. A stemming algorithm might also reduce the words fishing, fished, and fisher to the stem fish. The stem need not be a word, for example the Porter algorithm reduces, argue, argued, argues, arguing, and argus to the stem argu.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">()</span>
<span class="n">count</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">count</span><span class="p">[:</span><span class="mi">2</span><span class="p">,])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  (0, 14376)	1
  (0, 14654)	1
  (0, 4872)	1
  (0, 11739)	1
  (1, 14376)	1
  (1, 10529)	1
  (1, 15047)	1
  (1, 14296)	1
  (1, 2025)	1
  (1, 4095)	1
  (1, 13425)	1
  (1, 13216)	1
  (1, 5733)	1
  (1, 13021)	1
</pre></div>
</div>
</div>
</div>
<p>The list of all terms used can be accessed as follows.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">terms</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">vectorizer</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">terms</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;00&#39; &#39;000&#39; &#39;000114&#39; ... &#39;ü_ù__&#39; &#39;üi&#39; &#39;ýã&#39;]
</pre></div>
</div>
</div>
</div>
<p>Because of our use of the multivariate Bernoulli naive Bayes model, it will be more convenient to work with a variant of the document-term matrix where each word is either present or absent. Note that, in the context of tweet data which are very short documents with likely little word repetition, there is probably not much difference.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="p">(</span><span class="n">count</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">2</span><span class="p">,])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  (0, 4872)	1
  (0, 11739)	1
  (0, 14376)	1
  (0, 14654)	1
  (1, 2025)	1
  (1, 4095)	1
  (1, 5733)	1
  (1, 10529)	1
  (1, 13021)	1
  (1, 13216)	1
  (1, 13425)	1
  (1, 14296)	1
  (1, 14376)	1
  (1, 15047)	1
</pre></div>
</div>
</div>
</div>
<p>We also extract the labels (<code class="docutils literal notranslate"><span class="pre">neutral</span></code>, <code class="docutils literal notranslate"><span class="pre">postive</span></code>, <code class="docutils literal notranslate"><span class="pre">negative</span></code>) from the dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;sentiment&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;neutral&#39; &#39;positive&#39; &#39;neutral&#39; ... &#39;neutral&#39; &#39;negative&#39; &#39;neutral&#39;]
</pre></div>
</div>
</div>
</div>
<p>We split the data into a training set and a test set using <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"><code class="docutils literal notranslate"><span class="pre">sklearn.model_selection.train_test_split</span></code></a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">535</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We use the Naive Bayes method. We first construct the matrix <span class="math notranslate nohighlight">\(N_{k,m}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">label_set</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;positive&#39;</span><span class="p">,</span> <span class="s1">&#39;negative&#39;</span><span class="p">,</span> <span class="s1">&#39;neutral&#39;</span><span class="p">]</span>
<span class="n">N_km</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">label_set</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">terms</span><span class="p">)))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">label_set</span><span class="p">):</span>
    <span class="n">k_rows</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_train</span> <span class="o">==</span> <span class="n">k</span><span class="p">)</span>
    <span class="n">N_km</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">k_rows</span><span class="p">,</span> <span class="p">:]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We are ready to train on the dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">nb_fit_table</span><span class="p">(</span><span class="n">N_km</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pi_k</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.13066681 0.69345935 0.17587385]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">p_km</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[3.61089045e-05 2.52762331e-04 3.61089045e-05 ... 7.22178089e-05
  7.22178089e-05 3.61089045e-05]
 [8.16493162e-05 1.29278084e-04 6.80410968e-06 ... 6.80410968e-06
  6.80410968e-06 1.36082194e-05]
 [5.36552649e-05 1.07310530e-04 5.36552649e-05 ... 2.68276325e-05
  2.68276325e-05 5.36552649e-05]]
</pre></div>
</div>
</div>
</div>
<p>Next, we plot the vector <span class="math notranslate nohighlight">\(p_{k,m}\)</span> for each label <span class="math notranslate nohighlight">\(k\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span><span class="n">ax2</span><span class="p">,</span><span class="n">ax3</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p_km</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p_km</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p_km</span><span class="p">[</span><span class="mi">2</span><span class="p">,:],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/56c6566bc2c7638e248aed0a24cf120e13398204f5fb15b43908adf2aba02909.png" src="../../_images/56c6566bc2c7638e248aed0a24cf120e13398204f5fb15b43908adf2aba02909.png" />
</div>
</div>
<p>We can compute a prediction on the test tweets. For example, for the 5th test tweet:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mmids</span><span class="o">.</span><span class="n">nb_predict</span><span class="p">(</span><span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span><span class="p">,</span> <span class="n">X_test</span><span class="p">[</span><span class="mi">4</span><span class="p">,:]</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="n">label_set</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;positive&#39;
</pre></div>
</div>
</div>
</div>
<p>The following computes the overall accuracy over the test data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">acc</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">)):</span>
    <span class="k">if</span> <span class="n">mmids</span><span class="o">.</span><span class="n">nb_predict</span><span class="p">(</span><span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span><span class="p">,</span> <span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="n">label_set</span><span class="p">)</span> <span class="o">==</span> <span class="n">y_test</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
        <span class="n">acc</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="n">acc</span><span class="o">/</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.7219945355191257
</pre></div>
</div>
</div>
</div>
<p>To get a better understanding of the differences uncovered by Naive Bayes between the different labels, we identify words that are particularly common in one label, but on the other. Recall that label <code class="docutils literal notranslate"><span class="pre">1</span></code> corresponds to <code class="docutils literal notranslate"><span class="pre">positive</span></code> while label <code class="docutils literal notranslate"><span class="pre">2</span></code> corresponds to <code class="docutils literal notranslate"><span class="pre">negative</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pos_terms</span> <span class="o">=</span> <span class="p">(</span><span class="n">p_km</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span> <span class="o">&gt;</span> <span class="mf">0.002</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">p_km</span><span class="p">[</span><span class="mi">1</span><span class="p">,:]</span> <span class="o">&lt;</span> <span class="mf">0.002</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">terms</span><span class="p">[</span><span class="n">pos_terms</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;_ù&#39; &#39;airline&#39; &#39;amazing&#39; &#39;awesome&#39; &#39;best&#39; &#39;crew&#39; &#39;fly&#39; &#39;flying&#39; &#39;good&#39;
 &#39;got&#39; &#39;great&#39; &#39;guys&#39; &#39;love&#39; &#39;much&#39; &#39;please&#39; &#39;see&#39; &#39;thank&#39; &#39;thanks&#39;
 &#39;today&#39; &#39;very&#39; &#39;virginamerica&#39;]
</pre></div>
</div>
</div>
</div>
<p>One notices that many positive words do appear in this list: <code class="docutils literal notranslate"><span class="pre">awesome</span></code>, <code class="docutils literal notranslate"><span class="pre">best</span></code>, <code class="docutils literal notranslate"><span class="pre">great</span></code>, <code class="docutils literal notranslate"><span class="pre">love</span></code>, <code class="docutils literal notranslate"><span class="pre">thanks</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">neg_terms</span> <span class="o">=</span> <span class="p">(</span><span class="n">p_km</span><span class="p">[</span><span class="mi">1</span><span class="p">,:]</span> <span class="o">&gt;</span> <span class="mf">0.002</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">p_km</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span> <span class="o">&lt;</span> <span class="mf">0.002</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">terms</span><span class="p">[</span><span class="n">neg_terms</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;about&#39; &#39;after&#39; &#39;bag&#39; &#39;been&#39; &#39;call&#39; &#39;cancelled&#39; &#39;delayed&#39; &#39;don&#39;
 &#39;flightled&#39; &#39;flights&#39; &#39;had&#39; &#39;has&#39; &#39;hold&#39; &#39;hour&#39; &#39;hours&#39; &#39;how&#39; &#39;if&#39; &#39;late&#39;
 &#39;need&#39; &#39;one&#39; &#39;or&#39; &#39;over&#39; &#39;phone&#39; &#39;plane&#39; &#39;still&#39; &#39;there&#39; &#39;ve&#39; &#39;what&#39;
 &#39;when&#39; &#39;why&#39; &#39;would&#39;]
</pre></div>
</div>
</div>
</div>
<p>This time, we notice: <code class="docutils literal notranslate"><span class="pre">bag</span></code>, <code class="docutils literal notranslate"><span class="pre">cancelled</span></code>, <code class="docutils literal notranslate"><span class="pre">delayed</span></code>, <code class="docutils literal notranslate"><span class="pre">hours</span></code>, <code class="docutils literal notranslate"><span class="pre">phone</span></code>.</p>
<p><strong>CHAT &amp; LEARN</strong> The bag-of-words representation used in the sentiment analysis example is a simple but limited way to represent text data. More advanced representations such as word embeddings and transformer models can capture more semantic information. Ask your favorite AI chatbot to explain these representations and how they can be used for text classification tasks. <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
</section>
<section id="kalman-filtering-missing-data">
<h3><span class="section-number">6.7.2.2. </span>Kalman filtering: missing data<a class="headerlink" href="#kalman-filtering-missing-data" title="Link to this heading">#</a></h3>
<p>In Kalman filtering, we can also allow for the possibility that some observations are missing. Imagine for instance losing GPS signal while going through a tunnel. The recursions above are still valid, with the only modification that the <em>Update</em> equations involving <span class="math notranslate nohighlight">\(\bY_t\)</span> are dropped at those times <span class="math notranslate nohighlight">\(t\)</span> where there is no observation. In Numpy, we can use <a class="reference external" href="https://numpy.org/doc/stable/reference/constants.html#numpy.nan"><code class="docutils literal notranslate"><span class="pre">NaN</span></code></a> to indicate the lack of observation. (Alternatively, one can use the <a class="reference external" href="https://numpy.org/doc/stable/reference/maskedarray.generic.html">numpy.ma</a> module.)</p>
<p>We use a same sample path as above, but mask observations at times <span class="math notranslate nohighlight">\(t=10,\ldots,20\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">seed</span> <span class="o">=</span> <span class="mi">535</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">ss</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">os</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">F</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]])</span> 
<span class="n">H</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]])</span>
<span class="n">Q</span> <span class="o">=</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">ss</span><span class="p">))</span>
<span class="n">R</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">os</span><span class="p">))</span>
<span class="n">init_mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
<span class="n">init_Sig</span> <span class="o">=</span> <span class="n">Q</span>
<span class="n">T</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">lgSamplePath</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">ss</span><span class="p">,</span> <span class="n">os</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">R</span><span class="p">,</span> <span class="n">init_mu</span><span class="p">,</span> <span class="n">init_Sig</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">):</span>
    <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
    <span class="n">y</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
</pre></div>
</div>
</div>
</div>
<p>Here is the sample we are aiming to infer.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">y</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">s</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dotted&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,:])</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,:])</span><span class="o">+</span><span class="mi">5</span><span class="p">))</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">,:])</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">,:])</span><span class="o">+</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/f50483a562aeab113e7a2c066e9f1df40a3734ae2dcc59b104875762494a3a7e.png" src="../../_images/f50483a562aeab113e7a2c066e9f1df40a3734ae2dcc59b104875762494a3a7e.png" />
</div>
</div>
<p>We modify the recursion accordingly, that is, skip the <em>Update</em> step when there is no observation to use for the update.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">kalmanUpdate</span><span class="p">(</span><span class="n">ss</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">R</span><span class="p">,</span> <span class="n">y_t</span><span class="p">,</span> <span class="n">mu_prev</span><span class="p">,</span> <span class="n">Sig_prev</span><span class="p">):</span>
    <span class="n">mu_pred</span> <span class="o">=</span> <span class="n">F</span> <span class="o">@</span> <span class="n">mu_prev</span>
    <span class="n">Sig_pred</span> <span class="o">=</span> <span class="n">F</span> <span class="o">@</span> <span class="n">Sig_prev</span> <span class="o">@</span> <span class="n">F</span><span class="o">.</span><span class="n">T</span> <span class="o">+</span> <span class="n">Q</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">y_t</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">y_t</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="k">return</span> <span class="n">mu_pred</span><span class="p">,</span> <span class="n">Sig_pred</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">e_t</span> <span class="o">=</span> <span class="n">y_t</span> <span class="o">-</span> <span class="n">H</span> <span class="o">@</span> <span class="n">mu_pred</span>
        <span class="n">S</span> <span class="o">=</span> <span class="n">H</span> <span class="o">@</span> <span class="n">Sig_pred</span> <span class="o">@</span> <span class="n">H</span><span class="o">.</span><span class="n">T</span> <span class="o">+</span> <span class="n">R</span>
        <span class="n">Sinv</span> <span class="o">=</span> <span class="n">LA</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">Sig_pred</span> <span class="o">@</span> <span class="n">H</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">Sinv</span>
        <span class="n">mu_new</span> <span class="o">=</span> <span class="n">mu_pred</span> <span class="o">+</span> <span class="n">K</span> <span class="o">@</span> <span class="n">e_t</span>
        <span class="n">Sig_new</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">ss</span><span class="p">))</span> <span class="o">-</span> <span class="n">K</span> <span class="o">@</span> <span class="n">H</span><span class="p">)</span> <span class="o">@</span> <span class="n">Sig_pred</span>
        <span class="k">return</span> <span class="n">mu_new</span><span class="p">,</span> <span class="n">Sig_new</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">init_mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
<span class="n">init_Sig</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">ss</span><span class="p">))</span>
<span class="n">mu</span><span class="p">,</span> <span class="n">Sig</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">kalmanFilter</span><span class="p">(</span><span class="n">ss</span><span class="p">,</span> <span class="n">os</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">R</span><span class="p">,</span> <span class="n">init_mu</span><span class="p">,</span> <span class="n">init_Sig</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mu</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">mu</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">y</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">s</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dotted&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,:])</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,:])</span><span class="o">+</span><span class="mi">5</span><span class="p">))</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">,:])</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">,:])</span><span class="o">+</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/2e92c840f3a0f15546d76a33647bbdb2a46015c60b362e0062398e2de1579630.png" src="../../_images/2e92c840f3a0f15546d76a33647bbdb2a46015c60b362e0062398e2de1579630.png" />
</div>
</div>
</section>
<section id="cholesky-decomposition">
<h3><span class="section-number">6.7.2.3. </span>Cholesky decomposition<a class="headerlink" href="#cholesky-decomposition" title="Link to this heading">#</a></h3>
<p>In  this section, we derive an important matrix factorization and apply it to generating multivariate Gaussians. We also revisit the least-squares problem. We begin with the motivation.</p>
<p><strong>Generating multivariate Gaussians</strong> Suppose we want to generate samples from a multivariate Gaussian <span class="math notranslate nohighlight">\(\bX \sim N_d(\bmu, \bSigma)\)</span> with given mean vector <span class="math notranslate nohighlight">\(\bmu \in \mathbb{R}^d\)</span> and positive definite covariance matrix <span class="math notranslate nohighlight">\(\bSigma  \in \mathbb{R}^{d \times d}\)</span>. Of course, in Numpy, we could use <a class="reference external" href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.multivariate_normal.html"><code class="docutils literal notranslate"><span class="pre">numpy.random.Generator.multivariate_normal</span></code></a>. But what is behind it? More precisely, suppose we have access to unlimited samples <span class="math notranslate nohighlight">\(U_1, U_2, U_3, etc.\)</span> from uniform random variables in <span class="math notranslate nohighlight">\([0,1]\)</span>. How do we transform them to obtain samples from <span class="math notranslate nohighlight">\(N_d(\bmu, \bSigma)\)</span>.</p>
<p>We start with the simplest case: <span class="math notranslate nohighlight">\(d=1\)</span>, <span class="math notranslate nohighlight">\(\mu = 0\)</span>, and <span class="math notranslate nohighlight">\(\sigma^2 = 1\)</span>. That is, we first generate a univariate standard Normal. We have seen a recipe for doing this before, the inverse transform sampling method. Specifically, recall that the cumulative distribution function (CDF) of a random variable <span class="math notranslate nohighlight">\(Z\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[
F_Z(z) = \mathbb{P}[Z \leq z], \qquad \forall z \in \mathbb{R}.
\]</div>
<p>Let <span class="math notranslate nohighlight">\(\mathcal{Z}\)</span> be the interval where <span class="math notranslate nohighlight">\(F_Z(z) \in (0,1)\)</span> and assume that <span class="math notranslate nohighlight">\(F_X\)</span> is strictly increasing on <span class="math notranslate nohighlight">\(\mathcal{Z}\)</span>. Let <span class="math notranslate nohighlight">\(U \sim \mathrm{U}[0,1]\)</span>. Then it can be shown that</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}[F_X^{-1}(U) \leq z] = F_X(z).
\]</div>
<p>So take <span class="math notranslate nohighlight">\(F_Z = \Phi\)</span>, the CDF of the standard Normal. Then <span class="math notranslate nohighlight">\(Z = \Phi^{-1}(U)\)</span> is <span class="math notranslate nohighlight">\(N(0,1)\)</span>.</p>
<p>How do we generate a <span class="math notranslate nohighlight">\(N(\mu, \sigma^2)\)</span> variable, for arbitrary <span class="math notranslate nohighlight">\(\mu \in \mathbb{R}\)</span> and <span class="math notranslate nohighlight">\(\sigma^2 &gt; 0\)</span>? We use the fact that the linear transformation of Gaussian is still Gaussian. In particular, if <span class="math notranslate nohighlight">\(Z \sim N(0,1)\)</span>, then</p>
<div class="math notranslate nohighlight">
\[
X = \mu + \sigma Z
\]</div>
<p>is <span class="math notranslate nohighlight">\(N(\mu, \sigma^2)\)</span>.</p>
<p><strong>NUMERICAL CORNER:</strong> In Python, <span class="math notranslate nohighlight">\(\Phi^{-1}\)</span> can be accessed using <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html"><code class="docutils literal notranslate"><span class="pre">scipy.stats.norm.ppf</span></code></a>. We implement this next (with help from ChatGPT).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="k">def</span> <span class="nf">generate_standard_normal_samples_using_inverse_cdf</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma2</span><span class="p">):</span>
    <span class="c1"># Step 1: Generate uniform [0,1] random variables</span>
    <span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    
    <span class="c1"># Step 2: Apply the inverse CDF (ppf) of the standard normal distribution</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">U</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">sigma2</span> <span class="o">*</span> <span class="n">Z</span>
</pre></div>
</div>
</div>
</div>
<p>We generate 1000 samples and plot the empirical distribution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generate 1000 standard normal samples</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">generate_standard_normal_samples_using_inverse_cdf</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">0</span> <span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot the empirical PDF</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># Plot histogram of the samples with density=True to normalize the histogram</span>
<span class="n">count</span><span class="p">,</span> <span class="n">bins</span><span class="p">,</span> <span class="n">ignored</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>

<span class="c1"># Plot the theoretical standard normal PDF for comparison</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Empirical PDF of Generated Samples&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Value&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Density&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/648741740d20b45ce65b2b2791d1603ea77bd902d8873c497c1a1d5f0cc41bb6.png" src="../../_images/648741740d20b45ce65b2b2791d1603ea77bd902d8873c497c1a1d5f0cc41bb6.png" />
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>CHAT &amp; LEARN</strong> It turns out there is a neat trick to generate <em>two</em> independent samples from <span class="math notranslate nohighlight">\(N(0,1)\)</span> that does not rely on access to <span class="math notranslate nohighlight">\(\Phi^{-1}\)</span>. It is called the Box-Muller transform. Ask your favorite AI chatbot about it. Modify our code above to implement it. <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
<p>We move on to the multivariate case. We proceed similarly as before. First, how do we generate a <span class="math notranslate nohighlight">\(d\)</span>-dimensional Gaussian with mean vector <span class="math notranslate nohighlight">\(\bmu = \mathbf{0}\)</span> and identity covariance matrix <span class="math notranslate nohighlight">\(\bSigma = I_{d \times d}\)</span>? Easy – it has <span class="math notranslate nohighlight">\(d\)</span> independent components, each of which is standard Normal. So letting <span class="math notranslate nohighlight">\(U_1, \ldots, U_d\)</span> be independent uniform <span class="math notranslate nohighlight">\([0,1]\)</span> variables, then</p>
<div class="math notranslate nohighlight">
\[
\mathbf{Z}
= (\Phi^{-1}(U_1),\ldots,\Phi^{-1}(U_d))
\]</div>
<p>is <span class="math notranslate nohighlight">\(N(\mathbf{0}, I_{d \times d})\)</span>.</p>
<p>We now seek to generate a multivariate Gaussian with arbitrary mean vector <span class="math notranslate nohighlight">\(\bmu\)</span> and positive definite covariance matrix <span class="math notranslate nohighlight">\(\bSigma\)</span>. Again, we use a linear transformation</p>
<div class="math notranslate nohighlight">
\[
\mathbf{X}
= \mathbf{a} + A \mathbf{Z}.
\]</div>
<p>What are the right choices for <span class="math notranslate nohighlight">\(a \in \mathbb{R}^d\)</span> and <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{d \times d}\)</span>? We need to match the obtained and desired mean and covariance. We start with the mean. By linearity of expectation,</p>
<div class="math notranslate nohighlight">
\[
\E[\mathbf{X}]
= \E[\mathbf{a} + A \mathbf{Z}]
= \mathbf{a} + A \,\E[\mathbf{Z}]
= \mathbf{a}.
\]</div>
<p>Hence we pick <span class="math notranslate nohighlight">\(\mathbf{a} := \bmu\)</span>.</p>
<p>As for the covariance, using the <em>Covariance of a Linear Transformation</em>, we get</p>
<div class="math notranslate nohighlight">
\[
\cov[\mathbf{X}]
= A \,\cov[\mathbf{Z}] A^T
= A A^T.
\]</div>
<p>Now we have a problem: what is a matrix <span class="math notranslate nohighlight">\(A\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
A A^T = \bSigma?
\]</div>
<p>In some sense, we are looking for a sort of “square root” of the covariance matrix. There are several ways of doing this. The Cholesky decomposition is one of them. We return to generating samples from <span class="math notranslate nohighlight">\(N(\bmu, \bSigma)\)</span> after introducing it.</p>
<p><strong>A matrix factorization</strong> Our key linear-algebraic result of this section is the following. The matrix factorization in the next theorem is called a Cholesky decomposition. It has many <a class="reference external" href="https://en.wikipedia.org/wiki/Cholesky_decomposition#Applications">applications</a>.</p>
<p><strong>THEOREM</strong> <strong>(Cholesky Decomposition)</strong> Any positive definite matrix <span class="math notranslate nohighlight">\(B \in \mathbb{R}^{n \times n}\)</span> can be factorized uniquely as</p>
<div class="math notranslate nohighlight">
\[
B = L L^T
\]</div>
<p>where <span class="math notranslate nohighlight">\(L \in \mathbb{R}^{n \times n}\)</span> is a lower triangular matrix with positive entries on the diagonal. <span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p>The proof is provided below. It is based on deriving an algorithm for computing the Cholesky decomposition: we grow <span class="math notranslate nohighlight">\(L\)</span> starting from its top-left corner by successively computing its next row based on the previously constructed submatrix. Note that, because <span class="math notranslate nohighlight">\(L\)</span> is lower triangular, it suffices to compute its elements on and below the diagonal. We first give the algorithm, then establish that it is well-defined.</p>
<p><strong>Figure:</strong> Access pattern (<a class="reference external" href="https://en.wikipedia.org/wiki/File:Chol.gif">Source</a>)</p>
<p><img alt="Access pattern" src="https://upload.wikimedia.org/wikipedia/commons/b/be/Chol.gif" /></p>
<p><span class="math notranslate nohighlight">\(\bowtie\)</span></p>
<p><strong>EXAMPLE:</strong> Before proceeeding with the general method, we give a small example to provide some intuition as to how it operates. We need a positive definite matrix. Consider the matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A 
= 
\begin{pmatrix}
1 &amp; 2 &amp; 1\\
0 &amp; -2 &amp; 1\\
0 &amp; 0 &amp; 1\\
0 &amp; 0 &amp; 0
\end{pmatrix}.
\end{split}\]</div>
<p>It has full column rank (why?). Recall that, in that case, the <span class="math notranslate nohighlight">\(B = A^T A\)</span> is positive definite.</p>
<p>That is, the matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
B 
=
A^T A 
= 
\begin{pmatrix}
1 &amp; 2 &amp; 1\\
2 &amp; 8 &amp; 0\\
1 &amp; 0 &amp; 3
\end{pmatrix}
\end{split}\]</div>
<p>is positive definite.</p>
<p>Let <span class="math notranslate nohighlight">\(L = (\ell_{i,j})_{i,j=1}^3\)</span> be lower triangular. We seek to solve <span class="math notranslate nohighlight">\(L L^T = B\)</span> for the nonzero entries of <span class="math notranslate nohighlight">\(L\)</span>. Observe that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{pmatrix}
\ell_{1,1} &amp; 0 &amp; 0\\
\ell_{2,1} &amp; \ell_{2,2} &amp; 0\\
\ell_{3,1} &amp; \ell_{3,2} &amp; \ell_{3,3}
\end{pmatrix}
\begin{pmatrix}
\ell_{1,1} &amp; \ell_{2,1} &amp; \ell_{3,1}\\
0 &amp; \ell_{2,2} &amp; \ell_{3,2}\\
0 &amp; 0 &amp; \ell_{3,3}
\end{pmatrix}
=
\begin{pmatrix}
\ell_{1,1}^2 &amp; \ell_{1,1}\ell_{2,1} &amp; \ell_{1,1}\ell_{3,1}\\
\ell_{1,1}\ell_{2,1} &amp; \ell_{2,1}^2 + \ell_{2,2}^2 &amp; \ell_{2,1}\ell_{3,1} + \ell_{2,2}\ell_{3,2}\\
\ell_{1,1}\ell_{3,1} &amp; \ell_{2,1}\ell_{3,1} + \ell_{2,2}\ell_{3,2} &amp; \ell_{3,1}^2 + \ell_{3,2}^2 + \ell_{3,3}
\end{pmatrix}.
\end{split}\]</div>
<p>The system</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{pmatrix}
\ell_{1,1}^2 &amp; \ell_{1,1}\ell_{2,1} &amp; \ell_{1,1}\ell_{3,1}\\
\ell_{1,1}\ell_{2,1} &amp; \ell_{2,1}^2 + \ell_{2,2}^2 &amp; \ell_{2,1}\ell_{3,1} + \ell_{2,2}\ell_{3,2}\\
\ell_{1,1}\ell_{3,1} &amp; \ell_{2,1}\ell_{3,1} + \ell_{2,2}\ell_{3,2} &amp; \ell_{3,1}^2 + \ell_{3,2}^2 + \ell_{3,3}^2
\end{pmatrix}
=
\begin{pmatrix}
1 &amp; 2 &amp; 1\\
2 &amp; 8 &amp; 0\\
1 &amp; 0 &amp; 3
\end{pmatrix}
\end{split}\]</div>
<p>turns out to be fairly simple to solve.</p>
<ol class="arabic simple">
<li><p>From the first entry, we get <span class="math notranslate nohighlight">\(\ell_{1,1} = 1\)</span> (where we took the positive solution to <span class="math notranslate nohighlight">\(\ell_{1,1}^2 = 1\)</span>).</p></li>
<li><p>Given that <span class="math notranslate nohighlight">\(\ell_{1,1}\)</span> is known, entry <span class="math notranslate nohighlight">\(\ell_{2,1}\)</span> is determined from <span class="math notranslate nohighlight">\(\ell_{1,1}\ell_{2,1} =2\)</span> in the first entry of the second row. That is, <span class="math notranslate nohighlight">\(\ell_{2,1} =2\)</span>. Then the second entry of the second row gives <span class="math notranslate nohighlight">\(\ell_{2,2}\)</span> through <span class="math notranslate nohighlight">\(\ell_{2,1}^2 + \ell_{2,2}^2  = 8\)</span>. So <span class="math notranslate nohighlight">\(\ell_{2,2} = 2\)</span> (again we take the positive solution).</p></li>
<li><p>We move to the third row. The first entry gives <span class="math notranslate nohighlight">\(\ell_{3,1} = 1\)</span>, the second entry gives <span class="math notranslate nohighlight">\(\ell_{3,2} = -1\)</span> and finally the third entry leads to <span class="math notranslate nohighlight">\(\ell_{3,3} = 1\)</span>.</p></li>
</ol>
<p>Hence we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
L
=
\begin{pmatrix}
\ell_{1,1} &amp; 0 &amp; 0\\
\ell_{2,1} &amp; \ell_{2,2} &amp; 0\\
\ell_{3,1} &amp; \ell_{3,2} &amp; \ell_{3,3}
\end{pmatrix}
=
\begin{pmatrix}
1 &amp; 0 &amp; 0\\
2 &amp; 2 &amp; 0\\
1 &amp; -1 &amp; 1
\end{pmatrix}.
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p>To detail the computation of the Cholesky decomposition <span class="math notranslate nohighlight">\(L L^T\)</span> of <span class="math notranslate nohighlight">\(B\)</span>, we will need some notation. Write <span class="math notranslate nohighlight">\(B = (b_{i,j})_{i,j=1}^n\)</span> and <span class="math notranslate nohighlight">\(L = (\ell_{i,j})_{i,j=1}^n\)</span>. Let <span class="math notranslate nohighlight">\(L_{(k)} = (\ell_{i,j})_{i,j=1}^k\)</span> be the first <span class="math notranslate nohighlight">\(k\)</span> rows and columns of <span class="math notranslate nohighlight">\(L\)</span>, let <span class="math notranslate nohighlight">\(\bflambda_{(k)}^T = (\ell_{k,1},\ldots,\ell_{k,k-1})\)</span> be the row vector corresponding to the first <span class="math notranslate nohighlight">\(k-1\)</span> entries of row <span class="math notranslate nohighlight">\(k\)</span> of <span class="math notranslate nohighlight">\(L\)</span>, and let <span class="math notranslate nohighlight">\(\bfbeta_{(k)}^T = (b_{k,1},\ldots,b_{k,k-1})\)</span> be the row vector corresponding to the first <span class="math notranslate nohighlight">\(k-1\)</span> entries of row <span class="math notranslate nohighlight">\(k\)</span> of <span class="math notranslate nohighlight">\(B\)</span>.</p>
<p>The strategy is to compute <span class="math notranslate nohighlight">\(L_{(1)}\)</span>, then <span class="math notranslate nohighlight">\(L_{(2)}\)</span>, then <span class="math notranslate nohighlight">\(L_{(3)}\)</span> and so on. With the notation above, <span class="math notranslate nohighlight">\(L_{(j)}\)</span> can be written in block form as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
L_{(j)}
=
\begin{pmatrix}
L_{(j-1)} &amp; \mathbf{0}\\
\bflambda_{(j)}^T &amp; \ell_{j,j}.
\end{pmatrix}
\end{split}\]</div>
<p>Hence, once <span class="math notranslate nohighlight">\(L_{(j-1)}\)</span> is known, in order to compute <span class="math notranslate nohighlight">\(L_{(j)}\)</span> one only needs <span class="math notranslate nohighlight">\(\bflambda_{(j)}\)</span> and <span class="math notranslate nohighlight">\(\ell_{j,j}\)</span>. We show next that they satisfy easily solvable systems of equations.</p>
<p>We first note that the <span class="math notranslate nohighlight">\((1,1)\)</span> entry of the matrix equation <span class="math notranslate nohighlight">\(L L^T = B\)</span> implies that</p>
<div class="math notranslate nohighlight">
\[
\ell_{1,1}^2 = b_{1,1}.
\]</div>
<p>So we set</p>
<div class="math notranslate nohighlight">
\[
L_{(1)}
= \ell_{1,1}
= \sqrt{b_{1,1}}.
\]</div>
<p>For this step to be well-defined, it needs to be the case that <span class="math notranslate nohighlight">\(b_{1,1} &gt; 0\)</span>. It is easy to see that it follows from the positive definiteness of <span class="math notranslate nohighlight">\(B\)</span>:</p>
<div class="math notranslate nohighlight">
\[
0 &lt; \langle \mathbf{e}_1, B \mathbf{e}_1\rangle = \mathbf{e}_1^T B_{\cdot,1} = b_{1,1}.
\]</div>
<p>Proceeding by induction, assume <span class="math notranslate nohighlight">\(L_{(j-1)}\)</span> has been constructed. The first <span class="math notranslate nohighlight">\(j-1\)</span> elements of the <span class="math notranslate nohighlight">\(j\)</span>-th row of the matrix equation <span class="math notranslate nohighlight">\(L L^T = B\)</span> translate into</p>
<div class="math notranslate nohighlight">
\[
L_{j,\cdot} (L^T)_{\cdot,1:j-1} = \bflambda_{(j)}^T L_{(j-1)}^T = \bfbeta_{(j)}^T,
\]</div>
<p>where <span class="math notranslate nohighlight">\((L^T)_{\cdot,1:j-1}\)</span> denotes the first <span class="math notranslate nohighlight">\(j-1\)</span> columns of <span class="math notranslate nohighlight">\(L^T\)</span>. In the first equality above, we used the fact that <span class="math notranslate nohighlight">\(L^T\)</span> is upper triangular. Taking a transpose, the resulting linear system of equations</p>
<div class="math notranslate nohighlight">
\[
L_{(j-1)} \bflambda_{(j)} = \bfbeta_{(j)}
\]</div>
<p>can be solved by forward substitution (since <span class="math notranslate nohighlight">\(\bfbeta_{(j)}\)</span> is part of the input and <span class="math notranslate nohighlight">\(L_{(j-1)}\)</span> was previously computed). The fact that this system has a unique solution (more specifically, that the diagonal entries of <span class="math notranslate nohighlight">\(L_{(j-1)}\)</span> are strictly positive) is established in the proof of the <em>Cholesky Decomposition Theorem</em>.</p>
<p>The <span class="math notranslate nohighlight">\((j,j)\)</span>-th entry of the matrix equation <span class="math notranslate nohighlight">\(L L^T = B\)</span> translates into</p>
<div class="math notranslate nohighlight">
\[
L_{j,\cdot} (L^T)_{\cdot,j} = \sum_{k=1}^j \ell_{j,k}^2 = b_{j,j},
\]</div>
<p>where again we used the fact that <span class="math notranslate nohighlight">\(L^T\)</span> is upper triangular. Since <span class="math notranslate nohighlight">\(\ell_{j,1}, \ldots, \ell_{j,j-1}\)</span> are the elements of <span class="math notranslate nohighlight">\(\bflambda_{(j)}\)</span>, they have already been determined. So we can set</p>
<div class="math notranslate nohighlight">
\[
\ell_{j,j}
= \sqrt{b_{j,j} - \sum_{k=1}^{j-1} \ell_{j,k}^2}.
\]</div>
<p>The fact that we are taking the square root of a positive quantity is established in the proof of the <em>Cholesky Decomposition Theorem</em>. Finally, from <span class="math notranslate nohighlight">\(L_{(j-1)}\)</span>, <span class="math notranslate nohighlight">\(\bflambda_{(j)}\)</span>, and <span class="math notranslate nohighlight">\(\ell_{j,j}\)</span>, we construct <span class="math notranslate nohighlight">\(L_{(j)}\)</span>.</p>
<p><strong>NUMERICAL CORNER:</strong> We implement the algorithm above. In our naive implementation, we assume that <span class="math notranslate nohighlight">\(B\)</span> is positive definite, and therefore that all steps are well-defined.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">cholesky</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">B</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> 
    <span class="n">L</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">L</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">forwardsubs</span><span class="p">(</span><span class="n">L</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">j</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="n">j</span><span class="p">],</span><span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="n">j</span><span class="p">])</span>
        <span class="n">L</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">-</span> <span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">L</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="n">j</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">L</span> 
</pre></div>
</div>
</div>
</div>
<p>Here is a simple example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[2. 1.]
 [1. 2.]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">L</span> <span class="o">=</span> <span class="n">cholesky</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[1.41421356 0.        ]
 [0.70710678 1.22474487]]
</pre></div>
</div>
</div>
</div>
<p>We can check that it produces the right factorization.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">L</span> <span class="o">@</span> <span class="n">L</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[2. 1.]
 [1. 2.]]
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>Proof of Cholesky decomposition theorem</strong> We give a proof of the <em>Cholesky Decomposition Theorem</em>.</p>
<p><em>Proof idea:</em> Assuming by induction that the upper-left corner of the matrix <span class="math notranslate nohighlight">\(B\)</span> has a Cholesky decomposition, one finds equations for the remaining row that can be solved uniquely by the properties established in the previous subsection.</p>
<p><em>Proof:</em> If <span class="math notranslate nohighlight">\(n=1\)</span>, we have shown previously that <span class="math notranslate nohighlight">\(b_{1,1} &gt; 0\)</span>, and hence we can take <span class="math notranslate nohighlight">\(L = [\ell_{1,1}]\)</span> where <span class="math notranslate nohighlight">\(\ell_{1,1} = \sqrt{b_{1,1}}\)</span>. Assuming the result holds for positive definite matrices in <span class="math notranslate nohighlight">\(\mathbb{R}^{(n-1) \times (n-1)}\)</span>, we first re-write <span class="math notranslate nohighlight">\(B = L L^T\)</span> in block form</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{pmatrix}
B_{11} &amp; \bfbeta_{12}\\
\bfbeta_{12}^T &amp; \beta_{22}
\end{pmatrix}
= 
\begin{pmatrix}
\Lambda_{11} &amp; \mathbf{0}\\
\bflambda_{12}^T &amp; \lambda_{22}
\end{pmatrix}
\begin{pmatrix}
\Lambda_{11}^T &amp; \bflambda_{12}\\
\mathbf{0}^T &amp; \lambda_{22}
\end{pmatrix}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(B_{11}, \Lambda_{11} \in \mathbb{R}^{n-1 \times n-1}\)</span>, <span class="math notranslate nohighlight">\(\bfbeta_{12}, \bflambda_{12} \in \mathbb{R}^{n-1}\)</span> and <span class="math notranslate nohighlight">\(\beta_{22}, \lambda_{22} \in \mathbb{R}\)</span>. By block matrix algebra, we get the system</p>
<div class="math notranslate nohighlight">
\[\begin{split}
B_{11} = \Lambda_{11} \Lambda_{11}^T\\
\bfbeta_{12} = \Lambda_{11} \bflambda_{12}\\
\beta_{22} = \bflambda_{12}^T \bflambda_{12} + \lambda_{22}^2.
\end{split}\]</div>
<p>By the <em>Principal Submatrices Lemma</em>, the principal submatrix <span class="math notranslate nohighlight">\(B_{11}\)</span> is positive definite. Hence, by induction, there is a unique lower-triangular matrix <span class="math notranslate nohighlight">\(\Lambda_{11}\)</span> with positive diagonal elements satisfying the first equation. We can then obtain <span class="math notranslate nohighlight">\(\bfbeta_{12}\)</span> from the second equation by forward substitution. And finally we get</p>
<div class="math notranslate nohighlight">
\[
\lambda_{22}
= \sqrt{\beta_{22} - \bflambda_{12}^T \bflambda_{12}}.
\]</div>
<p>We do have to check that the square root above exists. That is, we need to argue that the expression inside the square root is non-negative. In fact, for the claim to go through, we need it to be strictly positive. We notice that the expression inside the square root is in fact the Schur complement of the block <span class="math notranslate nohighlight">\(B_{11}\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\beta_{22} - \bflambda_{12}^T \bflambda_{12}
&amp;= \beta_{22} - (\Lambda_{11}^{-1} \bfbeta_{12})^T (\Lambda_{11}^{-1} \bfbeta_{12})\\
&amp;= \beta_{22} - \bfbeta_{12}^T (\Lambda_{11}^{-1})^T \Lambda_{11}^{-1} \bfbeta_{12}\\
&amp;= \beta_{22} - \bfbeta_{12}^T (\Lambda_{11} \Lambda_{11}^T)^{-1} \bfbeta_{12}\\
&amp;= \beta_{22} - \bfbeta_{12}^T (B_{11})^{-1} \bfbeta_{12}
\end{align*}\]</div>
<p>where we used the equation <span class="math notranslate nohighlight">\(\bfbeta_{12} = \Lambda_{11} \bflambda_{12}\)</span> on the first line, the identities <span class="math notranslate nohighlight">\((Q W)^{-1} = W^{-1} Q^{-1}\)</span> and <span class="math notranslate nohighlight">\((Q^T)^{-1} = (Q^{-1})^T\)</span> (see the exercise below) on the third line and the equation <span class="math notranslate nohighlight">\(B_{11} = \Lambda_{11} \Lambda_{11}^T\)</span> on the fourth line. By the <em>Schur Complement Lemma</em>, the Schur complement is positive definite. Because it is a scalar in this case, it is strictly positive (prove it!), which concludes the proof. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>Back to multivariate Gaussians</strong> Returning to our motivation, we can generate samples from a <span class="math notranslate nohighlight">\(N(\bmu, \bSigma)\)</span> by first generating</p>
<div class="math notranslate nohighlight">
\[
\mathbf{Z}
= (\Phi^{-1}(U_1),\ldots,\Phi^{-1}(U_d))
\]</div>
<p>where <span class="math notranslate nohighlight">\(U_1, \ldots, U_d\)</span> are independent uniform <span class="math notranslate nohighlight">\([0,1]\)</span> variables, then setting</p>
<div class="math notranslate nohighlight">
\[
\mathbf{X}
= \bmu + L \mathbf{Z},
\]</div>
<p>where <span class="math notranslate nohighlight">\(\bSigma = L L^T\)</span> is a Cholesky decomposition of <span class="math notranslate nohighlight">\(\bSigma\)</span>.</p>
<p><strong>NUMERICAL CORNER:</strong> We implement this method.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">generate_multivariate_normal_samples_using_cholesky</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">Sig</span><span class="p">):</span>

    <span class="c1"># Compute Cholesky decomposition</span>
    <span class="n">L</span> <span class="o">=</span> <span class="n">cholesky</span><span class="p">(</span><span class="n">Sig</span><span class="p">)</span>
    
    <span class="c1"># Initialization</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="n">d</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        
            <span class="c1"># Generate standard normal vector</span>
            <span class="n">Z</span> <span class="o">=</span> <span class="n">generate_standard_normal_samples_using_inverse_cdf</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="mi">0</span> <span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            
            <span class="c1"># Apply the inverse CDF (ppf) of the standard normal distribution</span>
            <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">L</span> <span class="o">@</span> <span class="n">Z</span> 
    
    <span class="k">return</span> <span class="n">X</span>
</pre></div>
</div>
</div>
</div>
<p>We generate some samples as an example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span><span class="mf">0.</span><span class="p">,</span><span class="mf">1.</span><span class="p">])</span>
<span class="n">Sig</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">]])</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">generate_multivariate_normal_samples_using_cholesky</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">Sig</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[-0.47926185  1.97223283  2.73780609]
 [-2.69005319 -4.19788834 -0.43130768]
 [ 0.41957285  3.91719212  2.08604427]
 [-2.11532949 -5.34557983  0.69521104]
 [-2.41203356 -1.84032486 -0.82207565]
 [-1.46121329  0.4821332   0.55005982]
 [-0.84981594  0.67074839  0.16360931]
 [-2.19097155 -1.98022929 -1.06365711]
 [-2.75113597 -3.47560492 -0.26607926]
 [ 0.130848    6.07312936 -0.08800829]]
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>Using a Cholesky decomposition to solve the least squares problem</strong> Another application of the Cholesky decomposition is to solving the least squares problem. In this section, we restrict ourselves to the case where <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n\times m}\)</span> has full column rank. By the <em>Least Squares and Positive Semidefiniteness Lemma</em>, we then have that <span class="math notranslate nohighlight">\(A^T A\)</span> is positive definite. By the <em>Cholesky Decomposition Theorem</em>, we can factorize this matrix as <span class="math notranslate nohighlight">\(A^T A = L L^T\)</span> where <span class="math notranslate nohighlight">\(L\)</span> is lower triangular with positive diagonal elements. The normal equations then reduce to</p>
<div class="math notranslate nohighlight">
\[
L L^T \mathbf{x} = A^T \mathbf{b}.
\]</div>
<p>This system can be solved in two steps. We first obtain the solution to</p>
<div class="math notranslate nohighlight">
\[
L \mathbf{z} = A^T \mathbf{b}
\]</div>
<p>by forward substitution. Then we obtain the solution to</p>
<div class="math notranslate nohighlight">
\[
L^T \mathbf{x} = \mathbf{z}
\]</div>
<p>by back-substitution. Note that <span class="math notranslate nohighlight">\(L^T\)</span> is indeed an upper triangular matrix.</p>
<p><strong>NUMERICAL CORNER:</strong> We implement this algorithm below. In our naive implementation, we assume that <span class="math notranslate nohighlight">\(A\)</span> has full column rank, and therefore that all steps are well-defined.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ls_by_chol</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">L</span> <span class="o">=</span> <span class="n">cholesky</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">A</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">forwardsubs</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">A</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">b</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mmids</span><span class="o">.</span><span class="n">backsubs</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p>Other applications of the Cholesky decomposition are briefly described <a class="reference external" href="https://en.wikipedia.org/wiki/Cholesky_decomposition#Applications">here</a>.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chap06_prob/supp"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../exercises/roch-mmids-prob-exercises.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">6.6. </span>Exercises</p>
      </div>
    </a>
    <a class="right-next"
       href="../../chap07_rwmc/00_intro/roch-mmids-rwmc-intro.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">7. </span>Random walks on graphs and Markov chains</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quizzes-solutions-code-etc">6.7.1. Quizzes, solutions, code, etc.</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#just-the-code">6.7.1.1. Just the code</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#self-assessment-quizzes">6.7.1.2. Self-assessment quizzes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#auto-quizzes">6.7.1.3. Auto-quizzes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#solutions-to-odd-numbered-warm-up-exercises">6.7.1.4. Solutions to odd-numbered warm-up exercises</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-outcomes">6.7.1.5. Learning outcomes</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-sections">6.7.2. Additional sections</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sentiment-analysis">6.7.2.1. Sentiment analysis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kalman-filtering-missing-data">6.7.2.2. Kalman filtering: missing data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cholesky-decomposition">6.7.2.3. Cholesky decomposition</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Sebastien Roch
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>