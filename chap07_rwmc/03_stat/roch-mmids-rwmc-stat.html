
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>7.3. Limit behavior 1: stationary distributions &#8212; MMiDS Textbook</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=87e54e7c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-P5E8DW088F"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-P5E8DW088F');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-P5E8DW088F');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chap07_rwmc/03_stat/roch-mmids-rwmc-stat';</script>
    <link rel="canonical" href="https://mmids-textbook.github.io/chap07_rwmc/03_stat/roch-mmids-rwmc-stat.html" />
    <link rel="icon" href="https://mmids-textbook.github.io/favicon-32x32.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="7.4. Limit behavior 2: convergence to equilibrium" href="../04_mclimit/roch-mmids-rwmc-mclimit.html" />
    <link rel="prev" title="7.2. Background: elements of finite Markov chains" href="../02_mcdefs/roch-mmids-rwmc-mcdefs.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/mmids-cover-small.jpg" class="logo__image only-light" alt="MMiDS Textbook - Home"/>
    <script>document.write(`<img src="../../_static/mmids-cover-small.jpg" class="logo__image only-dark" alt="MMiDS Textbook - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../index.html">
                    <b>MATHEMATICAL METHODS in DATA SCIENCE (with Python)</b>
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap01_intro/00_intro/roch-mmids-intro-intro.html">1. Introduction: a first data science problem</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap01_intro/01_motiv/roch-mmids-intro-motiv.html">1.1. Motivating example: identifying penguin species</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap01_intro/02_review/roch-mmids-intro-review.html">1.2. Background: quick refresher of matrix algebra, differential calculus, and elementary probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap01_intro/03_clustering/roch-mmids-intro-clustering.html">1.3. Clustering: an objective, an algorithm and a guarantee</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap01_intro/04_highdim/roch-mmids-intro-highdim.html">1.4. Some observations about high-dimensional data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap01_intro/exercises/roch-mmids-intro-exercises.html">1.5. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap01_intro/supp/roch-mmids-intro-supp.html">1.6. Online suppplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap02_ls/00_intro/roch-mmids-ls-intro.html">2. Least squares: geometric, algebraic, and numerical aspects</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/01_motiv/roch-mmids-ls-motiv.html">2.1. Motivating example: predicting sales</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/02_spaces/roch-mmids-ls-spaces.html">2.2. Background: review of vector spaces and matrix inverses</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/03_orthog/roch-mmids-ls-orthog.html">2.3. Geometry of least squares: the orthogonal projection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/04_qr/roch-mmids-ls-qr.html">2.4. QR decomposition and Householder transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/05_regression/roch-mmids-ls-regression.html">2.5. Application: regression analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/exercises/roch-mmids-ls-exercises.html">2.6. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/supp/roch-mmids-ls-supp.html">2.7. Online suppplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap03_opt/00_intro/roch-mmids-opt-intro.html">3. Optimization theory and algorithms</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap03_opt/01_motiv/roch-mmids-opt-motiv.html">3.1. Motivating example:  analyzing customer satisfaction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap03_opt/02_several/roch-mmids-opt-several.html">3.2. Background: review of differentiable functions of several variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap03_opt/03_optimality/roch-mmids-opt-optimality.html">3.3. Optimality conditions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap03_opt/04_convexity/roch-mmids-opt-convexity.html">3.4. Convexity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap03_opt/05_gd/roch-mmids-opt-gd.html">3.5. Gradient descent and its convergence analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap03_opt/06_logistic/roch-mmids-opt-logistic.html">3.6. Application: logistic regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap03_opt/exercises/roch-mmids-opt-exercises.html">3.7. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap03_opt/supp/roch-mmids-opt-supp.html">3.8. Online suppplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap04_svd/00_intro/roch-mmids-svd-intro.html">4. Singular value decomposition</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_svd/01_motiv/roch-mmids-svd-motiv.html">4.1. Motivating example: visualizing viral evolution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_svd/02_spectral/roch-mmids-svd-spectral.html">4.2. Background: review of matrix rank  and spectral decomposition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_svd/03_svd/roch-mmids-svd-svd.html">4.3. Approximating subspaces and the SVD</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_svd/04_power/roch-mmids-svd-power.html">4.4. Power iteration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_svd/05_pca/roch-mmids-svd-pca.html">4.5. Application: principal components analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_svd/06_further/roch-mmids-svd-further.html">4.6. Further applications of the SVD: low-rank approximations and ridge regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_svd/exercises/roch-mmids-svd-exercises.html">4.7. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_svd/supp/roch-mmids-svd-supp.html">4.8. Online suppplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap05_specgraph/00_intro/roch-mmids-specgraph-intro.html">5. Spectral graph theory</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap05_specgraph/01_motiv/roch-mmids-specgraph-motiv.html">5.1. Motivating example: uncovering social groups</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap05_specgraph/02_graph/roch-mmids-specgraph-graph.html">5.2. Background: basic concepts in graph theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap05_specgraph/03_extremal/roch-mmids-specgraph-extremal.html">5.3. Variational characterization of eigenvalues</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap05_specgraph/04_laplacian/roch-mmids-specgraph-laplacian.html">5.4. Spectral properties of the Laplacian matrix</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap05_specgraph/05_partitioning/roch-mmids-specgraph-partitioning.html">5.5. Application: graph partitioning via spectral clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap05_specgraph/06_sbm/roch-mmids-specgraph-sbm.html">5.6. Erdős-Rényi random graph and stochastic blockmodel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap05_specgraph/exercises/roch-mmids-specgraph-exercises.html">5.7. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap05_specgraph/supp/roch-mmids-specgraph-supp.html">5.8. Online supplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap06_prob/00_intro/roch-mmids-prob-intro.html">6. Probabilistic models: from simple to complex</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap06_prob/01_motiv/roch-mmids-prob-motiv.html">6.1. Motivating example: tracking location</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap06_prob/02_parametric/roch-mmids-prob-parametric.html">6.2. Background: introduction to parametric families and maximum likelihood estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap06_prob/03_joint/roch-mmids-prob-joint.html">6.3. Modeling more complex dependencies 1: using conditional independence</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap06_prob/04_em/roch-mmids-prob-em.html">6.4. Modeling more complex dependencies 2: marginalizing out an unobserved variable</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap06_prob/05_kalman/roch-mmids-prob-kalman.html">6.5. Application: linear-Gaussian models and Kalman filtering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap06_prob/exercises/roch-mmids-prob-exercises.html">6.6. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap06_prob/supp/roch-mmids-prob-supp.html">6.7. Online supplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../00_intro/roch-mmids-rwmc-intro.html">7. Random walks on graphs and Markov chains</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../01_motiv/roch-mmids-rwmc-motiv.html">7.1. Motivating example: discovering mathematical topics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02_mcdefs/roch-mmids-rwmc-mcdefs.html">7.2. Background: elements of finite Markov chains</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">7.3. Limit behavior 1: stationary distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04_mclimit/roch-mmids-rwmc-mclimit.html">7.4. Limit behavior 2: convergence to equilibrium</a></li>
<li class="toctree-l2"><a class="reference internal" href="../05_pagerank/roch-mmids-rwmc-pagerank.html">7.5. Application: random walks on graphs and PageRank</a></li>
<li class="toctree-l2"><a class="reference internal" href="../06_gibbs/roch-mmids-rwmc-gibbs.html">7.6. Further applications: Gibbs sampling and generating images</a></li>
<li class="toctree-l2"><a class="reference internal" href="../exercises/roch-mmids-rwmc-exercises.html">7.7. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../supp/roch-mmids-rwmc-supp.html">7.8. Online supplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap08_nn/00_intro/roch-mmids-nn-intro.html">8. Neural networks, backpropagation and stochastic gradient descent</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/01_motiv/roch-mmids-nn-motiv.html">8.1. Motivating example:  classifying natural images</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/02_chain/roch-mmids-nn-chain.html">8.2. Background: Jacobian, chain rule, and a brief introduction to automatic differentiation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/03_backprop/roch-mmids-nn-backprop.html">8.3. Building blocks of AI 1: backpropagation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/04_sgd/roch-mmids-nn-sgd.html">8.4. Building blocks of AI 2: stochastic gradient descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/05_nn/roch-mmids-nn-nn.html">8.5. Building blocks of AI 3: neural networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/exercises/roch-mmids-nn-exercises.html">8.6. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/supp/roch-mmids-nn-supp.html">8.7. Online supplementary material</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/issues/new?title=Issue%20on%20page%20%2Fchap07_rwmc/03_stat/roch-mmids-rwmc-stat.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/chap07_rwmc/03_stat/roch-mmids-rwmc-stat.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Limit behavior 1: stationary distributions</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#definitions">7.3.1. Definitions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#existence">7.3.2. Existence</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <p><span class="math notranslate nohighlight">\(\newcommand{\bmu}{\boldsymbol{\mu}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bSigma}{\boldsymbol{\Sigma}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bfbeta}{\boldsymbol{\beta}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bflambda}{\boldsymbol{\lambda}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bgamma}{\boldsymbol{\gamma}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bsigma}{{\boldsymbol{\sigma}}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bpi}{\boldsymbol{\pi}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\btheta}{{\boldsymbol{\theta}}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bphi}{\boldsymbol{\phi}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\balpha}{\boldsymbol{\alpha}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\blambda}{\boldsymbol{\lambda}}\)</span>
<span class="math notranslate nohighlight">\(\renewcommand{\P}{\mathbb{P}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\E}{\mathbb{E}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\indep}{\perp\!\!\!\perp} \newcommand{\bx}{\mathbf{x}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bp}{\mathbf{p}}\)</span>
<span class="math notranslate nohighlight">\(\renewcommand{\bx}{\mathbf{x}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bX}{\mathbf{X}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\by}{\mathbf{y}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bY}{\mathbf{Y}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bz}{\mathbf{z}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bZ}{\mathbf{Z}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bw}{\mathbf{w}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bW}{\mathbf{W}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bv}{\mathbf{v}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bV}{\mathbf{V}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bfg}{\mathbf{g}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bfh}{\mathbf{h}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\horz}{\rule[.5ex]{2.5ex}{0.5pt}}\)</span>
<span class="math notranslate nohighlight">\(\renewcommand{\S}{\mathcal{S}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\X}{\mathcal{X}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\var}{\mathrm{Var}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\pa}{\mathrm{pa}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\Z}{\mathcal{Z}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bh}{\mathbf{h}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bb}{\mathbf{b}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bc}{\mathbf{c}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\cE}{\mathcal{E}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\cP}{\mathcal{P}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bbeta}{\boldsymbol{\beta}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bLambda}{\boldsymbol{\Lambda}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\cov}{\mathrm{Cov}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bfk}{\mathbf{k}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\idx}[1]{}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\xdi}{}\)</span></p>
<section id="limit-behavior-1-stationary-distributions">
<h1><span class="section-number">7.3. </span>Limit behavior 1: stationary distributions<a class="headerlink" href="#limit-behavior-1-stationary-distributions" title="Link to this heading">#</a></h1>
<p>We continue our exploration of basic Markov chain theory. In this section, we begin our study of the long-term behavior of a chain. As we did in the previous section, we restrict ourselves to finite-space discrete-time Markov chains that are also time-homogeneous.</p>
<section id="definitions">
<h2><span class="section-number">7.3.1. </span>Definitions<a class="headerlink" href="#definitions" title="Link to this heading">#</a></h2>
<p>An important property of Markov chains is that, when run for long enough, they converge to a sort of “equilibrium.” We develop parts of this theory here. What do we mean by “equilibrium”? Here is the key definition.</p>
<p><strong>DEFINITION</strong> <strong>(Stationary Distribution)</strong> <span class="math notranslate nohighlight">\(\idx{stationary distribution}\xdi\)</span> Let <span class="math notranslate nohighlight">\((X_t)_{t \geq 0}\)</span> be a Markov chain on <span class="math notranslate nohighlight">\(\mathcal{S} = [n]\)</span> with transition matrix <span class="math notranslate nohighlight">\(P = (p_{i,j})_{i,j=1}^n\)</span>. A probability distribution <span class="math notranslate nohighlight">\(\bpi = (\pi_i)_{i=1}^n\)</span> over <span class="math notranslate nohighlight">\([n]\)</span> is a stationary distribution of <span class="math notranslate nohighlight">\((X_t)_{t \geq 0}\)</span> (or of <span class="math notranslate nohighlight">\(P\)</span>) if:</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^n \pi_i p_{i,j} = \pi_j, \qquad \forall j \in \mathcal{S}.
\]</div>
<p><span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>In matrix form, this condition can be stated as</p>
<div class="math notranslate nohighlight">
\[
\bpi P = \bpi,
\]</div>
<p>where recall that we think of <span class="math notranslate nohighlight">\(\bpi\)</span> as a row vector. One way to put this is that <span class="math notranslate nohighlight">\(\bpi\)</span> is a fixed point of <span class="math notranslate nohighlight">\(P\)</span> (through multiplication from the left). Another way to put it is that <span class="math notranslate nohighlight">\(\bpi\)</span> is a left (row) eigenvector of <span class="math notranslate nohighlight">\(P\)</span> with eigenvalue <span class="math notranslate nohighlight">\(1\)</span>.</p>
<p>To see why a stationary distribution is indeed an equilibrium, we note the following.</p>
<p><strong>LEMMA</strong> <strong>(Stationarity)</strong> <span class="math notranslate nohighlight">\(\idx{stationarity lemma}\xdi\)</span> Let <span class="math notranslate nohighlight">\(\mathbf{z} \in \mathbb{R}^n\)</span> be a left eigenvector of transition matrix <span class="math notranslate nohighlight">\(P \in \mathbb{R}^{n \times n}\)</span> with eigenvalue <span class="math notranslate nohighlight">\(1\)</span>. Then <span class="math notranslate nohighlight">\(\mathbf{z} P^s = \mathbf{z}\)</span> for all integers <span class="math notranslate nohighlight">\(s \geq 0\)</span>. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> Indeed,</p>
<div class="math notranslate nohighlight">
\[
\mathbf{z} P^s
= (\mathbf{z} P)P^{s-1}
= \mathbf{z} P^{s-1}
= (\mathbf{z} P) P^{s-2}
= \mathbf{z} P^{s-2}
= \cdots
= \mathbf{z}.
\]</div>
<p><span class="math notranslate nohighlight">\(\square\)</span></p>
<p>Suppose the initial distribution is equal to a stationary distribution <span class="math notranslate nohighlight">\(\bpi\)</span>. Then, from the <em>Time Marginals Theorem</em> and the <em>Stationarity Lemma</em>, the distribution at <em>any time <span class="math notranslate nohighlight">\(s \geq 1\)</span></em> is</p>
<div class="math notranslate nohighlight">
\[
\bpi P^s
= \bpi.
\]</div>
<p>That is, the distribution at all times indeed remains stationary.</p>
<p>In the next section we will derive a remarkable fact:  under certain conditions, a Markov chain started from an arbitrary initial distribution converges in the limit of <span class="math notranslate nohighlight">\(t \to +\infty\)</span> to a stationary distribution.</p>
<p><strong>EXAMPLE:</strong> <strong>(Weather Model, continued)</strong> Going back to the <em>Weather Model</em>, we compute a stationary distribution. We need <span class="math notranslate nohighlight">\(\bpi = (\pi_1, \pi_2)\)</span> to satisfy</p>
<div class="math notranslate nohighlight">
\[\begin{split}
(\pi_1, \pi_2)
\begin{pmatrix}
3/4 &amp; 1/4\\
1/4 &amp; 3/4
\end{pmatrix}
= (\pi_1, \pi_2)
\end{split}\]</div>
<p>that is,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\frac{3}{4} \pi_1 + \frac{1}{4} \pi_2 = \pi_1\\
&amp;\frac{1}{4} \pi_1 + \frac{3}{4} \pi_2 = \pi_2.
\end{align*}\]</div>
<p>You can check that, after rearranging, these two equations are in fact the same one.</p>
<p>Note, however, that we have some further restrictions: <span class="math notranslate nohighlight">\(\bpi\)</span> is a probability distribution. So <span class="math notranslate nohighlight">\(\pi_1, \pi_2 \geq 0\)</span> and <span class="math notranslate nohighlight">\(\pi_1 + \pi_2 = 1\)</span>. Replacing the latter in the first equation we get</p>
<div class="math notranslate nohighlight">
\[
\frac{3}{4} \pi_1 + \frac{1}{4} (1 - \pi_1) = \pi_1
\]</div>
<p>so that we require</p>
<div class="math notranslate nohighlight">
\[
\pi_1 = \frac{1/4}{1/2} = \frac{1}{2}.
\]</div>
<p>And <span class="math notranslate nohighlight">\(\pi_2 = 1 - \pi_1 = 1/2\)</span>. The second equation above is also automatically satisfied (why?). <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p>The previous example is quite special. It generalizes to all doubly stochastic matrices (including the <em>Random Walk on the Petersen Graph</em> for instance). Indeed, we claim that the uniform distribution is always a stationary distribution in the doubly stochastic case. Let <span class="math notranslate nohighlight">\(P = (p_{i,j})_{i,j=1}^n\)</span> be doubly stochastic over <span class="math notranslate nohighlight">\([n]\)</span> and let <span class="math notranslate nohighlight">\(\bpi = (\pi_i)_{i=1}^n\)</span> be the uniform distribution on <span class="math notranslate nohighlight">\([n]\)</span>. Then for all <span class="math notranslate nohighlight">\(j \in [n]\)</span></p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^n \pi_i p_{i,j} 
= \sum_{i=1}^n \frac{1}{n} p_{i,j}
= \frac{1}{n} \sum_{i=1}^n p_{i,j}
= \frac{1}{n}
= \pi_i
\]</div>
<p>because the columns sum to <span class="math notranslate nohighlight">\(1\)</span>. That proves the claim.</p>
<p>Is a stationary distribution guaranteed to exist? Is it unique? To answer this question, we first need some graph-theoretic concepts relevant to the long-term behavior of the chain.</p>
<p><strong>DEFINITION</strong> <strong>(<span class="math notranslate nohighlight">\(x \to y\)</span>)</strong> A state <span class="math notranslate nohighlight">\(x \in \S\)</span> is said to communicate with a state <span class="math notranslate nohighlight">\(y \in \S\)</span> if there exists a sequence of states <span class="math notranslate nohighlight">\(z_0 = x, z_1, z_2, \ldots, z_{r-1}, z_r = y\)</span> such that for all <span class="math notranslate nohighlight">\(\ell = 1,\ldots,r\)</span></p>
<div class="math notranslate nohighlight">
\[
p_{z_{\ell-1},z_\ell} &gt; 0.
\]</div>
<p>We denote this property as <span class="math notranslate nohighlight">\(x \to y\)</span>. <span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>In terms of the transition graph of the chain, the condition <span class="math notranslate nohighlight">\(x \to y\)</span> says that there exists a directed path from <span class="math notranslate nohighlight">\(x\)</span> to <span class="math notranslate nohighlight">\(y\)</span>. It is important to see the difference between: (1) the existence of a direct edge from <span class="math notranslate nohighlight">\(x\)</span> to <span class="math notranslate nohighlight">\(y\)</span> (which implies <span class="math notranslate nohighlight">\(x \to y\)</span> but is not necessary) and (2) the existence of a directed path from <span class="math notranslate nohighlight">\(x\)</span> to <span class="math notranslate nohighlight">\(y\)</span>. See the next example.</p>
<p><strong>EXAMPLE:</strong> <strong>(Robot Vacuum, continued)</strong> Going back to the <em>Robot Vacuum Example</em>, recall the transition graph. While there is no direct edge from <span class="math notranslate nohighlight">\(4\)</span> to <span class="math notranslate nohighlight">\(3\)</span>, we do have <span class="math notranslate nohighlight">\(4 \to 3\)</span> through the path <span class="math notranslate nohighlight">\((4,2), (2,3)\)</span>. Do we have <span class="math notranslate nohighlight">\(3 \to 4\)</span>? <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p>Here is an important consequence of this graph-theoretic notion on the long-term behavior of the chain.</p>
<p><strong>LEMMA</strong> <strong>(Communication)</strong> <span class="math notranslate nohighlight">\(\idx{communication lemma}\xdi\)</span> If <span class="math notranslate nohighlight">\(x \to y\)</span>, then there is an integer <span class="math notranslate nohighlight">\(r \geq 1\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
\P[X_r = y\,|\,X_0 = x] = (\mathbf{e}_x^T P^r)_y = (P^r)_{x,y} &gt; 0.
\]</div>
<p><span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof idea:</em> We lower bound the probability in the statement with the probability of visiting the particular sequence of states in the definition of <span class="math notranslate nohighlight">\(x \to y\)</span>.</p>
<p><em>Proof:</em> By definition of <span class="math notranslate nohighlight">\(x \to y\)</span>, there exists a sequence of states <span class="math notranslate nohighlight">\(z_0 = x, z_1, z_2, \ldots, z_{r-1}, z_r = y\)</span> such that, for all <span class="math notranslate nohighlight">\(\ell = 1,\ldots,r\)</span>, <span class="math notranslate nohighlight">\(p_{z_{\ell-1},z_\ell} &gt; 0\)</span>. Hence,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\P[X_r = y\,|\,X_0 = x]\\
&amp;\geq \P[X_1=z_1,X_2=z_2,\ldots,X_{r-1}= z_{r-1}, X_r = y\,|\,X_0 = x]\\
&amp;= \prod_{\ell=1}^r \P[X_\ell = z_\ell\,|\,X_{\ell-1} = z_{\ell-1}]\\
&amp;= \prod_{\ell=1}^r p_{z_{\ell-1},z_\ell} &gt; 0,
\end{align*}\]</div>
<p>as claimed. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>The following example shows that the probability in the lemma is positive, but may not be <span class="math notranslate nohighlight">\(1\)</span>. It also gives some insights about the question of the uniqueness of the stationary distribution.</p>
<p><strong>NUMERICAL CORNER:</strong> Consider random walk on the following digraph, which we refer to as the <em>Two Sinks Example</em> (why do you think?).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">G_sinks</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">DiGraph</span><span class="p">()</span>
<span class="n">n_sinks</span> <span class="o">=</span> <span class="mi">5</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sinks</span><span class="p">):</span>
    <span class="n">G_sinks</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>

<span class="n">G_sinks</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">)</span>
<span class="n">G_sinks</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">)</span>
<span class="n">G_sinks</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">)</span>
<span class="n">G_sinks</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">)</span>
<span class="n">G_sinks</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">G_sinks</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">G_sinks</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">)</span>
<span class="n">G_sinks</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">)</span>
<span class="n">G_sinks</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G_sinks</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="n">nx</span><span class="o">.</span><span class="n">circular_layout</span><span class="p">(</span><span class="n">G_sinks</span><span class="p">),</span> 
                 <span class="n">labels</span><span class="o">=</span><span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sinks</span><span class="p">)},</span> 
                 <span class="n">node_color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> 
                 <span class="n">connectionstyle</span><span class="o">=</span><span class="s1">&#39;arc3, rad = -0.2&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/e7e03bf0a4fe013cfae58268f9b8102d5f4331c31372b782ad66045ce6adab36.png" src="../../_images/e7e03bf0a4fe013cfae58268f9b8102d5f4331c31372b782ad66045ce6adab36.png" />
</div>
</div>
<p>Here we have <span class="math notranslate nohighlight">\(1 \to 4\)</span> (Why?). The <em>Communication Lemma</em> implies that, when started at <span class="math notranslate nohighlight">\(1\)</span>, <span class="math notranslate nohighlight">\((X_t)_{t \geq 0}\)</span> visits <span class="math notranslate nohighlight">\(4\)</span> with positive probability. But that probability is not one. Indeed we also have <span class="math notranslate nohighlight">\(1 \to 3\)</span> (Why?), so there is a positive probability of visiting <span class="math notranslate nohighlight">\(3\)</span> as well. But if we do so before visiting <span class="math notranslate nohighlight">\(4\)</span>, we stay at <span class="math notranslate nohighlight">\(3\)</span> forever hence cannot subsequently reach <span class="math notranslate nohighlight">\(4\)</span>.</p>
<p>In fact, intuitively, if we run this chain long enough we will either get stuck at <span class="math notranslate nohighlight">\(3\)</span> or get stuck at <span class="math notranslate nohighlight">\(4\)</span>. These give rise to different stationary distributions. The transition probability is the following.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">P_sinks</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">adjacency_matrix</span><span class="p">(</span><span class="n">G_sinks</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">P_sinks</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[0.33333333 0.33333333 0.         0.         0.33333333]
 [0.         0.33333333 0.33333333 0.         0.33333333]
 [0.         0.         1.         0.         0.        ]
 [0.         0.         0.         1.         0.        ]
 [0.         0.         0.         1.         0.        ]]
</pre></div>
</div>
</div>
</div>
<p>It is easy to check that <span class="math notranslate nohighlight">\(\bpi = (0,0,1,0,0)\)</span> and <span class="math notranslate nohighlight">\(\bpi' = (0,0,0,1,0)\)</span> are both stationary distributions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span><span class="mf">0.</span><span class="p">,</span><span class="mf">1.</span><span class="p">,</span><span class="mf">0.</span><span class="p">,</span><span class="mf">0.</span><span class="p">])</span>
<span class="n">pi_prime</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span><span class="mf">0.</span><span class="p">,</span><span class="mf">0.</span><span class="p">,</span><span class="mf">1.</span><span class="p">,</span><span class="mf">0.</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">P_sinks</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">pi</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0., 0., 1., 0., 0.])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">P_sinks</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">pi_prime</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0., 0., 0., 1., 0.])
</pre></div>
</div>
</div>
</div>
<p>In fact, there are infinitely many stationary distributions in this case.</p>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p>To avoid the behavior in the previous example, we introduce the following assumption.</p>
<p><strong>DEFINITION</strong> <strong>(Irreducibility)</strong> <span class="math notranslate nohighlight">\(\idx{irreducibility}\xdi\)</span> A Markov chain on <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> is irreducible if for all <span class="math notranslate nohighlight">\(x, y \in \mathcal{S}\)</span> with <span class="math notranslate nohighlight">\(x \neq y\)</span> we have <span class="math notranslate nohighlight">\(x \to y\)</span> and <span class="math notranslate nohighlight">\(y \to x\)</span>. We also refer to the transition matrix as irreducible in that case. <span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>In graphical terms, a Markov chain is irreducible if and only if its transition graph is strongly connected.</p>
<p><strong>NUMERICAL CORNER:</strong> Because irreducibility is ultimately a graph-theoretic property, it is easy to check using <code class="docutils literal notranslate"><span class="pre">NetworkX</span></code>. For this, we use the function <a class="reference external" href="https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.components.is_strongly_connected.html"><code class="docutils literal notranslate"><span class="pre">is_strongly_connected()</span></code></a>. Revisiting the <em>Robot Vacuum Example</em>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">P_robot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                    <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                    <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.35</span><span class="p">,</span> <span class="mf">0.35</span><span class="p">],</span>
                    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
                    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>

<span class="n">G_robot</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">from_numpy_array</span><span class="p">(</span><span class="n">P_robot</span><span class="p">,</span> <span class="n">create_using</span><span class="o">=</span><span class="n">nx</span><span class="o">.</span><span class="n">DiGraph</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">nx</span><span class="o">.</span><span class="n">is_strongly_connected</span><span class="p">(</span><span class="n">G_robot</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
<p>Consider again the <em>Two Sinks Example</em>. It turns out not to be irreducible:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">nx</span><span class="o">.</span><span class="n">is_strongly_connected</span><span class="p">(</span><span class="n">G_sinks</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>False
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
</section>
<section id="existence">
<h2><span class="section-number">7.3.2. </span>Existence<a class="headerlink" href="#existence" title="Link to this heading">#</a></h2>
<p>In the irreducible case, it turns out that a stationary distribution always exists – and is in fact unique. (The presence of a single strongly connected component suffices for uniqueness to hold in this case, but we will not derive this here. Also, in the finite case, a stationary distribution always exists, but again we will not prove this here.)</p>
<p><strong>THEOREM</strong> <strong>(Existence of Stationary Distribution)</strong> <span class="math notranslate nohighlight">\(\idx{existence of stationary distribution}\xdi\)</span> Let <span class="math notranslate nohighlight">\(P\)</span> be an irreducible transition matrix on <span class="math notranslate nohighlight">\([n]\)</span>. Then there exists a unique stationary distribution <span class="math notranslate nohighlight">\(\bpi\)</span>. Further all entries of <span class="math notranslate nohighlight">\(\bpi\)</span> are strictly positive. <span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p><em>Proof:</em> The proof is not straightforward. We make a series of claims to establish existence.</p>
<p><strong>LEMMA</strong> <strong>(Step 1)</strong> There is a non-zero row vector <span class="math notranslate nohighlight">\(\mathbf{z} \in \mathbb{R}^n\)</span> such that <span class="math notranslate nohighlight">\(\mathbf{z} P = \mathbf{z}\)</span>. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><strong>LEMMA</strong> <strong>(Step 2)</strong> Let <span class="math notranslate nohighlight">\(\mathbf{z} \in \mathbb{R}^n\)</span> be a non-zero row vector <span class="math notranslate nohighlight">\(\mathbf{z} P = \mathbf{z}\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
\bpi = \frac{\mathbf{z}}{\sum_{x} z_x}
\]</div>
<p>is a strictly positive stationary distribution of <span class="math notranslate nohighlight">\(P\)</span>. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><strong>LEMMA</strong> <strong>(Step 3)</strong> Let <span class="math notranslate nohighlight">\(\bpi_1\)</span> and <span class="math notranslate nohighlight">\(\bpi_2\)</span> be stationary distributions of <span class="math notranslate nohighlight">\(P\)</span>. Then <span class="math notranslate nohighlight">\(\bpi_1 = \bpi_2\)</span>. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> <em>(Lemma (Step 1))</em> Because <span class="math notranslate nohighlight">\(P\)</span> is stochastic, we have by definition that <span class="math notranslate nohighlight">\(P \mathbf{1} = \mathbf{1}\)</span>. Put differently,</p>
<div class="math notranslate nohighlight">
\[
(P - I) \mathbf{1} = \mathbf{0}
\]</div>
<p>that is, the columns of <span class="math notranslate nohighlight">\(P - I\)</span> are linearly dependent. In particular <span class="math notranslate nohighlight">\(\mathrm{rk}(P-I) &lt; n\)</span>. That in turn implies that the rows of <span class="math notranslate nohighlight">\(P - I\)</span> are linearly dependent by the <em>Row Rank Equals Column Rank Theorem</em>. So there exists a non-zero row vector <span class="math notranslate nohighlight">\(\mathbf{z} \in \mathbb{R}^n\)</span> such that <span class="math notranslate nohighlight">\(\mathbf{z}(P-I) = \mathbf{0}\)</span>, or after rearranging</p>
<div class="math notranslate nohighlight">
\[
\mathbf{z}P = \mathbf{z}.
\]</div>
<p>That proves the claim. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><em>Proof:</em> <em>(Lemma (Step 2))</em> We break up the proof into several claims.</p>
<p>To take advantage of irreducibility, we first construct a positive stochastic matrix with <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> as a left eigenvector of eigenvalue <span class="math notranslate nohighlight">\(1\)</span>. We then show that all entries of <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> have the same sign. Finally, we normalize <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>.</p>
<p><strong>LEMMA</strong> <strong>(Step 2a)</strong> There exists a non-negative integer <span class="math notranslate nohighlight">\(h\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
R = \frac{1}{h+1}[I + P + P^2 + \cdots + P^h]
\]</div>
<p>has only strictly positive entries and satisfies <span class="math notranslate nohighlight">\(\mathbf{z} R = \mathbf{z}\)</span>. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><strong>LEMMA</strong> <strong>(Step 2b)</strong> The entries of <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> are either all non-negative or all non-positive. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><strong>LEMMA</strong> <strong>(Step 2c)</strong> Let <span class="math notranslate nohighlight">\(\bpi = \frac{\mathbf{z}}{\mathbf{z}\mathbf{1}}\)</span>. Then <span class="math notranslate nohighlight">\(\bpi\)</span> is a strictly positive stationary distribution. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p>We prove the claims next.</p>
<p><em>Proof:</em> <em>(Lemma (Step 2a))</em> By irreducibility and the <em>Communication Lemma</em>, for any <span class="math notranslate nohighlight">\(x, y \in [n]\)</span> there is <span class="math notranslate nohighlight">\(h_{xy}\)</span> such that <span class="math notranslate nohighlight">\((P^{h_{xy}})_{x,y} &gt; 0\)</span>. Now define</p>
<div class="math notranslate nohighlight">
\[
h = \max_{x,y \in [n]} h_{xy}.
\]</div>
<p>It can be shown (Try it!) that <span class="math notranslate nohighlight">\(P^s\)</span> (as a product of stochastic matrices) is itself a stochastic matrix for all <span class="math notranslate nohighlight">\(s\)</span>. In particular, it has nonnegative entries. Hence, for each <span class="math notranslate nohighlight">\(x,y\)</span>,</p>
<div class="math notranslate nohighlight">
\[
R_{x,y}
= \frac{1}{h+1}[I_{x,y} + P_{x,y} + (P^2)_{x,y} + \cdots + (P^h)_{x,y}]
\geq \frac{1}{h+1} (P^{h_{x,y}})_{x,y} &gt; 0.
\]</div>
<p>It can be shown (Try it!) that <span class="math notranslate nohighlight">\(R\)</span> (as a convex combination of stochastic matrices) is itself a stochastic matrix.</p>
<p>Moreover, by the <em>Stationarity Lemma</em>, since <span class="math notranslate nohighlight">\(\mathbf{z} P = \mathbf{z}\)</span> it follows that <span class="math notranslate nohighlight">\(\mathbf{z} P^s = \mathbf{z}\)</span> for all <span class="math notranslate nohighlight">\(s\)</span>. Therefore,</p>
<div class="math notranslate nohighlight">
\[
\mathbf{z} R 
= \frac{1}{h+1}[\mathbf{z}I + \mathbf{z}P + \mathbf{z}P^2 + \cdots + \mathbf{z}P^h]
= \frac{1}{h+1}[\mathbf{z} + \mathbf{z} + \mathbf{z} + \cdots + \mathbf{z}]
= \mathbf{z}.
\]</div>
<p>That concludes the proof. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><em>Proof:</em> <em>(Lemma (Step 2b))</em> We argue by contradiction. Suppose that two entries of <span class="math notranslate nohighlight">\(\mathbf{z} = (z_x)_{x \in [n]}\)</span> have different signs. Say <span class="math notranslate nohighlight">\(z_i &gt; 0\)</span> while <span class="math notranslate nohighlight">\(z_j &lt; 0\)</span>. Let <span class="math notranslate nohighlight">\(R = (r_{x,y})_{x,y=1}^n\)</span>. By the previous claim, <span class="math notranslate nohighlight">\(r_{x,y} &gt; 0\)</span> for all <span class="math notranslate nohighlight">\(x,y\)</span>, therefore</p>
<div class="math notranslate nohighlight">
\[
|z_y|
= \left|\sum_{x} z_x r_{x,y}\right|
= \left|\sum_{x: z_x \geq 0} z_x r_{x,y}
+ \sum_{x: z_x &lt; 0} z_x r_{x,y}\right|.
\]</div>
<p>The first term on the right-hand side is strictly positive (since it is at least <span class="math notranslate nohighlight">\(z_i r_{i,y} &gt; 0\)</span>)
while the second term is strictly negative (since it is at most <span class="math notranslate nohighlight">\(z_j r_{j,y} &lt; 0\)</span>). Hence, because of cancellations, the expression above is strictly smaller than the sum of the absolute values</p>
<div class="math notranslate nohighlight">
\[
|z_y|
&lt; \sum_{x} |z_x| r_{x,y}.
\]</div>
<p>Since <span class="math notranslate nohighlight">\(R\)</span> is stochastic by the previous claim, we deduce after summing over <span class="math notranslate nohighlight">\(y\)</span></p>
<div class="math notranslate nohighlight">
\[
\sum_{y} |z_y|
&lt; \sum_{y} \sum_{x} |z_x| r_{x,y}
= \sum_{x} |z_x|  \sum_{y}  r_{x,y}
= \sum_{x} |z_x|,
\]</div>
<p>a contradiction, proving the claim. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><em>Proof:</em> <em>(Lemma (Step 2c))</em> Now define <span class="math notranslate nohighlight">\(\bpi = (\pi_x)_{x \in [n]}\)</span> as</p>
<div class="math notranslate nohighlight">
\[
\pi_ x 
= \frac{z_x}{\sum_{i} z_i}
= \frac{|z_x|}{\sum_{i} |z_i|}
\geq 0,
\]</div>
<p>where the second equality comes from the previous claim. We also used the fact that <span class="math notranslate nohighlight">\(\mathbf{z} \neq \mathbf{0}\)</span>. For all <span class="math notranslate nohighlight">\(y\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\sum_{x} \pi_x p_{x,y}
= \sum_{x} \frac{z_x}{\sum_{i} z_i} p_{x,y}
= \frac{1}{\sum_{i} z_i} \sum_{x} z_x p_{x,y}
= \frac{z_y}{\sum_{i} z_i}
= \pi_y.
\]</div>
<p>The same holds with <span class="math notranslate nohighlight">\(p_{x,y}\)</span> replaced with <span class="math notranslate nohighlight">\(r_{x,y}\)</span> by <em>Claim 3</em>. Since <span class="math notranslate nohighlight">\(r_{x,y} &gt; 0\)</span> and <span class="math notranslate nohighlight">\(\mathbf{z} \neq \mathbf{0}\)</span> it follows that <span class="math notranslate nohighlight">\(\pi_y &gt; 0\)</span> for all <span class="math notranslate nohighlight">\(y\)</span>. That proves the claim. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>That concludes the proof of <em>Lemma (Step 2)</em>. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>It remains to prove uniqueness.</p>
<p>Suppose there are two distinct stationary distributions <span class="math notranslate nohighlight">\(\bpi_1\)</span> and <span class="math notranslate nohighlight">\(\bpi_2\)</span>. Since they are distinct, they are not a multiple of each other and therefore are linearly independent. Apply the Gram-Schmidt algorithm:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{q}_1 = \frac{\bpi_1}{\|\bpi_1\|}
\qquad
\text{and}
\qquad\mathbf{q}_2 = \frac{\bpi_2 - \langle \bpi_2, \mathbf{q}_1 \rangle \mathbf{q}_1}{\|\bpi_2 - \langle \bpi_2, \mathbf{q}_1 \rangle \mathbf{q}_1\|}.
\]</div>
<p>Then</p>
<div class="math notranslate nohighlight">
\[
\mathbf{q}_1 P = \frac{\bpi_1}{\|\bpi_1\|} P = \frac{\bpi_1 P}{\|\bpi_1\|} = \frac{\bpi_1}{\|\bpi_1\|} = \mathbf{q}_1
\]</div>
<p>and all entries of <span class="math notranslate nohighlight">\(\mathbf{q}_1\)</span> are strictly positive.</p>
<p>Similarly,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{q}_2 P 
&amp;= \frac{\bpi_2 - \langle \bpi_2, \mathbf{q}_1 \rangle \mathbf{q}_1}{\|\bpi_2 - \langle \bpi_2, \mathbf{q}_1 \rangle \mathbf{q}_1\|} P\\
&amp;= \frac{\bpi_2 P - \langle \bpi_2, \mathbf{q}_1 \rangle \mathbf{q}_1 P}{\|\bpi_2 - \langle \bpi_2, \mathbf{q}_1 \rangle \mathbf{q}_1\|}\\
&amp;= \frac{\bpi_2 - \langle \bpi_2, \mathbf{q}_1 \rangle \mathbf{q}_1}{\|\bpi_2 - \langle \bpi_2, \mathbf{q}_1 \rangle \mathbf{q}_1\|}\\
&amp;= \mathbf{q}_2.
\end{align*}\]</div>
<p>By the claims above, there is a multiple of <span class="math notranslate nohighlight">\(\mathbf{q}_2\)</span>, say <span class="math notranslate nohighlight">\(\mathbf{q}_2' = \alpha \mathbf{q}_2\)</span> with <span class="math notranslate nohighlight">\(\alpha \neq 0\)</span>, such that <span class="math notranslate nohighlight">\(\mathbf{q}_2' P = \mathbf{q}_2'\)</span> and all entries of <span class="math notranslate nohighlight">\(\mathbf{q}_2'\)</span> are strictly positive.</p>
<p>By the properties of the Gram-Schmidt algorithm,</p>
<div class="math notranslate nohighlight">
\[
\langle \mathbf{q}_1, \mathbf{q}_2' \rangle 
= \langle \mathbf{q}_1, \alpha \mathbf{q}_2 \rangle 
= \alpha \langle \mathbf{q}_1, \mathbf{q}_2 \rangle 
= 0.
\]</div>
<p>But this is a contradiction – both vectors are strictly positive. That concludes the proof. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>A few more observations about the eigenvalues of <span class="math notranslate nohighlight">\(P\)</span>.</p>
<p>(1) Whenever <span class="math notranslate nohighlight">\(\lambda\)</span> is a left eigenvalue of <span class="math notranslate nohighlight">\(P\)</span> (i.e. <span class="math notranslate nohighlight">\(\mathbf{z} P = \lambda \mathbf{z}\)</span> for some <span class="math notranslate nohighlight">\(\mathbf{z} \in \mathbb{R}^n\)</span> as a row vector), it is also a right eigenvalue of <span class="math notranslate nohighlight">\(P\)</span> (i.e. <span class="math notranslate nohighlight">\(P \mathbf{y} = \lambda \mathbf{y}\)</span> for some <span class="math notranslate nohighlight">\(\mathbf{y} \in \mathbb{R}^n\)</span>). One way to see this using our previous results is to note that <span class="math notranslate nohighlight">\(\mathbf{z} P = \lambda \mathbf{z}\)</span> is equivalent to <span class="math notranslate nohighlight">\(P^T \mathbf{z}^T = \lambda \mathbf{z}^T\)</span>, or put differently <span class="math notranslate nohighlight">\((P^T - \lambda I) \mathbf{z}^T = \mathbf{0}\)</span>, so that</p>
<div class="math notranslate nohighlight">
\[
\mathbf{z}^T \in \mathrm{null}(P^T - \lambda I).
\]</div>
<p>Similarly, <span class="math notranslate nohighlight">\(P \mathbf{y} = \lambda \mathbf{y}\)</span> is equivalent to</p>
<div class="math notranslate nohighlight">
\[
\mathbf{y} \in \mathrm{null}(P - \lambda I).
\]</div>
<p>By the <em>Rank-Nullity Theorem</em>, these two null spaces have the same dimension since <span class="math notranslate nohighlight">\((P - \lambda I)^T 
= P^T - \lambda I^T = P^T - \lambda I\)</span>. In particular, when one of them has dimension greater than <span class="math notranslate nohighlight">\(0\)</span> (i.e., it contains non-zero vectors), so does the other.</p>
<p>That is not to say that they are the same space – only their dimension match! In other words, the left and right eigenvalues are the same, but the left and right eigenvectors <em>are not</em>.</p>
<p>(2) What we have shown in the previous theorem is that, if <span class="math notranslate nohighlight">\(P\)</span> is irreducible then it has a unique (up to scaling) left eigenvector of eigenvalue <span class="math notranslate nohighlight">\(1\)</span>. By the first observation, <span class="math notranslate nohighlight">\(\mathbf{1}\)</span> is also the unique right eigenvector of <span class="math notranslate nohighlight">\(P\)</span> with eigenvalue <span class="math notranslate nohighlight">\(1\)</span> in this case. That is, the <a class="reference external" href="https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors#Eigenspaces,_geometric_multiplicity,_and_the_eigenbasis_for_matrices">geometric multiplicity</a> of <span class="math notranslate nohighlight">\(1\)</span> is <span class="math notranslate nohighlight">\(1\)</span>.</p>
<p>(3) What about the other eigenvalues? Suppose that <span class="math notranslate nohighlight">\(\mathbf{z} P = \lambda \mathbf{z}\)</span> for a non-zero row vector <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>, then taking the <span class="math notranslate nohighlight">\(\ell_1\)</span>-norm of the left-hand side we get</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\|\mathbf{z} P\|_1
&amp;= \sum_{j=1}^n \left|\sum_{i=1}^n z_i p_{i,j} \right|
\leq \sum_{j=1}^n \sum_{i=1}^n |z_i| p_{i,j}
\leq \sum_{i=1}^n  |z_i| \sum_{j=1}^n p_{i,j}
\leq \sum_{i=1}^n  |z_i|
= \|\mathbf{z}\|_1,
\end{align*}\]</div>
<p>where we used that <span class="math notranslate nohighlight">\(P\)</span> is stochastic.</p>
<p>The <span class="math notranslate nohighlight">\(\ell_1\)</span>-norm of the right-hand side is <span class="math notranslate nohighlight">\(\|\lambda \mathbf{z} \|_1 = |\lambda| \|\lambda \mathbf{z} \|_1\)</span>. Hence <span class="math notranslate nohighlight">\(\|\lambda \mathbf{z} \|_1 \leq \|\mathbf{z}\|_1\)</span>, which after simplifying implies <span class="math notranslate nohighlight">\(|\lambda| \leq 1\)</span>.</p>
<p>(3) So all left and right eigenvalues of <span class="math notranslate nohighlight">\(P\)</span> are smaller or equal than <span class="math notranslate nohighlight">\(1\)</span> in absolute value. In the irreducible case, we know that <span class="math notranslate nohighlight">\(1\)</span> is achieved and has geometric multiplicity <span class="math notranslate nohighlight">\(1\)</span>. What about <span class="math notranslate nohighlight">\(-1\)</span>? Suppose <span class="math notranslate nohighlight">\(\mathbf{z} P = - \mathbf{z}\)</span>. Then applying <span class="math notranslate nohighlight">\(P\)</span> again to both sides we get</p>
<div class="math notranslate nohighlight">
\[
\mathbf{z} P^2 = - \mathbf{z} P = \mathbf{z}.
\]</div>
<p>So if <span class="math notranslate nohighlight">\(P^2\)</span> (which is stochastic; why?) is irreducible, then there is a unique such <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>.</p>
<p>But we already know of one. Indeed, the unique stationary distribution <span class="math notranslate nohighlight">\(\bpi\)</span> of <span class="math notranslate nohighlight">\(P\)</span> satisfies</p>
<div class="math notranslate nohighlight">
\[
\bpi P^2 = \bpi P = \bpi.
\]</div>
<p>But it does not satisfy <span class="math notranslate nohighlight">\(\bpi P = - \bpi\)</span>. Hence there is no eigenvector with eigenvalue <span class="math notranslate nohighlight">\(-1\)</span> in that case.</p>
<p><strong>NUMERICAL CORNER:</strong> In general, computing stationary distributions is not as straigthforward as in the simple example we considered above. We conclude this subsection with some numerical recipes.</p>
<p>Going back to the <em>Robot Vacuum</em>, finding a solution to <span class="math notranslate nohighlight">\(\bpi P =\bpi\)</span> in this case is not obvious. One way to do this is to note that, taking transposes, this condition is equivalent to <span class="math notranslate nohighlight">\(P^T \bpi^T = \bpi^T\)</span>. That is, <span class="math notranslate nohighlight">\(\bpi^T\)</span> is an eigenvector of <span class="math notranslate nohighlight">\(P^T\)</span> with eigenvalue <span class="math notranslate nohighlight">\(1\)</span>. (Or, as we noted previously, the row vector <span class="math notranslate nohighlight">\(\bpi\)</span> is a left eigenvector of <span class="math notranslate nohighlight">\(P\)</span> with eigenvalue <span class="math notranslate nohighlight">\(1\)</span>.) It must also satisfy <span class="math notranslate nohighlight">\(\bpi \geq 0\)</span> with at least one entry non-zero. Here, we use NumPy.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">LA</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">P_robot</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The first eigenvalue is approximately <span class="math notranslate nohighlight">\(1\)</span>, as seen below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[ 1.        +0.j          0.67955052+0.j          0.50519638+0.j
 -0.70014828+0.j         -0.59989603+0.j         -0.47710224+0.32524037j
 -0.47710224-0.32524037j  0.03475095+0.04000569j  0.03475095-0.04000569j]
</pre></div>
</div>
</div>
</div>
<p>The corresponding eigenvector is approximately non-negative.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">v</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.08933591+0.j 0.27513917+0.j 0.15744007+0.j 0.06794162+0.j
 0.20029774+0.j 0.68274825+0.j 0.24751961+0.j 0.48648149+0.j
 0.28761004+0.j]
</pre></div>
</div>
</div>
</div>
<p>To obtain a stationary distribution, we remove the imaginary part and normalize it to sum to <span class="math notranslate nohighlight">\(1\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pi_robot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">real</span><span class="p">(</span><span class="n">v</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">real</span><span class="p">(</span><span class="n">v</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pi_robot</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.03581295 0.11029771 0.06311453 0.02723642 0.0802953  0.27369992
 0.09922559 0.19502056 0.11529703]
</pre></div>
</div>
</div>
</div>
<p>Alternatively, we can solve the linear system</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^n \pi_i p_{i,j} = \pi_j, \qquad \forall j \in [n].
\]</div>
<p>It turns out that the last equation is a linear combination over the other equations (see <em>Exercise 3.48</em>), so we remove it and replace it instead with the condition <span class="math notranslate nohighlight">\(\sum_{i=1}^n \pi_i = 1\)</span>.</p>
<p>The left-hand side of the resulting linear system is (after taking the transpose to work with column vectors):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_robot</span> <span class="o">=</span> <span class="n">P_robot</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">P_robot</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_robot</span><span class="p">))</span>
<span class="n">A</span><span class="p">[</span><span class="n">n_robot</span><span class="o">-</span><span class="mi">1</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_robot</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[-1.    0.3   0.    0.1   0.    0.    0.    0.    0.  ]
 [ 0.8  -1.    0.6   0.1   0.    0.15  0.    0.    0.  ]
 [ 0.    0.2  -1.    0.    0.    0.15  0.    0.    0.  ]
 [ 0.2   0.    0.   -1.    0.25  0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.8  -1.    0.    0.    0.3   0.  ]
 [ 0.    0.5   0.4   0.    0.   -1.    0.    0.4   1.  ]
 [ 0.    0.    0.    0.    0.75  0.   -1.    0.2   0.  ]
 [ 0.    0.    0.    0.    0.    0.35  1.   -1.    0.  ]
 [ 1.    1.    1.    1.    1.    1.    1.    1.    1.  ]]
</pre></div>
</div>
</div>
</div>
<p>The right-hand side of the resulting linear system is:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_robot</span><span class="o">-</span><span class="mi">1</span><span class="p">),[</span><span class="mf">1.</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0. 0. 0. 0. 0. 0. 0. 0. 1.]
</pre></div>
</div>
</div>
</div>
<p>We solve the linear system using <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html"><code class="docutils literal notranslate"><span class="pre">numpy.linalg.solve()</span></code></a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pi_robot_solve</span> <span class="o">=</span> <span class="n">LA</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pi_robot_solve</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.03581295 0.11029771 0.06311453 0.02723642 0.0802953  0.27369992
 0.09922559 0.19502056 0.11529703]
</pre></div>
</div>
</div>
</div>
<p>This last approach is known as “Replace an Equation”.</p>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>CHAT &amp; LEARN</strong> The Perron-Frobenius theorem is a powerful result about the eigenvalues and eigenvectors of certain types of matrices, including irreducible stochastic matrices. Ask your favorite AI chatbot to explain the Perron-Frobenius theorem and how it relates to the material in this section. <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
<p><em><strong>Self-assessment quiz</strong></em> <em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>1</strong> Which of the following is the correct condition for a probability distribution <span class="math notranslate nohighlight">\(\pi = (\pi_i)_{i=1}^n\)</span> to be a stationary distribution of a Markov chain with transition matrix <span class="math notranslate nohighlight">\(P = (p_{i,j})_{i,j=1}^n\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(\sum_{j=1}^n \pi_i p_{i,j} = \pi_j\)</span> for all <span class="math notranslate nohighlight">\(i \in [n]\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(\sum_{i=1}^n \pi_i p_{i,j} = \pi_j\)</span> for all <span class="math notranslate nohighlight">\(j \in [n]\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(\sum_{j=1}^n \pi_i p_{i,j} = \pi_i\)</span> for all <span class="math notranslate nohighlight">\(i \in [n]\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(\sum_{i=1}^n \pi_i p_{i,j} = \pi_i\)</span> for all <span class="math notranslate nohighlight">\(j \in [n]\)</span></p>
<p><strong>2</strong> Which of the following is the matrix form of the condition for a probability distribution <span class="math notranslate nohighlight">\(\pi\)</span> to be a stationary distribution of a Markov chain with transition matrix <span class="math notranslate nohighlight">\(P\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(\pi P = \pi\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(P \pi = \pi\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(\pi P^T = \pi^T\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(P^T \pi^T = \pi^T\)</span></p>
<p><strong>3</strong> A Markov chain is irreducible if:</p>
<p>a) Every state communicates with every other state.</p>
<p>b) There exists a state that communicates with every other state.</p>
<p>c) The transition graph of the chain is strongly connected.</p>
<p>d) Both a and c.</p>
<p><strong>4</strong> Consider the following transition graph of a Markov chain:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">DiGraph</span><span class="p">()</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edges_from</span><span class="p">([(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)])</span>
</pre></div>
</div>
<p>Is this Markov chain irreducible?</p>
<p>a) Yes</p>
<p>b) No</p>
<p><strong>5</strong> In an irreducible Markov chain, the left and right eigenvectors corresponding to eigenvalue 1 are:</p>
<p>a) The same up to scaling.</p>
<p>b) Always different.</p>
<p>c) Transposes of each other.</p>
<p>d) Not necessarily related to each other.</p>
<p>Answer for 1: b. Justification: The text states that a probability distribution <span class="math notranslate nohighlight">\(\pi = (\pi_i)_{i=1}^n\)</span> over <span class="math notranslate nohighlight">\([n]\)</span> is a stationary distribution of a Markov chain with transition matrix <span class="math notranslate nohighlight">\(P = (p_{i,j})_{i,j=1}^n\)</span> if <span class="math notranslate nohighlight">\(\sum_{i=1}^n \pi_i p_{i,j} = \pi_j\)</span> for all <span class="math notranslate nohighlight">\(j \in [n]\)</span>.</p>
<p>Answer for 2: a. Justification: The text states that the condition for a probability distribution <span class="math notranslate nohighlight">\(\pi\)</span> to be a stationary distribution of a Markov chain with transition matrix <span class="math notranslate nohighlight">\(P\)</span> can be written in matrix form as <span class="math notranslate nohighlight">\(\pi P = \pi\)</span>, where <span class="math notranslate nohighlight">\(\pi\)</span> is thought of as a row vector.</p>
<p>Answer for 3: d. Justification: The text states that a Markov chain on <span class="math notranslate nohighlight">\(S\)</span> is irreducible if for all <span class="math notranslate nohighlight">\(x, y \in S\)</span> with <span class="math notranslate nohighlight">\(x \neq y\)</span>, we have <span class="math notranslate nohighlight">\(x \to y\)</span> and <span class="math notranslate nohighlight">\(y \to x\)</span>. It also mentions that a Markov chain is irreducible if and only if its transition graph is strongly connected.</p>
<p>Answer for 4: b. Justification: The Markov chain is not irreducible because there is no way to get back to state 1 from state 3.</p>
<p>Answer for 5: d. Justification: The text states that for an irreducible Markov chain, the left and right eigenvalues are the same, but the left and right eigenvectors are not necessarily the same. It also mentions that the geometric multiplicity of eigenvalue 1 is 1, implying that the left and right eigenvectors corresponding to eigenvalue 1 are unique up to scaling, but they are not necessarily related to each other.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chap07_rwmc/03_stat"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../02_mcdefs/roch-mmids-rwmc-mcdefs.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">7.2. </span>Background: elements of finite Markov chains</p>
      </div>
    </a>
    <a class="right-next"
       href="../04_mclimit/roch-mmids-rwmc-mclimit.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">7.4. </span>Limit behavior 2: convergence to equilibrium</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#definitions">7.3.1. Definitions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#existence">7.3.2. Existence</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Sebastien Roch
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>