
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>7.8. Online supplementary material &#8212; MMiDS Textbook</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=87e54e7c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-P5E8DW088F"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-P5E8DW088F');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-P5E8DW088F');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chap05_rwmc/06_adv/roch-mmids-rwmc-adv';</script>
    <link rel="canonical" href="https://mmids-textbook.github.io/chap05_rwmc/06_adv/roch-mmids-rwmc-adv.html" />
    <link rel="icon" href="https://mmids-textbook.github.io/favicon-32x32.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="8. Neural networks, backpropagation and stochastic gradient descent" href="../../chap08_nn/00_intro/roch-mmids-nn-intro.html" />
    <link rel="prev" title="7.7. Exercises" href="../exercises/roch-mmids-rwmc-exercises.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/mmids-cover-small.jpg" class="logo__image only-light" alt="MMiDS Textbook - Home"/>
    <script>document.write(`<img src="../../_static/mmids-cover-small.jpg" class="logo__image only-dark" alt="MMiDS Textbook - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../index.html">
                    <b>MATHEMATICAL METHODS in DATA SCIENCE (with Python)</b>
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap01_intro/00_intro/roch-mmids-intro-intro.html">1. Introduction: a first data science problem</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap01_intro/01_motiv/roch-mmids-intro-motiv.html">1.1. Motivating example: identifying penguin species</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap01_intro/02_review/roch-mmids-intro-review.html">1.2. Background: quick refresher of matrix algebra, differential calculus, and elementary probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap01_intro/03_clustering/roch-mmids-intro-clustering.html">1.3. Clustering: an objective, an algorithm and a guarantee</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap01_intro/04_highdim/roch-mmids-intro-highdim.html">1.4. Some observations about high-dimensional data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap01_intro/exercises/roch-mmids-intro-exercises.html">1.5. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap01_intro/05_adv/roch-mmids-intro-adv.html">1.6. Online suppplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap02_ls/00_intro/roch-mmids-ls-intro.html">2. Least squares: geometric, algebraic, and numerical aspects</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/01_motiv/roch-mmids-ls-motiv.html">2.1. Motivating example: predicting sales</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/02_subspaces/roch-mmids-ls-spaces.html">2.2. Background: review of vector spaces and matrix inverses</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/03_orthog/roch-mmids-ls-orthog.html">2.3. Geometry of least squares: the orthogonal projection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/04_qr/roch-mmids-ls-qr.html">2.4. QR decomposition and Householder transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/05_regression/roch-mmids-ls-regression.html">2.5. Application: regression analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/exercises/roch-mmids-ls-exercises.html">2.6. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/06_adv/roch-mmids-ls-adv.html">2.7. Online suppplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap06_opt/00_intro/roch-mmids-opt-intro.html">3. Optimization theory and algorithms</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap06_opt/01_motiv/roch-mmids-opt-motiv.html">3.1. Motivating example:  analyzing customer satisfaction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap06_opt/02_several/roch-mmids-opt-several.html">3.2. Background: review of differentiable functions of several variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap06_opt/03_optimality/roch-mmids-opt-optimality.html">3.3. Optimality conditions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap06_opt/03part2_convexity/roch-mmids-opt-convexity.html">3.4. Convexity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap06_opt/04_gd/roch-mmids-opt-gd.html">3.5. Gradient descent and its convergence analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap06_opt/04part2_logistic/roch-mmids-opt-logistic.html">3.6. Application: logistic regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap06_opt/exercises/roch-mmids-opt-exercises.html">3.7. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap06_opt/06_adv/roch-mmids-opt-adv.html">3.8. Online suppplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap03_svd/00_intro/roch-mmids-svd-intro.html">4. Singular value decomposition</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap03_svd/01_motiv/roch-mmids-svd-motiv.html">4.1. Motivating example: visualizing viral evolution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap03_svd/02_spectral/roch-mmids-svd-spectral.html">4.2. Background: review of matrix rank  and spectral decomposition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap03_svd/03_svd/roch-mmids-svd-svd.html">4.3. Approximating subspaces and the SVD</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap03_svd/04_power/roch-mmids-svd-power.html">4.4. Power iteration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap03_svd/05_pca/roch-mmids-svd-pca.html">4.5. Application: principal components analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap03_svd/06_further/roch-mmids-svd-further.html">4.6. Further applications of the SVD: low-rank approximations and ridge regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap03_svd/exercises/roch-mmids-svd-exercises.html">4.7. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap03_svd/07_adv/roch-mmids-svd-adv.html">4.8. Online suppplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap04_specgraph/00_intro/roch-mmids-specgraph-intro.html">5. Spectral graph theory</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_specgraph/01_motiv/roch-mmids-specgraph-motiv.html">5.1. Motivating example: uncovering social groups</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_specgraph/02_graph/roch-mmids-specgraph-graph.html">5.2. Background: basic concepts in graph theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_specgraph/03_extremal/roch-mmids-specgraph-extremal.html">5.3. Variational characterization of eigenvalues</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_specgraph/04_laplacian/roch-mmids-specgraph-laplacian.html">5.4. Spectral properties of the Laplacian matrix</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_specgraph/05_partitioning/roch-mmids-specgraph-partitioning.html">5.5. Application: graph partitioning via spectral clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_specgraph/05part2_sbm/roch-mmids-specgraph-sbm.html">5.6. Erdős-Rényi random graph and stochastic blockmodel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_specgraph/exercises/roch-mmids-specgraph-exercises.html">5.7. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_specgraph/06_adv/roch-mmids-specgraph-adv.html">5.8. Online supplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap07_prob/00_intro/roch-mmids-prob-intro.html">6. Probabilistic models: from simple to complex</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap07_prob/01_motiv/roch-mmids-prob-motiv.html">6.1. Motivating example: tracking location</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap07_prob/02_parametric/roch-mmids-prob-parametric.html">6.2. Background: introduction to parametric families and maximum likelihood estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap07_prob/03_joint/roch-mmids-prob-joint.html">6.3. Modeling more complex dependencies 1: using conditional independence</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap07_prob/03part2_em/roch-mmids-prob-em.html">6.4. Modeling more complex dependencies 2: marginalizing out an unobserved variable</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap07_prob/04_kalman/roch-mmids-prob-kalman.html">6.5. Application: linear-Gaussian models and Kalman filtering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap07_prob/exercises/roch-mmids-prob-exercises.html">6.6. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap07_prob/06_adv/roch-mmids-prob-adv.html">6.7. Online supplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../00_intro/roch-mmids-rwmc-intro.html">7. Random walks on graphs and Markov chains</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../01_motiv/roch-mmids-rwmc-motiv.html">7.1. Motivating example: discovering mathematical topics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03_mcdefs/roch-mmids-rwmc-mcdefs.html">7.2. Background: elements of finite Markov chains</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04_stat/roch-mmids-rwmc-stat.html">7.3. Limit behavior 1: stationary distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04part2_mclimit/roch-mmids-rwmc-mclimit.html">7.4. Limit behavior 2: convergence to equilibrium</a></li>
<li class="toctree-l2"><a class="reference internal" href="../05_pagerank/roch-mmids-rwmc-pagerank.html">7.5. Application: random walks on graphs and PageRank</a></li>
<li class="toctree-l2"><a class="reference internal" href="../05part2_gibbs/roch-mmids-rwmc-gibbs.html">7.6. Further applications: Gibbs sampling and generating images</a></li>
<li class="toctree-l2"><a class="reference internal" href="../exercises/roch-mmids-rwmc-exercises.html">7.7. Exercises</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">7.8. Online supplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap08_nn/00_intro/roch-mmids-nn-intro.html">8. Neural networks, backpropagation and stochastic gradient descent</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/01_motiv/roch-mmids-nn-motiv.html">8.1. Motivating example:  classifying natural images</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/02_chain/roch-mmids-nn-chain.html">8.2. Background: Jacobian, chain rule, and a brief introduction to automatic differentiation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/03_backprop/roch-mmids-nn-backprop.html">8.3. Building blocks of AI 1: backpropagation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/04_sgd/roch-mmids-nn-sgd.html">8.4. Building blocks of AI 2: stochastic gradient descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/05_nn/roch-mmids-nn-nn.html">8.5. Building blocks of AI 3: neural networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/exercises/roch-mmids-nn-exercises.html">8.6. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/06_adv/roch-mmids-nn-adv.html">8.7. Online supplementary material</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/issues/new?title=Issue%20on%20page%20%2Fchap05_rwmc/06_adv/roch-mmids-rwmc-adv.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/chap05_rwmc/06_adv/roch-mmids-rwmc-adv.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Online supplementary material</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#self-assessment-quizzes">7.8.1. Self-assessment quizzes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#just-the-code">7.8.2. Just the code</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#auto-quizzes">7.8.3. Auto-quizzes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#solutions-to-odd-numbered-warm-up-exercises">7.8.4. Solutions to odd-numbered warm-up exercises</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#proof-of-the-convergence-theorem">7.8.5. Proof of the convergence theorem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#random-walk-on-a-weighted-graph">7.8.6. Random walk on a weighted graph</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#spectral-techniques-for-random-walks-on-graphs">7.8.7. Spectral techniques for random walks on graphs</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <p><span class="math notranslate nohighlight">\(\newcommand{\P}{\mathbb{P}}\)</span> <span class="math notranslate nohighlight">\(\newcommand{\E}{\mathbb{E}}\)</span> <span class="math notranslate nohighlight">\(\newcommand{\S}{\mathcal{S}}\)</span> <span class="math notranslate nohighlight">\(\newcommand{\indep}{\perp\!\!\!\perp}\)</span> <span class="math notranslate nohighlight">\(\newcommand{\bmu}{\boldsymbol{\mu}}\)</span> <span class="math notranslate nohighlight">\(\newcommand{\bpi}{\boldsymbol{\pi}}\)</span></p>
<section id="online-supplementary-material">
<h1><span class="section-number">7.8. </span>Online supplementary material<a class="headerlink" href="#online-supplementary-material" title="Link to this heading">#</a></h1>
<p><em>Learning outcomes</em></p>
<ul class="simple">
<li><p>Define a discrete-time Markov chain and its state space, initial distribution, and transition probabilities.</p></li>
<li><p>Identify Markov chains in real-world scenarios, such as weather patterns or random walks on graphs.</p></li>
<li><p>Construct the transition matrix for a time-homogeneous Markov chain.</p></li>
<li><p>Construct the transition graph of a Markov chain from its transition matrix.</p></li>
<li><p>Prove that the transition matrix of a Markov chain is a stochastic matrix.</p></li>
<li><p>Apply the Markov property to calculate the probability of specific sequences of events in a Markov chain using the distribution of a sample path.</p></li>
<li><p>Compute the marginal distribution of a Markov chain at a specific time using matrix powers.</p></li>
<li><p>Use simulation to generate sample paths of a Markov chain.</p></li>
<li><p>Define a stationary distribution of a finite-state, discrete-time, time-homogeneous Markov chain and express the defining condition in matrix form.</p></li>
<li><p>Explain the relationship between stationary distributions and left eigenvectors of the transition matrix.</p></li>
<li><p>Determine whether a given probability distribution is a stationary distribution for a given Markov chain.</p></li>
<li><p>Identify irreducible Markov chains and explain their significance in the context of stationary distributions.</p></li>
<li><p>State the main theorem on the existence and uniqueness of a stationary distribution for an irreducible Markov chain.</p></li>
<li><p>Compute the stationary distribution of a Markov chain numerically using either eigenvalue methods or by solving a linear system of equations, including the “Replace an Equation” method.</p></li>
<li><p>Define the concepts of aperiodicity and weak laziness for finite-space discrete-time Markov chains.</p></li>
<li><p>State the Convergence to Equilibrium Theorem and the Ergodic Theorem for irreducible Markov chains.</p></li>
<li><p>Verify the Ergodic Theorem by simulating a Markov chain and comparing the frequency of visits to each state with the stationary distribution.</p></li>
<li><p>Explain the concept of coupling and its role in proving the Convergence to Equilibrium Theorem for irreducible, weakly lazy Markov chains.</p></li>
<li><p>Define random walks on directed and undirected graphs, and express their transition matrices in terms of the adjacency matrix.</p></li>
<li><p>Determine the stationary distribution of a random walk on an undirected graph and explain its relation to degree centrality.</p></li>
<li><p>Explain the concept of reversibility and its connection to the stationary distribution of a random walk.</p></li>
<li><p>Describe the PageRank algorithm and its interpretation as a modified random walk on a directed graph.</p></li>
<li><p>Compute the PageRank vector by finding the stationary distribution of the modified random walk using power iteration.</p></li>
<li><p>Apply the PageRank algorithm to real-world datasets to identify central nodes in a network.</p></li>
<li><p>Explain the concept of Personalized PageRank and how it differs from the standard PageRank algorithm.</p></li>
<li><p>Modify the PageRank algorithm to implement Personalized PageRank and interpret the results based on the chosen teleportation distribution.</p></li>
<li><p>Describe the concept of Markov Chain Monte Carlo (MCMC) and its application in sampling from complex probability distributions.</p></li>
<li><p>Explain the Metropolis-Hastings algorithm, including the proposal distribution and acceptance-rejection steps.</p></li>
<li><p>Calculate acceptance probabilities in the Metropolis-Hastings algorithm for a given target and proposal distribution.</p></li>
<li><p>Prove the correctness of the Metropolis-Hastings algorithm by showing that the resulting Markov chain is irreducible and reversible with respect to the target distribution.</p></li>
<li><p>Implement the Gibbs sampling algorithm for a given probabilistic model, such as a Restricted Boltzmann Machine (RBM).</p></li>
<li><p>Analyze the conditional independence properties of RBMs and their role in Gibbs sampling.</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\aleph\)</span></p>
<section id="self-assessment-quizzes">
<h2><span class="section-number">7.8.1. </span>Self-assessment quizzes<a class="headerlink" href="#self-assessment-quizzes" title="Link to this heading">#</a></h2>
<p>A more extensive web version of the self-assessment quizzes is available by following the links below.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/auto_quizzes/self-assessment/quiz_7_2.html">Section 7.2</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/auto_quizzes/self-assessment/quiz_7_3.html">Section 7.3</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/auto_quizzes/self-assessment/quiz_7_4.html">Section 7.4</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/auto_quizzes/self-assessment/quiz_7_5.html">Section 7.5</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/auto_quizzes/self-assessment/quiz_7_6.html">Section 7.6</a></p></li>
</ul>
</section>
<section id="just-the-code">
<h2><span class="section-number">7.8.2. </span>Just the code<a class="headerlink" href="#just-the-code" title="Link to this heading">#</a></h2>
<p>An interactive Jupyter notebook featuring the code in this chapter can be accessed below (Google Colab recommended). You are encouraged to tinker with it. Some suggested computational exercises are scattered throughout. The notebook is also available as a slideshow.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_rwmc_notebook.ipynb">Notebook</a> (<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_rwmc_notebook.ipynb">Open In Colab</a>)</p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_rwmc_notebook_slides.slides.html">Slideshow</a></p></li>
</ul>
</section>
<section id="auto-quizzes">
<h2><span class="section-number">7.8.3. </span>Auto-quizzes<a class="headerlink" href="#auto-quizzes" title="Link to this heading">#</a></h2>
<p>Automatically generated quizzes for this chapter can be accessed here (Google Colab recommended).</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/auto_quizzes/roch-mmids-rwmc-autoquiz.ipynb">Auto-quizzes</a>
(<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/auto_quizzes/roch-mmids-rwmc-autoquiz.ipynb">Open In Colab</a>)</p></li>
</ul>
</section>
<section id="solutions-to-odd-numbered-warm-up-exercises">
<h2><span class="section-number">7.8.4. </span>Solutions to odd-numbered warm-up exercises<a class="headerlink" href="#solutions-to-odd-numbered-warm-up-exercises" title="Link to this heading">#</a></h2>
<p><em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>E7.2.1</strong> We need to check that all entries of <span class="math notranslate nohighlight">\(P\)</span> are non-negative and that all rows sum to one.</p>
<p>Non-negativity: All entries of <span class="math notranslate nohighlight">\(P\)</span> are clearly non-negative.</p>
<p>Row sums:
Row 1: <span class="math notranslate nohighlight">\(0.2 + 0.5 + 0.3 = 1\)</span>
Row 2: <span class="math notranslate nohighlight">\(0.4 + 0.1 + 0.5 = 1\)</span>
Row 3: <span class="math notranslate nohighlight">\(0.6 + 0.3 + 0.1 = 1\)</span></p>
<p>Therefore, <span class="math notranslate nohighlight">\(P\)</span> is a stochastic matrix.</p>
<p><strong>E7.2.3</strong></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
P[X_2 = 2] &amp;= (\mu P^2)_2 \\
&amp;= (0.2, 0.3, 0.5) \begin{pmatrix}
0.44 &amp; 0.31 &amp; 0.25 \\
0.44 &amp; 0.37 &amp; 0.19 \\
0.36 &amp; 0.33 &amp; 0.31
\end{pmatrix}_2 \\
&amp;= 0.2 \cdot 0.31 + 0.3 \cdot 0.37 + 0.5 \cdot 0.33 \\
&amp;= 0.338.
\end{align*}\]</div>
<p><strong>E7.2.5</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">DiGraph</span><span class="p">()</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_nodes_from</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edges_from</span><span class="p">([(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)])</span>
</pre></div>
</div>
<p>The transition graph has a directed edge from state <span class="math notranslate nohighlight">\(i\)</span> to state <span class="math notranslate nohighlight">\(j\)</span> if and only if <span class="math notranslate nohighlight">\(p_{i,j} &gt; 0\)</span>, as stated in the definition.</p>
<p><strong>E7.2.7</strong></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
P[X_2 = 2 | X_0 = 3] &amp;= (P^2)_{3,2} \\
&amp;= \begin{pmatrix}
0.1 &amp; 0.4 &amp; 0.5 \\
0.2 &amp; 0.6 &amp; 0.2 \\
0.3 &amp; 0.3 &amp; 0.4
\end{pmatrix}^2_{3,2} \\
&amp;= \begin{pmatrix}
0.29 &amp; 0.38 &amp; 0.33 \\
0.26 &amp; 0.44 &amp; 0.30 \\
0.27 &amp; 0.36 &amp; 0.37
\end{pmatrix}_{3,2} \\
&amp;= 0.36.
\end{align*}\]</div>
<p><strong>E7.2.9</strong> The probability is given by the entry <span class="math notranslate nohighlight">\((P^2)_{0,1}\)</span>, where <span class="math notranslate nohighlight">\(P^2\)</span> is the matrix product <span class="math notranslate nohighlight">\(P \times P\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
P^2 = \begin{pmatrix} 
    1/3 &amp; 2/3 \\
    1/2 &amp; 1/2
    \end{pmatrix}
    \begin{pmatrix} 
    1/3 &amp; 2/3 \\
    1/2 &amp; 1/2
    \end{pmatrix}
    = \begin{pmatrix}
    7/18 &amp; 11/18 \\
    5/12 &amp; 7/12
    \end{pmatrix}
\end{split}\]</div>
<p>Therefore, the probability is <span class="math notranslate nohighlight">\(\boxed{11/18}\)</span>.</p>
<p><strong>E7.2.11</strong> The marginal distribution at time 2 is given by <span class="math notranslate nohighlight">\(\mu P^2\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
P^2 = \begin{pmatrix}
1/2 &amp; 0 &amp; 1/2 \\
0 &amp; 1 &amp; 0 \\
1/3 &amp; 1/3 &amp; 1/3
\end{pmatrix}
\begin{pmatrix}
1/2 &amp; 0 &amp; 1/2 \\
0 &amp; 1 &amp; 0 \\
1/3 &amp; 1/3 &amp; 1/3
\end{pmatrix}
= \begin{pmatrix}
5/12 &amp; 1/6 &amp; 5/12 \\
0 &amp; 1 &amp; 0 \\
4/9 &amp; 4/9 &amp; 1/9
\end{pmatrix}
\end{split}\]</div>
<p>Thus,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mu P^2 = (1/4, 1/2, 1/4) \begin{pmatrix}
5/12 &amp; 1/6 &amp; 5/12 \\
0 &amp; 1 &amp; 0 \\
4/9 &amp; 4/9 &amp; 1/9
\end{pmatrix} = \boxed{(13/36, 19/36, 4/36)}
\end{split}\]</div>
<p><strong>E7.2.13</strong> The distribution at time 1 is given by <span class="math notranslate nohighlight">\(\mu P\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mu P = (1/3, 2/3) \begin{pmatrix}1/2 &amp; 1/2 \\ 1 &amp; 0\end{pmatrix} = \boxed{(2/3, 1/3)}
\end{split}\]</div>
<p><strong>E7.2.15</strong> The chain alternates deterministically between states 1 and 2. Therefore, starting in state 1, it will return to state 1 after exactly <span class="math notranslate nohighlight">\(\boxed{2}\)</span> steps.</p>
<p><strong>E7.3.1</strong> Yes, the matrix is irreducible. The corresponding transition graph is a cycle, which is strongly connected.</p>
<p><strong>E7.3.3</strong> We need to check if <span class="math notranslate nohighlight">\(\pi P = \pi\)</span>. Indeed,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
(0.6, 0.4) \begin{pmatrix} 0.4 &amp; 0.6 \\ 0.7 &amp; 0.3 \end{pmatrix} = (0.6 \cdot 0.4 + 0.4 \cdot 0.7, 0.6 \cdot 0.6 + 0.4 \cdot 0.3) = (0.52, 0.48) \neq (0.6, 0.4),
\end{split}\]</div>
<p>so <span class="math notranslate nohighlight">\(\pi\)</span> is not a stationary distribution of the Markov chain.</p>
<p><strong>E7.3.5</strong> Let <span class="math notranslate nohighlight">\(\pi = (\pi_1, \pi_2)\)</span> be a stationary distribution. Then, we need to solve the system of equations:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\pi_1 \cdot 0.5 + \pi_2 \cdot 0.5 &amp;= \pi_1 \\
\pi_1 + \pi_2 &amp;= 1
\end{align*}\]</div>
<p>The first equation simplifies to <span class="math notranslate nohighlight">\(\pi_1 = \pi_2\)</span>, and substituting this into the second equation yields <span class="math notranslate nohighlight">\(2\pi_1 = 1\)</span>, or <span class="math notranslate nohighlight">\(\pi_1 = \pi_2 = 0.5\)</span>. Therefore, <span class="math notranslate nohighlight">\(\pi = (0.5, 0.5)\)</span> is a stationary distribution.</p>
<p><strong>E7.3.7</strong> To verify that <span class="math notranslate nohighlight">\(\pi\)</span> is a stationary distribution, we need to check if <span class="math notranslate nohighlight">\(\pi P = \pi\)</span>. Let’s perform the matrix multiplication step by step:</p>
<p><span class="math notranslate nohighlight">\(\pi P = (\frac{1}{3}, \frac{1}{3}, \frac{1}{3}) \begin{pmatrix} 0.4 &amp; 0.3 &amp; 0.3 \\ 0.2 &amp; 0.5 &amp; 0.3 \\ 0.4 &amp; 0.2 &amp; 0.4 \end{pmatrix}\)</span></p>
<p><span class="math notranslate nohighlight">\(= (\frac{1}{3} \cdot 0.4 + \frac{1}{3} \cdot 0.2 + \frac{1}{3} \cdot 0.4, \frac{1}{3} \cdot 0.3 + \frac{1}{3} \cdot 0.5 + \frac{1}{3} \cdot 0.2, \frac{1}{3} \cdot 0.3 + \frac{1}{3} \cdot 0.3 + \frac{1}{3} \cdot 0.4)\)</span></p>
<p><span class="math notranslate nohighlight">\(= (\frac{0.4 + 0.2 + 0.4}{3}, \frac{0.3 + 0.5 + 0.2}{3}, \frac{0.3 + 0.3 + 0.4}{3})\)</span></p>
<p><span class="math notranslate nohighlight">\(= (\frac{1}{3}, \frac{1}{3}, \frac{1}{3})\)</span></p>
<p><span class="math notranslate nohighlight">\(= \pi\)</span></p>
<p>The result is equal to <span class="math notranslate nohighlight">\(\pi = (\frac{1}{3}, \frac{1}{3}, \frac{1}{3})\)</span>. Therefore, the uniform distribution <span class="math notranslate nohighlight">\(\pi\)</span> is indeed a stationary distribution for the matrix <span class="math notranslate nohighlight">\(P\)</span>. Note that the transition matrix <span class="math notranslate nohighlight">\(P\)</span> is doubly stochastic because each row and each column sums to 1. This property ensures that the uniform distribution is always a stationary distribution for doubly stochastic matrices, as mentioned in the text.</p>
<p><strong>E7.3.9</strong> Let <span class="math notranslate nohighlight">\(\mathbf{1} = (1, 1, \ldots, 1)^T\)</span> be the column vector of all ones. For any stochastic matrix <span class="math notranslate nohighlight">\(P\)</span>, we have:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
P \mathbf{1} = \begin{pmatrix} 
p_{1,1} &amp; \cdots &amp; p_{1,n} \\
\vdots &amp; \ddots &amp; \vdots \\
p_{n,1} &amp; \cdots &amp; p_{n,n}
\end{pmatrix} \begin{pmatrix} 1 \\ \vdots \\ 1 \end{pmatrix} = \begin{pmatrix} \sum_{j=1}^n p_{1,j} \\ \vdots \\ \sum_{j=1}^n p_{n,j} \end{pmatrix} = \begin{pmatrix} 1 \\ \vdots \\ 1 \end{pmatrix} = \mathbf{1},
\end{split}\]</div>
<p>because each row of a stochastic matrix sums to 1. Therefore, <span class="math notranslate nohighlight">\(\mathbf{1}\)</span> is a right eigenvector of <span class="math notranslate nohighlight">\(P\)</span> with eigenvalue 1.</p>
<p><strong>E7.3.11</strong> Let <span class="math notranslate nohighlight">\(\pi = (\pi_1, \pi_2, \pi_3)\)</span> be the stationary distribution. We need to solve the system of equations:</p>
<p>Step 1: Write out the system of equations based on <span class="math notranslate nohighlight">\(\pi P = \pi\)</span>.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\pi_1 \cdot 0.7 + \pi_2 \cdot 0.4 + \pi_3 \cdot 0.6 &amp;= \pi_1 \\
\pi_1 \cdot 0.2 + \pi_2 \cdot 0.4 + \pi_3 \cdot 0.1 &amp;= \pi_2 \\
\pi_1 \cdot 0.1 + \pi_2 \cdot 0.2 + \pi_3 \cdot 0.3 &amp;= \pi_3
\end{align*}\]</div>
<p>Step 2: Rearrange the equations to have zero on the right-hand side.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\pi_1 \cdot 0.7 + \pi_2 \cdot 0.4 + \pi_3 \cdot 0.6 - \pi_1 &amp;= 0 \\
\pi_1 \cdot 0.2 + \pi_2 \cdot 0.4 + \pi_3 \cdot 0.1 - \pi_2 &amp;= 0 \\
\pi_1 \cdot 0.1 + \pi_2 \cdot 0.2 + \pi_3 \cdot 0.3 - \pi_3 &amp;= 0
\end{align*}\]</div>
<p>Step 3: Simplify the equations.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
-0.3\pi_1 + 0.4\pi_2 + 0.6\pi_3 &amp;= 0 \\
0.2\pi_1 - 0.6\pi_2 + 0.1\pi_3 &amp;= 0 \\
0.1\pi_1 + 0.2\pi_2 - 0.7\pi_3 &amp;= 0
\end{align*}\]</div>
<p>Step 4: Use the condition <span class="math notranslate nohighlight">\(\pi_1 + \pi_2 + \pi_3 = 1\)</span> to eliminate one variable, say <span class="math notranslate nohighlight">\(\pi_3\)</span>.</p>
<div class="math notranslate nohighlight">
\[
\pi_3 = 1 - \pi_1 - \pi_2
\]</div>
<p>Step 5: Substitute the expression for <span class="math notranslate nohighlight">\(\pi_3\)</span> into the simplified equations from Step 3.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
-0.3\pi_1 + 0.4\pi_2 + 0.6(1 - \pi_1 - \pi_2) &amp;= 0 \\
0.2\pi_1 - 0.6\pi_2 + 0.1(1 - \pi_1 - \pi_2) &amp;= 0
\end{align*}\]</div>
<p>Step 6: Simplify the equations further.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
-0.9\pi_1 - 0.2\pi_2 &amp;= -0.6 \\
0.1\pi_1 - 0.7\pi_2 &amp;= -0.1
\end{align*}\]</div>
<p>Step 7: Solve the linear system of two equations with two unknowns using any suitable method (e.g., substitution, elimination, or matrix inversion).</p>
<p>Using the substitution method: From the second equation, express <span class="math notranslate nohighlight">\(\pi_1\)</span> in terms of <span class="math notranslate nohighlight">\(\pi_2\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\pi_1 = 7\pi_2 - 1
\]</div>
<p>Substitute this into the first equation:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
-0.9(7\pi_2 - 1) - 0.2\pi_2 &amp;= -0.6 \\
-6.3\pi_2 + 0.9 - 0.2\pi_2 &amp;= -0.6 \\
-6.5\pi_2 &amp;= -1.5 \\
\pi_2 &amp;= \frac{3}{13}
\end{align*}\]</div>
<p>Now, substitute <span class="math notranslate nohighlight">\(\pi_2 = \frac{3}{13}\)</span> back into the expression for <span class="math notranslate nohighlight">\(\pi_1\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\pi_1 &amp;= 7 \cdot \frac{3}{13} - 1 \\
&amp;= \frac{21}{13} - \frac{13}{13} \\
&amp;= \frac{8}{13}
\end{align*}\]</div>
<p>Finally, use <span class="math notranslate nohighlight">\(\pi_1 + \pi_2 + \pi_3 = 1\)</span> to find <span class="math notranslate nohighlight">\(\pi_3\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\pi_3 &amp;= 1 - \pi_1 - \pi_2 \\
&amp;= 1 - \frac{8}{13} - \frac{3}{13} \\
&amp;= \frac{2}{13}
\end{align*}\]</div>
<p>Therefore, the stationary distribution is <span class="math notranslate nohighlight">\((\frac{8}{13}, \frac{3}{13}, \frac{2}{13})\)</span>.</p>
<p><strong>E7.4.1</strong> Given the transition matrix of a Markov chain:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
P = \begin{pmatrix}
0.2 &amp; 0.8 \\
0.6 &amp; 0.4
\end{pmatrix},
\end{split}\]</div>
<p>determine if the chain is lazy.</p>
<p><strong>E7.4.3</strong> Given a Markov chain with transition matrix:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
P = \begin{pmatrix} 0.4 &amp; 0.6 \\ 0.7 &amp; 0.3 \end{pmatrix},
\end{split}\]</div>
<p>and initial distribution <span class="math notranslate nohighlight">\(\mu = (0.2, 0.8)\)</span>, compute <span class="math notranslate nohighlight">\(\lim_{t \to \infty} \mu P^t\)</span>.</p>
<p><strong>E7.5.1</strong> The degree matrix <span class="math notranslate nohighlight">\(D\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
D = \begin{pmatrix}
2 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 3 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 3 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 2 \\
\end{pmatrix}.
\end{split}\]</div>
<p>The degrees are computed by summing the entries in each row of <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p><strong>E7.5.3</strong> A stochastic matrix has rows that sum to 1. The rows of <span class="math notranslate nohighlight">\(P\)</span> sum to 1:</p>
<div class="math notranslate nohighlight">
\[ 
\frac{1}{2} + \frac{1}{2} = 1, \quad \frac{1}{3} + \frac{1}{3} + \frac{1}{3} = 1, \quad \frac{1}{3} + \frac{1}{3} + \frac{1}{3} = 1, \quad \frac{1}{2} + \frac{1}{2} = 1. 
\]</div>
<p>Thus, <span class="math notranslate nohighlight">\(P\)</span> is stochastic.</p>
<p><strong>E7.5.5</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}
P = D^{-1}A = \begin{pmatrix}
1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1/2 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1
\end{pmatrix}
\begin{pmatrix}
0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; 1 &amp; 0
\end{pmatrix} = \begin{pmatrix}
0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 \\
1/2 &amp; 0 &amp; 0 &amp; 1/2 \\
0 &amp; 0 &amp; 1 &amp; 0
\end{pmatrix}.
\end{split}\]</div>
<p><strong>E7.5.7</strong> First, compute the transition matrix:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
P = D^{-1}A = \begin{pmatrix}
1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1/2 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1/2 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1
\end{pmatrix}
\begin{pmatrix}
0 &amp; 1 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; 1 &amp; 0
\end{pmatrix} = \begin{pmatrix}
0 &amp; 1 &amp; 0 &amp; 0 \\
1/2 &amp; 0 &amp; 1/2 &amp; 0 \\
0 &amp; 1/2 &amp; 0 &amp; 1/2 \\
0 &amp; 0 &amp; 1 &amp; 0
\end{pmatrix}.
\end{split}\]</div>
<p>Then, compute the modified transition matrix with the damping factor:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
Q = \alpha P + (1-\alpha)\frac{1}{4}\mathbf{1}\mathbf{1}^T = 0.8P + 0.2\begin{pmatrix}
1/4 &amp; 1/4 &amp; 1/4 &amp; 1/4 \\
1/4 &amp; 1/4 &amp; 1/4 &amp; 1/4 \\
1/4 &amp; 1/4 &amp; 1/4 &amp; 1/4 \\
1/4 &amp; 1/4 &amp; 1/4 &amp; 1/4
\end{pmatrix}.
\end{split}\]</div>
<p><strong>E7.5.9</strong> First, compute the transition matrix:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
P = \begin{pmatrix}
0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\
1/2 &amp; 0 &amp; 0 &amp; 1/2 &amp; 0 \\
1/2 &amp; 0 &amp; 0 &amp; 0 &amp; 1/2 \\
0 &amp; 0 &amp; 0 &amp; 1 &amp; 0
\end{pmatrix}.
\end{split}\]</div>
<p>Then, compute the modified transition matrix with the damping factor:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
Q = 0.9P + 0.1\begin{pmatrix}
1/5 &amp; 1/5 &amp; 1/5 &amp; 1/5 &amp; 1/5 \\
1/5 &amp; 1/5 &amp; 1/5 &amp; 1/5 &amp; 1/5 \\
1/5 &amp; 1/5 &amp; 1/5 &amp; 1/5 &amp; 1/5 \\
1/5 &amp; 1/5 &amp; 1/5 &amp; 1/5 &amp; 1/5 \\
1/5 &amp; 1/5 &amp; 1/5 &amp; 1/5 &amp; 1/5
\end{pmatrix}.
\end{split}\]</div>
<p><strong>E7.5.11</strong> The new adjacency matrix <span class="math notranslate nohighlight">\(A'\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split} 
A' = \begin{pmatrix}
0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; 0 &amp; 1 \\
\end{pmatrix}. 
\end{split}\]</div>
<p>A self-loop is added to vertex 4.</p>
<p><strong>E7.5.13</strong> The modified transition matrix <span class="math notranslate nohighlight">\(Q\)</span> is</p>
<div class="math notranslate nohighlight">
\[ 
Q = \alpha P + (1 - \alpha) \frac{1}{n} 11^T = 0.85 P + 0.15 \frac{1}{4} 11^T. 
\]</div>
<p>Using the values from <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(11^T\)</span>, we get:</p>
<div class="math notranslate nohighlight">
\[\begin{split} 
Q = \begin{pmatrix}
0.0375 &amp; 0.8875 &amp; 0.0375 &amp; 0.0375 \\
0.0375 &amp; 0.0375 &amp; 0.8875 &amp; 0.0375 \\
0.4625 &amp; 0.0375 &amp; 0.0375 &amp; 0.4625 \\
0.0375 &amp; 0.0375 &amp; 0.0375 &amp; 0.8875 \\
\end{pmatrix}. 
\end{split}\]</div>
<p><strong>E7.6.1</strong> The acceptance probability is given by</p>
<div class="math notranslate nohighlight">
\[
\min\left\{1, \frac{\pi_1}{\pi_2}\frac{Q(1, 2)}{Q(2, 1)}\right\} = \min\left\{1, \frac{0.1}{0.2}\frac{0.5}{0.5}\right\} = \frac{1}{2}.
\]</div>
<p><strong>E7.6.3</strong> Since the proposal chain is symmetric, the acceptance probability simplifies to
$<span class="math notranslate nohighlight">\(
\min\left\{1, \frac{\pi_2}{\pi_1}\right\} = \min\left\{1, \frac{0.2}{0.1}\right\} = 1.
\)</span>$</p>
<p><strong>E7.6.5</strong> The acceptance probability is given by</p>
<div class="math notranslate nohighlight">
\[
\min \left\{ 1, \frac{\pi(y)}{\pi(x)} \frac{Q(y, x)}{Q(x, y)} \right\} = \min \left\{ 1, \frac{e^{-9/2}}{e^{-2}} \right\} = e^{-5/2}.
\]</div>
<p><strong>E7.6.7</strong> The conditional probability is given by</p>
<div class="math notranslate nohighlight">
\[
\pi_1^v(1|v_{-1}, h) = \sigma\left(\sum_{j=1}^2 w_{1,j}h_j + b_1\right) = \sigma(1\cdot 1 + (-1)\cdot 0 + 1) = \sigma(2) \approx 0.881.
\]</div>
<p><strong>E7.6.9</strong> The energy is given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
E(v, h) = -v^T W h = -\begin{pmatrix} 1 &amp; 0 \end{pmatrix} \begin{pmatrix} 1 &amp; -2 \\ 3 &amp; 0 \end{pmatrix} \begin{pmatrix} 1 \\ 1 \end{pmatrix} = 1.
\end{split}\]</div>
<p><strong>E7.6.11</strong> The conditional mean for the hidden units is:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[h_j | \mathbf{v}] = \sigma \left( \sum_{i} W_{ij} v_i + c_j \right).
\]</div>
<p>For <span class="math notranslate nohighlight">\(h_1\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[h_1 | \mathbf{v}] = \sigma (0.5 \cdot 1 + 0.3 \cdot 0 - 0.6 \cdot 1 + 0.1) = \sigma (0.5 - 0.6 + 0.1) = \sigma (0) = 0.5.
\]</div>
<p>For <span class="math notranslate nohighlight">\(h_2\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[h_2 | \mathbf{v}] = \sigma (-0.2 \cdot 1 + 0.8 \cdot 0 + 0.1 \cdot 1 - 0.3) = \sigma (-0.2 + 0.1 - 0.3) = \sigma (-0.4) = \frac{1}{1 + e^{0.4}} \approx 0.4013.
\]</div>
</section>
<section id="proof-of-the-convergence-theorem">
<h2><span class="section-number">7.8.5. </span>Proof of the convergence theorem<a class="headerlink" href="#proof-of-the-convergence-theorem" title="Link to this heading">#</a></h2>
<p>We prove the convergence to equilibrium theorem in the irreducible, lazy case. Throughout this section, <span class="math notranslate nohighlight">\((X_t)_{t \geq 0}\)</span> is an irreducible, lazy Markov chain on state space <span class="math notranslate nohighlight">\(\mathcal{S} = [n]\)</span> with transition matrix <span class="math notranslate nohighlight">\(P = (p_{i,j})_{i,j=1}^n\)</span>, initial distribution <span class="math notranslate nohighlight">\(\bmu = (\mu_1,\ldots,\mu_n)\)</span> and unique stationary distribution <span class="math notranslate nohighlight">\(\bpi = (\pi_1,\ldots,\pi_n)\)</span>.</p>
<p>We give a probabilistic proof of the <em>Convergence to Equilibrium Theorem</em>. The proof uses a clever idea: coupling. Separately from <span class="math notranslate nohighlight">\((X_t)_{t \geq 0}\)</span>, we consider an independent Markov chain <span class="math notranslate nohighlight">\((Y_t)_{t \geq 0}\)</span> with the same transition matrix but initial distribution <span class="math notranslate nohighlight">\(\bpi\)</span>. By the definition of stationarity,</p>
<div class="math notranslate nohighlight">
\[
\P[Y_t = i] = \pi_i,
\]</div>
<p>for all <span class="math notranslate nohighlight">\(i\)</span> and all <span class="math notranslate nohighlight">\(t\)</span>. Hence it suffices to show that</p>
<div class="math notranslate nohighlight">
\[
|\P[X_t = i] - \pi_i|
= |\P[X_t = i] - \P[Y_t = i]|
\to 0
\]</div>
<p>as <span class="math notranslate nohighlight">\(t \to +\infty\)</span> for all <span class="math notranslate nohighlight">\(i\)</span>.</p>
<p><em>Step 1: Showing the joint process is Markov.</em> We observe first that the joint process <span class="math notranslate nohighlight">\((X_0,Y_0),(X_1,Y_1),\ldots\)</span> is itself a Markov chain! Let’s just check the definition. By the independence of <span class="math notranslate nohighlight">\((X_t)\)</span> and <span class="math notranslate nohighlight">\((Y_t)\)</span>,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\P[(X_t,Y_t) = (x_t,y_t)\,|\,(X_{t-1},Y_{t-1}) = (x_{t-1},y_{t-1}),\ldots,(X_0,Y_0) = (x_0,y_0)]\\
&amp;=\frac{\P[(X_t,Y_t) = (x_t,y_t),(X_{t-1},Y_{t-1}) = (x_{t-1},y_{t-1}),\ldots,(X_0,Y_0) = (x_0,y_0)]}
{\P[(X_{t-1},Y_{t-1}) = (x_{t-1},y_{t-1}),\ldots,(X_0,Y_0) = (x_0,y_0)]}\\
&amp;=\frac{\P[X_t = x_t,X_{t-1} = x_{t-1},\ldots,X_0 = x_0]\,\P[Y_t = y_t,Y_{t-1} = y_{t-1},\ldots,Y_0 = y_0]}
{\P[X_{t-1} = x_{t-1},\ldots,X_0 = x_0]\,\P[Y_{t-1} = y_{t-1},\ldots,Y_0 = y_0]}\\
&amp;=\frac{\P[X_t = x_t,X_{t-1} = x_{t-1},\ldots,X_0 = x_0]}
{\P[X_{t-1} = x_{t-1},\ldots,X_0 = x_0]}
\frac{\P[Y_t = y_t,Y_{t-1} = y_{t-1},\ldots,Y_0 = y_0]}
{\P[Y_{t-1} = y_{t-1},\ldots,Y_0 = y_0]}\\
&amp;=\P[X_t = x_t\,|\,X_{t-1} = x_{t-1}, \ldots,X_0 = x_0]
\,\P[Y_t = y_t\,|\,Y_{t-1} = y_{t-1}, \ldots,Y_0 = y_0].
\end{align*}\]</div>
<p>Now we use the fact that each is separately a Markov chain to simplify this last expression</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;= \P[X_t = x_t\,|\,X_{t-1} = x_{t-1}]
\,\P[Y_t = y_t\,|\,Y_{t-1} = y_{t-1}]\\
&amp;= \frac{\P[X_t = x_t,X_{t-1} = x_{t-1}]}{\P[X_{t-1} = x_{t-1}]}
\frac{\P[Y_t = y_t,Y_{t-1} = y_{t-1}]}{\P[Y_{t-1} = y_{t-1}]}\\
&amp;= \frac{\P[X_t = x_t,X_{t-1} = x_{t-1}]\,\P[Y_t = y_t,Y_{t-1} = y_{t-1}]}{\P[X_{t-1} = x_{t-1}]\,\P[Y_{t-1} = y_{t-1}]}\\
&amp;= \frac{\P[(X_t,Y_t) = (x_t,y_t),(X_{t-1},Y_{t-1}) = (x_{t-1},y_{t-1})]}{\P[(X_{t-1},Y_{t-1}) = (x_{t-1},y_{t-1})]}\\
&amp;= \P[(X_t,Y_t) = (x_t,y_t)\,|\,(X_{t-1},Y_{t-1}) = (x_{t-1},y_{t-1})].
\end{align*}\]</div>
<p>That proves the claim.</p>
<p><em>Step 2: Waiting until the marginal processes meet.</em> The idea of the coupling argument is to consider the first time <span class="math notranslate nohighlight">\(T\)</span> that <span class="math notranslate nohighlight">\(X_T = Y_T\)</span>. Note that <span class="math notranslate nohighlight">\(T\)</span> is a random time. But it is a special kind of random time often referred to as a stopping time. That is, the event <span class="math notranslate nohighlight">\(\{T=s\}\)</span> only depends on the trajectory of the joint chain <span class="math notranslate nohighlight">\((X_t,Y_t)\)</span> up to time <span class="math notranslate nohighlight">\(s\)</span>. Specifically,</p>
<div class="math notranslate nohighlight">
\[
\{T=s\}
= 
\left\{
((X_0,Y_0),\ldots,(X_{s-1},Y_{s-1})) \in \mathcal{N}^2_{s-1}, X_s = Y_s
\right\}
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
\mathcal{N}^2_{s-1}
=
\{
((x_0,y_0),\ldots,(x_{s-1},y_{s-1})) \in [\mathcal{S}\times\mathcal{S}]^{s}\,:\,x_i \neq y_i, \forall 0 \leq i \leq s-1
\}.
\]</div>
<p>Here is a remarkable observation. The distributions of <span class="math notranslate nohighlight">\(X_s\)</span> and <span class="math notranslate nohighlight">\(Y_s\)</span> are the same after the coupling time <span class="math notranslate nohighlight">\(T\)</span>.</p>
<p><strong>LEMMA</strong> <strong>(Distribution after Coupling)</strong> For all <span class="math notranslate nohighlight">\(s\)</span> and all <span class="math notranslate nohighlight">\(i\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\P[X_s = i, T \leq s]
= \P[Y_s = i, T \leq s].
\]</div>
<p><span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> We sum over the possible values of <span class="math notranslate nohighlight">\(T\)</span> and <span class="math notranslate nohighlight">\(X_T=Y_T\)</span>, and use the multiplication rule</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\P[X_s = i, T \leq s]\\
&amp;= \sum_{j=1}^n \sum_{h=0}^s 
\P[X_s = i, T=h, X_h = j]\\
&amp;= \sum_{j=1}^n \sum_{h=0}^s 
\P[X_s = i \,|\, T=h, X_h = j] \,\P[T=h, X_h = j]\\
&amp;= \sum_{j=1}^n \sum_{h=0}^s 
\P[X_s = i \,|\, ((X_0,Y_0),\ldots,(X_{h-1},Y_{h-1})) \in \mathcal{N}^2_{h-1}, (X_h, Y_h) = (j,j)]\\ 
&amp;\qquad\qquad\qquad\qquad \times \P[((X_0,Y_0),\ldots,(X_{h-1},Y_{h-1})) \in \mathcal{N}^2_{h-1}, (X_h, Y_h) = (j,j)].
\end{align*}\]</div>
<p>Using the Markov property for the joint process, in the form stated in <em>Exercise 3.36</em>, we get that this last expression is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;= \sum_{j=1}^n \sum_{h=0}^s 
\P[X_s = i \,|\, (X_h, Y_h) = (j,j)]\\ 
&amp;\qquad\qquad\times \P[((X_0,Y_0),\ldots,(X_{h-1},Y_{h-1})) \in \mathcal{N}^2_{h-1}, (X_h, Y_h) = (j,j)].
\end{align*}\]</div>
<p>By the independence of the marginal processes and the fact that they have the same transition matrix, this is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;= \sum_{j=1}^n \sum_{h=0}^s 
\P[X_s = i \,|\, X_h = j]\\ 
&amp;\qquad\qquad\times \P[((X_0,Y_0),\ldots,(X_{h-1},Y_{h-1})) \in \mathcal{N}^2_{h-1}, (X_h, Y_h) = (j,j)]\\
&amp;= \sum_{j=1}^n \sum_{h=0}^s 
\P[Y_s = i \,|\, Y_h = j]\\ 
&amp;\qquad\qquad\times \P[((X_0,Y_0),\ldots,(X_{h-1},Y_{h-1})) \in \mathcal{N}^2_{h-1}, (X_h, Y_h) = (j,j)].
\end{align*}\]</div>
<p>Arguing backwards gives <span class="math notranslate nohighlight">\(\P[Y_s = i, T \leq s]\)</span> and concludes the proof. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><em>Step 3: Bounding how long it takes for the marginal processes to meet.</em> Since</p>
<div class="math notranslate nohighlight">
\[
\P[X_t = i]
= \P[X_t = i, T \leq t]
+ \P[X_t = i, T &gt; t]
\]</div>
<p>and similarly for <span class="math notranslate nohighlight">\(\P[Y_t = i]\)</span>, using the previous lemma we get</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;|\P[X_t = i] - \P[Y_t = i]|\\
&amp;=
|\P[X_t = i, T &gt; t] - \P[Y_t = i, T &gt; t]|\\
&amp;\leq
\P[X_t = i, T &gt; t] + \P[Y_t = i, T &gt; t]\\
&amp;\leq 2 \P[T &gt; t].
\end{align*}\]</div>
<p>So it remains to show that <span class="math notranslate nohighlight">\(\P[T &gt; t]\)</span> goes to <span class="math notranslate nohighlight">\(0\)</span> as <span class="math notranslate nohighlight">\(t \to +\infty\)</span>.</p>
<p>We note that <span class="math notranslate nohighlight">\(\P[T &gt; t]\)</span> is non-increasing as a function of <span class="math notranslate nohighlight">\(t\)</span>. Indeed, for <span class="math notranslate nohighlight">\(h &gt; 0\)</span>, we have the implication</p>
<div class="math notranslate nohighlight">
\[
\{T &gt; t+h\} \subseteq \{T &gt; t\},
\]</div>
<p>so by the monotonicity of probabilities</p>
<div class="math notranslate nohighlight">
\[
\P[T &gt; t+h] \leq \P[T &gt; t].
\]</div>
<p>So it remains to prove the following lemma.</p>
<p><strong>LEMMA</strong> <strong>(Tail of Coupling Time)</strong> There is a <span class="math notranslate nohighlight">\(0 &lt; \beta &lt; 1\)</span> and a positive integer <span class="math notranslate nohighlight">\(m\)</span> such that, for all positive integers <span class="math notranslate nohighlight">\(k\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\P[T &gt; k m]
\leq \beta^{k m}.
\]</div>
<p><span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> Recall that the state space of the marginal processes is <span class="math notranslate nohighlight">\([n]\)</span>, so the joint process lives in <span class="math notranslate nohighlight">\([n]\times [n]\)</span>. Since the event <span class="math notranslate nohighlight">\(\{(X_m, Y_m) = (1,1)\}\)</span> implies <span class="math notranslate nohighlight">\(\{T \leq m\}\)</span>, to bound <span class="math notranslate nohighlight">\(\P[T &gt; m]\)</span> we note that</p>
<div class="math notranslate nohighlight">
\[
\P[T &gt; m]
= 1 - \P[T \leq m]
\leq 1 - \P[(X_m, Y_m) = (1,1)].
\]</div>
<p>To bound the probability on right-hand side, we construct a path of length <span class="math notranslate nohighlight">\(m\)</span> in the transition graph of the joint process from any state to <span class="math notranslate nohighlight">\((1,1)\)</span>.</p>
<p>For <span class="math notranslate nohighlight">\(i \in [n]\)</span>, let <span class="math notranslate nohighlight">\(\mathcal{P}_i = (z_{i,0},\ldots,z_{i,\ell_i})\)</span> be the shortest path in the transition graph of <span class="math notranslate nohighlight">\((X_t)_{t \geq 0}\)</span> from <span class="math notranslate nohighlight">\(i\)</span> to <span class="math notranslate nohighlight">\(1\)</span>, where <span class="math notranslate nohighlight">\(z_{i,0} = i\)</span> and <span class="math notranslate nohighlight">\(z_{i,\ell_i} = 1\)</span>. By irreducibility there exists such a path for any <span class="math notranslate nohighlight">\(i\)</span>. Here <span class="math notranslate nohighlight">\(\ell_i\)</span> is the length of <span class="math notranslate nohighlight">\(\mathcal{P}_i\)</span>, and we define</p>
<div class="math notranslate nohighlight">
\[
\ell^*
= \max_{i \neq 1} \ell_i.
\]</div>
<p>We make all the paths above the same length <span class="math notranslate nohighlight">\(\ell^*\)</span> by padding them with <span class="math notranslate nohighlight">\(1\)</span>s. That is, we define <span class="math notranslate nohighlight">\(\mathcal{P}_i^* = (z_{i,0},\ldots,z_{i,\ell_i},1,\ldots,1)\)</span> such that this path now has length <span class="math notranslate nohighlight">\(\ell^*\)</span>. This remains a path in the transition graph of <span class="math notranslate nohighlight">\((X_t)_{t \geq  0}\)</span> because the chain is lazy.</p>
<p>Now, for any pair of states <span class="math notranslate nohighlight">\((i,j) \in [n] \times [n]\)</span>, consider the path of length <span class="math notranslate nohighlight">\(m := 2 \ell^*\)</span></p>
<div class="math notranslate nohighlight">
\[
\mathcal{Q}^*_{(i,j)}
= ((z_{i,0},j),\ldots,(z_{i,\ell_i},j),(1,j),\ldots,(1,j),(1,z_{j,0}),\ldots,(1,z_{i,\ell_i}),(1,1),\ldots,(1,1)). 
\]</div>
<p>In words, we leave the second component at <span class="math notranslate nohighlight">\(j\)</span> while running through path <span class="math notranslate nohighlight">\(\mathcal{P}_i^*\)</span> on the first component, then we leave the first component at <span class="math notranslate nohighlight">\(1\)</span> while running through path <span class="math notranslate nohighlight">\(\mathcal{P}_j^*\)</span> on the second component. Path <span class="math notranslate nohighlight">\(\mathcal{Q}^*_{(i,j)}\)</span> is a valid path in the transition graph of the joint chain. Again, we are using that the marginal processes are lazy.</p>
<p>Denote by <span class="math notranslate nohighlight">\(\mathcal{Q}^*_{(i,j)}[r]\)</span> be the <span class="math notranslate nohighlight">\(r\)</span>-th state in path <span class="math notranslate nohighlight">\(\mathcal{Q}^*_{(i,j)}\)</span>. Going back to bounding <span class="math notranslate nohighlight">\(\P[(X_m, Y_m) = (1,1)]\)</span>, we define <span class="math notranslate nohighlight">\(\beta\)</span> as follows</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\P[(X_m, Y_m) = (1,1)\,|\,(X_0, Y_0) = (i,j)]\\
&amp;\geq \min_{(i,j)} \P[(X_r, Y_r) = \mathcal{Q}^*_{(i,j)}[r], \forall r=0,\ldots,m \,|\,(X_0, Y_0) = (i_0,j_0)]\\
&amp;=: 1-\beta \in (0,1).
\end{align*}\]</div>
<p>By the law of total probability,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\P[(X_m, Y_m) = (1,1)]\\
&amp;= \sum_{(i,j) \in [n]\times [n]}\P[(X_m, Y_m) = (1,1)\,|\,(X_0, Y_0) = (i,j)]\, \P[(X_0, Y_0) = (i,j)]\\
&amp;= \sum_{(i,j) \in [n]\times [n]}\P[(X_m, Y_m) = (1,1)\,|\,(X_0, Y_0) = (i,j)]\, \mu_i \pi_j\\
&amp;\geq \sum_{(i,j) \in [n]\times [n]} \P[(X_r, Y_r) = \mathcal{Q}^*_{(i,j)}[r], \forall r=0,\ldots,m \,|\,(X_0, Y_0) = (i,j)]\, \mu_i \pi_j\\
&amp;\geq (1-\beta) \sum_{(i,j) \in [n]\times [n]} \mu_i \pi_j\\
&amp;= 1-\beta.
\end{align*}\]</div>
<p>So we have</p>
<div class="math notranslate nohighlight">
\[
\P[T &gt; m]
\leq \beta.
\]</div>
<p>Because</p>
<div class="math notranslate nohighlight">
\[
\{T &gt; 2m\} \subseteq \{T &gt; m\},
\]</div>
<p>it holds that by the multiplication rule that</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\P[T &gt; 2m]
&amp;= \P[\{T &gt; 2m\}\cap\{T &gt; m\}]\\
&amp;= \P[T &gt; 2m\,|\, T &gt; m]\,\P[T &gt; m]\\
&amp;\leq \P[T &gt; 2m\,|\, T &gt; m]\,\beta.
\end{align*}\]</div>
<p>Summing over all state pairs at time <span class="math notranslate nohighlight">\(m\)</span>, we get</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\P[T &gt; 2m\,|\, T &gt; m]\\
&amp;= \sum_{(i,j) \in [n]\times [n]} \P[T &gt; 2m\,|\, T &gt; m, (X_m,Y_m) = (i,j)] 
\,\P[(X_m,Y_m) = (i,j) \,|\, T &gt; m].
\end{align*}\]</div>
<p>Arguing as above, note that for <span class="math notranslate nohighlight">\(i \neq j\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\P[T \leq 2m\,|\, T &gt; m, (X_m,Y_m) = (i,j)]\\
&amp;\geq  \P[(X_{2m},Y_{2m}) = (1,1) \,|\, T &gt; m, (X_m,Y_m) = (i,j)]\\
&amp;=  \P[(X_{2m},Y_{2m}) = (1,1) \,|\, (X_m,Y_m) = (i,j), ((X_0,Y_0),\ldots,(X_{m-1},Y_{m-1})) \in \mathcal{N}^2_{m-1}]\\
&amp;=  \P[(X_{2m},Y_{2m}) = (1,1) \,|\, (X_m,Y_m) = (i,j)]\\
&amp;=  \P[(X_{m},Y_{m}) = (1,1) \,|\, (X_0,Y_0) = (i,j)]\\
&amp;\geq 1-\beta,
\end{align*}\]</div>
<p>by the Markov property and the time-homogeneity of the process.</p>
<p>Plugging above and noting that <span class="math notranslate nohighlight">\(\P[(X_m,Y_m) = (i,j) \,|\, T &gt; m] = 0\)</span> when <span class="math notranslate nohighlight">\(i = j\)</span>, we get that</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\P[T &gt; 2m\,|\, T &gt; m]\\
&amp;\geq \sum_{\substack{(i,j) \in [n]\times [n]\\i \neq j}} \beta
\,\P[(X_m,Y_m) = (i,j) \,|\, T &gt; m]\\
&amp;= \beta.
\end{align*}\]</div>
<p>So we have proved that <span class="math notranslate nohighlight">\(\P[T &gt; 2m] \leq \beta^2\)</span>. Proceeding similarly by induction gives the claim. <span class="math notranslate nohighlight">\(\square\)</span></p>
</section>
<section id="random-walk-on-a-weighted-graph">
<h2><span class="section-number">7.8.6. </span>Random walk on a weighted graph<a class="headerlink" href="#random-walk-on-a-weighted-graph" title="Link to this heading">#</a></h2>
<p>The previous definitions extend naturally to the weighted case. Again we allow loops, i.e., self-weights <span class="math notranslate nohighlight">\(w_{i,i} &gt; 0\)</span>. For a weighted graph <span class="math notranslate nohighlight">\(G\)</span>, recall that the degree of a vertex is defined as</p>
<div class="math notranslate nohighlight">
\[
\delta(i)
= \sum_{j} w_{i,j},
\]</div>
<p>which includes the self-weight <span class="math notranslate nohighlight">\(w_{i,i}\)</span>, and where we use the convention that <span class="math notranslate nohighlight">\(w_{i,j} = 0\)</span> if <span class="math notranslate nohighlight">\(\{i,j\} \notin E\)</span>. Recall also that <span class="math notranslate nohighlight">\(w_{i,j} = w_{j,i}\)</span>.</p>
<p><strong>DEFINITION</strong> <strong>(Random Walk on a Weighted Graph)</strong> Let <span class="math notranslate nohighlight">\(G = (V,E,w)\)</span> be a weighted graph with positive edge weights. Assume all vertices have a positive degree. A random walk on <span class="math notranslate nohighlight">\(G\)</span> is a time-homogeneous Markov chain <span class="math notranslate nohighlight">\((X_t)_{t \geq 0}\)</span> with state space <span class="math notranslate nohighlight">\(\mathcal{S} = V\)</span> and transition probabilities</p>
<div class="math notranslate nohighlight">
\[
p_{i,j} = \P[X_{t+1} = j\,|\,X_{t} = i] = \frac{w_{i,j}}{\sum_{k} w_{i,k}},
\qquad
\forall i,j \in V.
\]</div>
<p><span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>Once again, it is easily seen that the transition matrix of random walk on <span class="math notranslate nohighlight">\(G\)</span> satisfying the conditions of the definition above is
<span class="math notranslate nohighlight">\(
P = D^{-1} A,
\)</span>
where <span class="math notranslate nohighlight">\(D = \mathrm{diag}(A \mathbf{1})\)</span> is the degree matrix.</p>
<p><strong>EXAMPLE:</strong> <strong>(A Weighted Graph)</strong> Here is another example. Consider the following adjacency matrix on <span class="math notranslate nohighlight">\(5\)</span> vertices.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A_ex</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">7</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
</pre></div>
</div>
</div>
</div>
<p>It is indeed a symmetric matrix.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">A_ex</span><span class="o">.</span><span class="n">T</span> <span class="o">-</span> <span class="n">A_ex</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.0
</pre></div>
</div>
</div>
</div>
<p>We define a graph from its adjacency matrix.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_ex</span> <span class="o">=</span> <span class="n">A_ex</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># number of vertices</span>
<span class="n">G_ex</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">from_numpy_array</span><span class="p">(</span><span class="n">A_ex</span><span class="p">)</span> <span class="c1"># graph</span>
</pre></div>
</div>
</div>
</div>
<p>To draw it, we first define edge labels by creating a dictionary that assigns to each edge (as a tuple) its weight. Here <code class="docutils literal notranslate"><span class="pre">G.edges.data('weight')</span></code> (see <a class="reference external" href="https://networkx.org/documentation/stable/reference/classes/generated/networkx.Graph.edges.html"><code class="docutils literal notranslate"><span class="pre">G.edges</span></code></a>) iterates through the edges <code class="docutils literal notranslate"><span class="pre">(u,v)</span></code> and includes their weight as the third entry of the tuple <code class="docutils literal notranslate"><span class="pre">(u,v,w)</span></code>. Then we use the function <a class="reference external" href="https://networkx.org/documentation/stable/reference/generated/networkx.drawing.nx_pylab.draw_networkx_edge_labels.html#networkx.drawing.nx_pylab.draw_networkx_edge_labels"><code class="docutils literal notranslate"><span class="pre">networkx.draw_networkx_edge_labels()</span></code></a> to add the weights as edge labels.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">edge_labels</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="p">(</span><span class="n">u</span><span class="p">,</span><span class="n">v</span><span class="p">,</span><span class="n">w</span><span class="p">)</span> <span class="ow">in</span> <span class="n">G_ex</span><span class="o">.</span><span class="n">edges</span><span class="o">.</span><span class="n">data</span><span class="p">(</span><span class="s1">&#39;weight&#39;</span><span class="p">):</span>
    <span class="n">edge_labels</span><span class="p">[(</span><span class="n">u</span><span class="p">,</span><span class="n">v</span><span class="p">)]</span> <span class="o">=</span> <span class="n">w</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pos</span><span class="o">=</span><span class="n">nx</span><span class="o">.</span><span class="n">circular_layout</span><span class="p">(</span><span class="n">G_ex</span><span class="p">)</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G_ex</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_ex</span><span class="p">)},</span>
                 <span class="n">node_color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">)</span>
<span class="n">edge_labels</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx_edge_labels</span><span class="p">(</span><span class="n">G_ex</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">edge_labels</span><span class="o">=</span><span class="n">edge_labels</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/a850bd86578901e27f0cca2f4f7ba8657081d47dce941afe5ef2e52a7904f017.png" src="../../_images/a850bd86578901e27f0cca2f4f7ba8657081d47dce941afe5ef2e52a7904f017.png" />
</div>
</div>
<p>The transition matrix of the random walk on this graph can be computed using the lemma above. We first compute the degree matrix, then apply the formula.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">degrees_ex</span> <span class="o">=</span> <span class="n">A_ex</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_ex</span><span class="p">)</span>
<span class="n">inv_degrees_ex</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span> <span class="n">degrees_ex</span>
<span class="n">Dinv_ex</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">inv_degrees_ex</span><span class="p">)</span>
<span class="n">P_ex</span> <span class="o">=</span> <span class="n">Dinv_ex</span> <span class="o">@</span> <span class="n">A_ex</span>
<span class="nb">print</span><span class="p">(</span><span class="n">P_ex</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[0.         0.88888889 0.         0.11111111 0.        ]
 [0.53333333 0.         0.4        0.06666667 0.        ]
 [0.         0.46153846 0.         0.         0.53846154]
 [0.08333333 0.08333333 0.         0.         0.83333333]
 [0.         0.         0.41176471 0.58823529 0.        ]]
</pre></div>
</div>
</div>
</div>
<p>This is indeed a stochastic matrix.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">P_ex</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_ex</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1. 1. 1. 1. 1.]
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>LEMMA</strong> <strong>(Irreducibility in Undirected Case)</strong> Let <span class="math notranslate nohighlight">\(G = (V,E,w)\)</span> be a graph with positive edge weights. Assume all vertices have a positive degree. Random walk on <span class="math notranslate nohighlight">\(G\)</span> is irreducible if and only if <span class="math notranslate nohighlight">\(G\)</span> is connected. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><strong>THEOREM</strong> <strong>(Stationary Distribution on a Graph)</strong> Let <span class="math notranslate nohighlight">\(G = (V,E,w)\)</span> be a graph with positive edge weights. Assume further that <span class="math notranslate nohighlight">\(G\)</span> is connected. Then the unique stationary distribution of random walk on <span class="math notranslate nohighlight">\(G\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[
\pi_i = \frac{\delta(i)}{\sum_{i \in V} \delta(i)}, \qquad \forall i \in V.
\]</div>
<p><span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p><strong>EXAMPLE:</strong> <strong>(A Weighted Graph, continued)</strong> Going back to our weighted graph example, we use the previous theorem to compute the stationary distribution. Note that the graph is indeed connected so the stationary distribution is unique. We have already computed the degrees.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">degrees_ex</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[ 9. 15. 13. 12. 17.]
</pre></div>
</div>
</div>
</div>
<p>We compute <span class="math notranslate nohighlight">\(\sum_{i \in V} \delta(i)\)</span> next.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">total_degrees_ex</span> <span class="o">=</span> <span class="n">degrees_ex</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_ex</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">total_degrees_ex</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>66.0
</pre></div>
</div>
</div>
</div>
<p>Finally, the stationary distribution is:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pi_ex</span> <span class="o">=</span> <span class="n">degrees_ex</span> <span class="o">/</span> <span class="n">total_degrees_ex</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pi_ex</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.13636364 0.22727273 0.1969697  0.18181818 0.25757576]
</pre></div>
</div>
</div>
</div>
<p>We check stationarity.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">P_ex</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">pi_ex</span> <span class="o">-</span> <span class="n">pi_ex</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2.7755575615628914e-17
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p>A random walk on a weighted undirected graph is reversible. Vice versa, it turns out that any reversible chain can be seen as a random walk on an appropriately defined weighted undirected graph. See the exercises.</p>
</section>
<section id="spectral-techniques-for-random-walks-on-graphs">
<h2><span class="section-number">7.8.7. </span>Spectral techniques for random walks on graphs<a class="headerlink" href="#spectral-techniques-for-random-walks-on-graphs" title="Link to this heading">#</a></h2>
<p>In this section, we use techniques from spectral graph theory to analyze random walks on graphs.</p>
<p><strong>Applying the spectral theorem via the normalized Laplacian</strong> We have seen how to compute the unique stationary distribution <span class="math notranslate nohighlight">\(\bpi\)</span> of random walk on a connected weighted (undirected) graph. Recall that <span class="math notranslate nohighlight">\(\bpi\)</span> is a (left) eigenvector of <span class="math notranslate nohighlight">\(P\)</span> with eigenvalue <span class="math notranslate nohighlight">\(1\)</span> (i.e., <span class="math notranslate nohighlight">\(\bpi P = \bpi\)</span>). In general, however, the matrix <span class="math notranslate nohighlight">\(P\)</span> is <em>not</em> symmetric in this case (see the previous example) - even though the adjacency matrix is. So we cannot apply the spectral theorem to get the rest of the eigenvectors - if they even exist. But, remarkably, a symmetric matrix with the a closely related spectral decomposition is hiding in the background.</p>
<p>Recall that the normalized Laplacian of a weighted graph <span class="math notranslate nohighlight">\(G = (V,E,w)\)</span> with adjacency matrix <span class="math notranslate nohighlight">\(A\)</span> and degree matrix <span class="math notranslate nohighlight">\(D\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L} = I - D^{-1/2} A D^{-1/2}.
\]</div>
<p>Recall that in the weighted case, the degree is defined as <span class="math notranslate nohighlight">\(\delta(i) = \sum_{j:\{i,j\} \in E} w_{i,j}\)</span>. Because it is symmetric and positive semi-definite, we can write</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}
= \sum_{i=1}^n
\eta_i \mathbf{z}_i \mathbf{z}_i^T,
\]</div>
<p>where the <span class="math notranslate nohighlight">\(\mathbf{z}_i\)</span>s are orthonormal eigenvectors of <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> and the eigenvalues satisfy <span class="math notranslate nohighlight">\(0 \leq \eta_1 \leq \eta_2 \leq \cdots \leq \eta_n\)</span>.</p>
<p>Moreover, <span class="math notranslate nohighlight">\(D^{1/2} \mathbf{1}\)</span> is an eigenvector of <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> with eigenvalue <span class="math notranslate nohighlight">\(0\)</span>. So <span class="math notranslate nohighlight">\(\eta_1 = 0\)</span> and we set</p>
<div class="math notranslate nohighlight">
\[
(\mathbf{z}_1)_i 
= \left(\frac{D^{1/2} \mathbf{1}}{\|D^{1/2} \mathbf{1}\|_2}\right)_i
= \sqrt{\frac{\delta(i)}{\sum_{i\in V} \delta(i)}}, \quad \forall i \in [n],
\]</div>
<p>which makes <span class="math notranslate nohighlight">\(\mathbf{z}_1\)</span> into a unit norm vector.</p>
<p>We return to the eigenvectors of <span class="math notranslate nohighlight">\(P\)</span>. When a matrix <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times n}\)</span> is diagonalizable, it has an eigendecomposition of the form</p>
<div class="math notranslate nohighlight">
\[
A = Q \Lambda Q^{-1},
\]</div>
<p>where <span class="math notranslate nohighlight">\(\Lambda\)</span> is a diagonal matrix whose diagonal entries are the eigenvalues of <span class="math notranslate nohighlight">\(A\)</span>. The columns of <span class="math notranslate nohighlight">\(Q\)</span> are the eigenvectors of <span class="math notranslate nohighlight">\(A\)</span> and they form a basis of <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>. Unlike the symmetric case, however, the eigenvectors need not be orthogonal.</p>
<p><strong>THEOREM</strong> <strong>(Eigendecomposition of Random Walk on a Graph)</strong> Let <span class="math notranslate nohighlight">\(G = (V,E,w)\)</span> be a graph with positive edge weights and no isolated vertex, and with degree matrix <span class="math notranslate nohighlight">\(D\)</span>. Let <span class="math notranslate nohighlight">\(P \in \mathbb{R}^{n \times n}\)</span> be the transition matrix of random walk on <span class="math notranslate nohighlight">\(G\)</span>. Let <span class="math notranslate nohighlight">\(\mathbf{z}_1,\ldots,\mathbf{z}_n \in \mathbb{R}^n\)</span> and <span class="math notranslate nohighlight">\(0 \leq \eta_1 \leq \cdots \eta_n \leq 2\)</span> be the eigenvectors and eigenvalues of the normalized Laplacian. Then <span class="math notranslate nohighlight">\(P\)</span> has the following eigendecomposition</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
P
&amp;= (D^{-1/2} Z)(I - H) (D^{-1/2} Z)^{-1}
\end{align*}\]</div>
<p>where the columns of <span class="math notranslate nohighlight">\(Z\)</span> are the <span class="math notranslate nohighlight">\(\mathbf{z}_i\)</span>s and <span class="math notranslate nohighlight">\(H\)</span> is a diagonal matrix with the <span class="math notranslate nohighlight">\(\eta_i\)</span>s on its diagonal. This can also be written as</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
P
&amp;= \mathbf{1}\bpi
+ \sum_{i=2}^n \lambda_i D^{-1/2} \mathbf{z}_i \mathbf{z}_i^T D^{1/2},
\end{align*}\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
\pi_i = \frac{\delta(i)}{\sum_{j\in V} \delta(j)} \quad \text{and} \quad \lambda_i = 1- \eta_i, \qquad i =1,\ldots,n.
\]</div>
<p><span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p><em>Proof:</em> We write <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> in terms of <span class="math notranslate nohighlight">\(P\)</span>. Recall that <span class="math notranslate nohighlight">\(P = D^{-1} A\)</span>. Hence</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L} 
= I - D^{1/2} P D^{-1/2}.
\]</div>
<p>Rearranging this becomes</p>
<div class="math notranslate nohighlight">
\[
P = I - D^{-1/2} \mathcal{L} D^{1/2}.
\]</div>
<p>Hence for all <span class="math notranslate nohighlight">\(i=1,\ldots,n\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
P (D^{-1/2} \mathbf{z}_i) 
&amp;= (I - D^{-1/2} \mathcal{L} D^{1/2})\,(D^{-1/2} \mathbf{z}_i)\\
&amp;= (D^{-1/2} \mathbf{z}_i) - D^{-1/2} \mathcal{L} D^{1/2} (D^{-1/2} \mathbf{z}_i)\\
&amp;= (D^{-1/2} \mathbf{z}_i) - D^{-1/2} \mathcal{L} \mathbf{z}_i\\
&amp;= (D^{-1/2} \mathbf{z}_i) - D^{-1/2} \eta_i \mathbf{z}_i\\
&amp;= (1 - \eta_i) (D^{-1/2} \mathbf{z}_i).
\end{align*}\]</div>
<p>Because <span class="math notranslate nohighlight">\(P\)</span> is a transition matrix, all its eigenvalues are bounded in absolute value by <span class="math notranslate nohighlight">\(1\)</span>. So <span class="math notranslate nohighlight">\(|1-\eta_i|\leq 1\)</span>, which implies <span class="math notranslate nohighlight">\(0 \leq \eta_i \leq 2\)</span>.</p>
<p>We also note that</p>
<div class="math notranslate nohighlight">
\[
(D^{-1/2} Z)
(D^{1/2} Z)^T
= D^{-1/2} Z Z^T D^{1/2}
= D^{-1/2} D^{1/2}
= I,
\]</div>
<p>by the orthonormality of the eigenvectors of the normalized Laplacian. So the columns of <span class="math notranslate nohighlight">\(D^{-1/2} Z\)</span>, i.e, <span class="math notranslate nohighlight">\(D^{-1/2} \mathbf{z}_i\)</span> for <span class="math notranslate nohighlight">\(i=1,\ldots,n\)</span>, are linearly independent and</p>
<div class="math notranslate nohighlight">
\[
(D^{-1/2} Z)^{-1}
= (D^{1/2} Z)^T.
\]</div>
<p>That gives the first claim.</p>
<p>To get the second claim, we first note that (for similar calculations, see the definition of an SVD)</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
P
&amp;= (D^{-1/2} Z)(I - H) (D^{-1/2} Z)^{-1}\\
&amp;= (D^{-1/2} Z)(I - H) (D^{1/2} Z)^T\\
&amp;= D^{-1/2} Z (I - H) Z^T D^{1/2}\\
&amp;= \sum_{i=1}^n \lambda_i D^{-1/2} \mathbf{z}_i \mathbf{z}_i^T D^{1/2},
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda_i = 1- \eta_i\)</span>.</p>
<p>We then use the expression for <span class="math notranslate nohighlight">\(\mathbf{z}_1\)</span> above. We have</p>
<div class="math notranslate nohighlight">
\[
D^{-1/2} \mathbf{z}_1
= D^{-1/2} \frac{D^{1/2}\mathbf{1}}{\|D^{1/2}\mathbf{1}\|_2}
= \frac{\mathbf{1}}{\|D^{1/2}\mathbf{1}\|_2},
\]</div>
<p>while</p>
<div class="math notranslate nohighlight">
\[
\mathbf{z}_i^T D^{1/2}
= (D^{1/2} \mathbf{z}_1)^T
= \left(D^{1/2} \frac{D^{1/2}\mathbf{1}}{\|D^{1/2}\mathbf{1}\|_2}\right)^T
= \left(\frac{D \mathbf{1}}{\|D^{1/2}\mathbf{1}\|_2}\right)^T.
\]</div>
<p>So</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
D^{-1/2} \mathbf{z}_1 \mathbf{z}_1^T D^{1/2}
&amp;= \frac{\mathbf{1}}{\|D^{1/2}\mathbf{1}\|_2} \left(\frac{D \mathbf{1}}{\|D^{1/2}\mathbf{1}\|_2}\right)^T\\
&amp;= \mathbf{1} \left(\frac{D \mathbf{1}}{\|D^{1/2}\mathbf{1}\|_2^2}\right)^T\\
&amp;= \mathbf{1} \left(\frac{D \mathbf{1}}{\|D \mathbf{1}\|_1}\right)^T\\
&amp;= \mathbf{1} \bpi.
\end{align*}\]</div>
<p>That proves the second claim. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>If <span class="math notranslate nohighlight">\(G\)</span> is connected and <span class="math notranslate nohighlight">\(w_{i,i} &gt; 0\)</span> for all <span class="math notranslate nohighlight">\(i\)</span>, then the chain is irreducible and lazy. In that case, there is a unique eigenvalue <span class="math notranslate nohighlight">\(1\)</span> and <span class="math notranslate nohighlight">\(-1\)</span> is not an eigenvalue, so we must have <span class="math notranslate nohighlight">\(0 &lt; \eta_2 \leq \cdots \leq \eta_n &lt; 2\)</span>.</p>
<p><strong>Limit theorems revisited</strong> The <em>Convergence to Equilibrium Theorem</em> implies that in the irreducible, aperiodic case</p>
<div class="math notranslate nohighlight">
\[
\bmu P^t \to \bpi,
\]</div>
<p>as <span class="math notranslate nohighlight">\(t \to +\infty\)</span>, for any initial distribution <span class="math notranslate nohighlight">\(\bmu\)</span> and the unique stationary distribution <span class="math notranslate nohighlight">\(\bpi\)</span>. Here we give a simpler proof for random walk on a graph (or more generally a reversible chain), with the added bonus of a convergence rate. This follows from the same argument we used in the <em>Power Iteration Lemma</em>.</p>
<p><strong>DEFINITION</strong> <strong>(Spectral Gap)</strong> Let <span class="math notranslate nohighlight">\(G = (V,E,w)\)</span> be a graph with positive edge weights and no isolated vertex. Let <span class="math notranslate nohighlight">\(P \in \mathbb{R}^{n \times n}\)</span> be the transition matrix of random walk on <span class="math notranslate nohighlight">\(G\)</span>. The absolute spectral gap of <span class="math notranslate nohighlight">\(G\)</span> is defined as <span class="math notranslate nohighlight">\(\gamma_{\star} = 1 - \lambda_{\star}\)</span> where</p>
<div class="math notranslate nohighlight">
\[
\lambda_{\star}
= \max\{|\lambda|\,:\, \text{$\lambda$ is an eigenvalue of $P$, $\lambda \neq 1$} \}.
\]</div>
<p><span class="math notranslate nohighlight">\(\natural\)</span></p>
<p><strong>THEOREM</strong> <strong>(Convergence to Equilibrium: Reversible Case)</strong> Let <span class="math notranslate nohighlight">\(G = (V,E,w)\)</span> be a connected graph with positive edge weights and <span class="math notranslate nohighlight">\(w_{x,x} &gt; 0\)</span> for all <span class="math notranslate nohighlight">\(x \in V\)</span>. Let <span class="math notranslate nohighlight">\(P \in \mathbb{R}^{n \times n}\)</span> be the transition matrix of random walk on <span class="math notranslate nohighlight">\(G\)</span> and <span class="math notranslate nohighlight">\(\bpi\)</span> its unique stationary distribution. Then</p>
<div class="math notranslate nohighlight">
\[
\bmu P^t \to \bpi,
\]</div>
<p>as <span class="math notranslate nohighlight">\(t \to +\infty\)</span> for any initial distribution <span class="math notranslate nohighlight">\(\bmu\)</span>. Moreover,</p>
<div class="math notranslate nohighlight">
\[
\left|P^t_{x,y} - \pi_y\right| \leq \gamma_\star^t \sqrt{\frac{\bar{\delta}}{\underline{\delta}}},
\]</div>
<p>where <span class="math notranslate nohighlight">\(\bar{\delta} = \max_x \delta(x)\)</span>, <span class="math notranslate nohighlight">\(\underline{\delta} = \min_x \delta(x)\)</span> and <span class="math notranslate nohighlight">\(\gamma_\star\)</span> is the absolute spectral gap. <span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p><em>Proof:</em> Similarly to the <em>Power Iteration Lemma</em>, using the <em>Eigendecomposition of Random Walk on a Graph</em> we get</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
P^2
&amp;= (D^{-1/2} Z)(I - H) (D^{-1/2} Z)^{-1}(D^{-1/2} Z)(I - H) (D^{-1/2} Z)^{-1}\\
&amp;= (D^{-1/2} Z)(I - H)^2 (D^{-1/2} Z)^{-1}.
\end{align*}\]</div>
<p>By induction,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
P^t
&amp;= (D^{-1/2} Z)(I - H)^t (D^{-1/2} Z)^{-1}\\
&amp;= \sum_{i=1}^n \lambda_i^t (D^{-1/2} \mathbf{z}_i) (D^{1/2} \mathbf{z}_i)^T\\
&amp;= \mathbf{1} \bpi + \sum_{i=2}^n \lambda_i^t (D^{-1/2} \mathbf{z}_i) (D^{1/2} \mathbf{z}_i)^T,
\end{align*}\]</div>
<p>by calculations similar to the proof of the <em>Eigendecomposition of Random Walk on a Graph</em>.</p>
<p>In the irreducible, lazy case, for <span class="math notranslate nohighlight">\(i=2,\ldots,n\)</span>, <span class="math notranslate nohighlight">\(\lambda_i^t \to 0\)</span> as <span class="math notranslate nohighlight">\(t \to +\infty\)</span>.</p>
<p>Moreover, <span class="math notranslate nohighlight">\(|\lambda_i| \leq (1-\gamma_\star)\)</span> for all <span class="math notranslate nohighlight">\(i=2,\ldots,n\)</span>. Hence,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\left|P^t_{x,y} - \pi_y\right|
&amp;= \sum_{i=2}^n \lambda_i^t \delta(x)^{-1/2}(\mathbf{z}_i)_x (\mathbf{z}_i)_y \delta(y)^{1/2}\\
&amp;\leq (1-\gamma_\star)^t \sqrt{\frac{\delta(y)}{\delta(x)}} \sum_{i=2}^n  |(\mathbf{z}_i)_x (\mathbf{z}_i)_y|.
\end{align*}\]</div>
<p>We then use <em>Cauchy-Schwarz</em> and the fact that <span class="math notranslate nohighlight">\(Z Z^T = I\)</span> (as <span class="math notranslate nohighlight">\(Z\)</span> is an orthogonal matrix), which implies <span class="math notranslate nohighlight">\(\sum_{i=1}^n (\mathbf{z}_i)_x^2 = 1\)</span>.</p>
<p>We get that the above is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\leq (1-\gamma_\star)^t \sqrt{\frac{\delta(y)}{\delta(x)}} 
\sum_{i=2}^n  (\mathbf{z}_i)_x^2 \sum_{i=2}^n  (\mathbf{z}_i)_y^2\\
&amp;\leq (1-\gamma_\star)^t \sqrt{\frac{\delta(y)}{\delta(x)}}\\
&amp;\leq (1-\gamma_\star)^t \sqrt{\frac{\bar{\delta}}{\underline{\delta}}}.
\end{align*}\]</div>
<p><span class="math notranslate nohighlight">\(\square\)</span></p>
<p>We record an immediate corollary that will be useful next.  Let <span class="math notranslate nohighlight">\(f : V \to \mathbb{R}\)</span> be a function over the vertices. Define the (column) vector <span class="math notranslate nohighlight">\(\mathbf{f} = (f(1),\ldots,f(n))^T\)</span> and note that</p>
<div class="math notranslate nohighlight">
\[
\bpi \mathbf{f}
= \sum_{x \in V} \pi_x f(x).
\]</div>
<p>It will be convenient to use to <span class="math notranslate nohighlight">\(\ell_\infty\)</span>-norm. For a vector <span class="math notranslate nohighlight">\(\mathbf{x} = (x_1,\ldots,x_n)^T\)</span>, we let <span class="math notranslate nohighlight">\(\|\mathbf{x}\|_\infty = \max_{i \in [n]} |x_i|\)</span>.</p>
<p><strong>THEOREM</strong> For any initial distribution <span class="math notranslate nohighlight">\(\bmu\)</span> and any <span class="math notranslate nohighlight">\(t\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\left|\,\E[f(X_t)] - \bpi \mathbf{f}\,\right| \leq (1-\gamma_\star)^t \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty.
\end{align*}\]</div>
<p><span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p><em>Proof:</em> By the <em>Time Marginals Theorem</em>,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\left|\,\E[f(X_t)] - \bpi \mathbf{f}\,\right|
&amp;= \left|\,\sum_{x} \sum_y \mu_x (P^t)_{x,y} f(y) - \sum_{y} \pi_y f(y)\,\right|.
\end{align*}\]</div>
<p>Because <span class="math notranslate nohighlight">\(\sum_{x} \mu_x = 1\)</span>, the right-hand side is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;= \left|\,\sum_{x} \sum_y \mu_x (P^t)_{x,y} f(y) - \sum_x \sum_{y} \mu_x \pi_y f(y)\,\right|\\
&amp;\leq \sum_{x} \mu_x  \sum_y \left| (P^t)_{x,y} - \pi_y  \right| |f(y)|,
\end{align*}\]</div>
<p>by the triangle inequality.</p>
<p>Now by the theorem this is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\leq \sum_{x} \mu_x  \sum_y  (1-\gamma_\star)^t \frac{\pi_y}{\pi_{\min{}}}|f(y)|\\
&amp;= (1-\gamma_\star)^t \frac{1}{\pi_{\min{}}} \sum_{x} \mu_x  \sum_y   \pi_y |f(y)|\\
&amp;\leq (1-\gamma_\star)^t \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty.
\end{align*}\]</div>
<p>That proves the claim. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>We also prove a version of the <em>Ergodic Theorem</em>.</p>
<p><strong>THEOREM</strong> <strong>(Ergodic Theorem: Reversible Case)</strong> Let <span class="math notranslate nohighlight">\(G = (V,E,w)\)</span> be a connected graph with positive edge weights and <span class="math notranslate nohighlight">\(w_{x,x} &gt; 0\)</span> for all <span class="math notranslate nohighlight">\(x \in V\)</span>. Let <span class="math notranslate nohighlight">\(P \in \mathbb{R}^{n \times n}\)</span> be the transition matrix of random walk on <span class="math notranslate nohighlight">\(G\)</span> and <span class="math notranslate nohighlight">\(\bpi\)</span> its unique stationary distribution. Let <span class="math notranslate nohighlight">\(f : V \to \mathbb{R}\)</span> be a function over the vertices. Then for any initial distribution <span class="math notranslate nohighlight">\(\bmu\)</span></p>
<div class="math notranslate nohighlight">
\[
\frac{1}{T} \sum_{t=1}^T f(X_{t}) \to \sum_{x \in V} \pi_x f(x),
\]</div>
<p>in probability as <span class="math notranslate nohighlight">\(T \to +\infty\)</span>. <span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p><em>Proof idea:</em> We use <em>Chebyshev’s Inequality</em>. By the <em>Convergence Theorem: Reversible Case</em>, the expectation converges to the limit. To bound the variance, we use the <em>Eigendecomposition of Random Walk on a Graph</em>.</p>
<p><em>Proof:</em> We use <em>Chebyshev’s Inequality</em>, similarly to our proof of the <em>Weak Law of Large Numbers</em>. However, unlike that case, here the terms in the sum are not independent are require some finessing. Define again the (column) vector <span class="math notranslate nohighlight">\(\mathbf{f} = (f(1),\ldots,f(n))\)</span>. Then the limit can be written as</p>
<div class="math notranslate nohighlight">
\[
\sum_{x \in V} \pi_x f(x)
= \bpi \mathbf{f}.
\]</div>
<p>By the corollary, the expectation of the sum can be bounded as follows</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\left|\,\E\left[\frac{1}{T} \sum_{t=1}^T f(X_{t})\right] - \bpi \mathbf{f}\,\right|
&amp;\leq \frac{1}{T} \sum_{t=1}^T\left|\E[f(X_{t})] - \bpi \mathbf{f}\right|\\
&amp;\leq \frac{1}{T} \sum_{t=1}^T (1-\gamma_\star)^t \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty\\
&amp;\leq  \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty \frac{1}{T} \sum_{t=0}^{+\infty} (1-\gamma_\star)^t\\
&amp;= \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty \gamma_\star^{-1}\frac{1}{T} \to 0
\end{align*}\]</div>
<p>as <span class="math notranslate nohighlight">\(T \to +\infty\)</span>.</p>
<p>Next we bound the variance of the sum. By the <em>Variance of a Sum</em>,</p>
<div class="math notranslate nohighlight">
\[
\mathrm{Var}\left[\frac{1}{T} \sum_{t=1}^T f(X_{t})\right]
= \frac{1}{T^2} \sum_{t=1}^T \mathrm{Var}[f(X_{t})] + \frac{2}{T^2} \sum_{1 \leq s &lt; t\leq T} \mathrm{Cov}[f(X_{s}),f(X_{t})].
\]</div>
<p>We bound the variance and covariance separately using the <em>Eigendecomposition of Random Walk on a Graph</em>.</p>
<p>To obtain convergence, a trivial bound on the variance suffices. Then</p>
<div class="math notranslate nohighlight">
\[
0 \leq \mathrm{Var}[f(X_{t})]
\leq \E[f(X_{t})^2]
\leq \|\mathbf{f}\|_\infty^2.
\]</div>
<p>Hence,</p>
<div class="math notranslate nohighlight">
\[
0 \leq 
\frac{1}{T^2} \sum_{t=1}^T \mathrm{Var}[f(X_{t})]
\leq \frac{T \|\mathbf{f}\|_\infty^2}{T^2} 
\to 0,
\]</div>
<p>as <span class="math notranslate nohighlight">\(T \to +\infty\)</span>.</p>
<p>Bounding the covariance requires a more delicate argument. Fix <span class="math notranslate nohighlight">\(1 \leq s &lt; t\leq T\)</span>. The trick is to condition on <span class="math notranslate nohighlight">\(X_s\)</span> and use the <em>Markov Property</em>. By definition of the covariance and the <em>Law of Total Expectation</em>,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\mathrm{Cov}[f(X_{s}),f(X_{t})]\\
&amp;= \E\left[(f(X_{s}) - \E[f(X_{s})]) (f(X_{t}) - \E[f(X_{t})])\right]\\
&amp;= \sum_{x} \E\left[(f(X_{s}) - \E[f(X_{s})]) (f(X_{t}) - \E[f(X_{t})])\,\middle|\,X_s = x\right]\,\P[X_s = x]\\
&amp;= \sum_{x} \E\left[f(X_{t}) - \E[f(X_{t})]\,\middle|\,X_s = x\right](f(x) - \E[f(X_{s})]) \,\P[X_s = x].
\end{align*}\]</div>
<p>We now use the time-homogeneity of the chain to note that</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\E\left[f(X_{t}) - \E[f(X_{t})]\,\middle|\,X_s = x\right]\\
&amp;= \E\left[f(X_{t})\,\middle|\,X_0 = x\right] - \E[f(X_{t})]\\
&amp;= \E\left[f(X_{t-s})\,\middle|\,X_0 = x\right] - \E[f(X_{t})].
\end{align*}\]</div>
<p>We now use the corollary. This expression in absolute value is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\left|\E\left[f(X_{t}) - \E[f(X_{t})]\,\middle|\,X_s = x\right]\right|\\
&amp;= \left|\E\left[f(X_{t-s})\,\middle|\,X_0 = x\right] - \E[f(X_{t})]\right|\\
&amp;= \left|(\E\left[f(X_{t-s})\,\middle|\,X_0 = x\right] - \bpi \mathbf{f})  - (\E[f(X_{t})] - \bpi \mathbf{f})\right|\\
&amp;\leq \left|\E\left[f(X_{t-s})\,\middle|\,X_0 = x\right] - \bpi \mathbf{f}\right|  + \left|\E[f(X_{t})] - \bpi \mathbf{f}\right|\\
&amp;\leq (1-\gamma_\star)^{t-s} \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty + (1-\gamma_\star)^t \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty.
\end{align*}\]</div>
<p>Plugging back above,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\left|\mathrm{Cov}[f(X_{s}),f(X_{t})]\right|\\
&amp;\leq \sum_{x} \left|\E\left[f(X_{t}) - \E[f(X_{t})]\,\middle|\,X_s = x\right]\right| \left|f(x) - \E[f(X_{s})]\right| \,\P[X_s = x]\\
&amp;\leq \sum_{x} ((1-\gamma_\star)^{t-s} \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty + (1-\gamma_\star)^t \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty)  \left|f(x) - \E[f(X_{s})]\right| \,\P[X_s = x]\\
&amp;\leq ((1-\gamma_\star)^{t-s} \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty + (1-\gamma_\star)^t \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty) \sum_{x}   2 \|\mathbf{f}\|_\infty\P[X_s = x]\\
&amp;\leq 4 (1-\gamma_\star)^{t-s} \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty^2,
\end{align*}\]</div>
<p>where we used that <span class="math notranslate nohighlight">\((1-\gamma_\star)^t \leq (1-\gamma_\star)^{t-s}\)</span> since <span class="math notranslate nohighlight">\((1-\gamma_\star) &lt; 1\)</span> and <span class="math notranslate nohighlight">\(t -s \leq t\)</span>.</p>
<p>Returning to the sum over the covariances, the previous bound gives</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\left|\frac{2}{T^2} \sum_{1 \leq s &lt; t\leq T} \mathrm{Cov}[f(X_{s}),f(X_{t})]\right|\\
&amp;\leq \frac{2}{T^2} \sum_{1 \leq s &lt; t\leq T} \left|\mathrm{Cov}[f(X_{s}),f(X_{t})]\right|\\
&amp;\leq \frac{2}{T^2} \sum_{1 \leq s &lt; t\leq T} 4 (1-\gamma_\star)^{t-s} \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty^2.
\end{align*}\]</div>
<p>To evaluate the sum we make the change of variable <span class="math notranslate nohighlight">\(h = t - s\)</span> to get that the previous expression is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\leq  4 \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty^2\frac{2}{T^2} \sum_{1 \leq s \leq T} \sum_{h=1}^{T-s} (1-\gamma_\star)^{h}\\
&amp;\leq  4 \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty^2\frac{2}{T^2} \sum_{1 \leq s \leq T} \sum_{h=0}^{+\infty} (1-\gamma_\star)^{h}\\
&amp;=  4 \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty^2\frac{2}{T^2} \sum_{1 \leq s \leq T} \frac{1}{\gamma_\star}\\
&amp;=  8 \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty^2 \gamma_\star^{-1} \frac{1}{T} \to 0,
\end{align*}\]</div>
<p>as <span class="math notranslate nohighlight">\(T \to +\infty\)</span>.</p>
<p>We have shown that</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathrm{Var}\left[\frac{1}{T} \sum_{t=1}^T f(X_{t})\right]
\leq \|\mathbf{f}\|_\infty^2 \frac{1}{T} + 8 \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty^2 \gamma_\star^{-1} \frac{1}{T} 
\leq 9 \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty^2 \gamma_\star^{-1} \frac{1}{T}. 
\end{align*}\]</div>
<p>For any <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\P\left[\left|\,\frac{1}{T} \sum_{t=1}^T f(X_{t}) - \bpi \mathbf{f}\,\right| \geq \varepsilon \right]\\
&amp;= \P\left[\left|\,\frac{1}{T} \sum_{t=1}^T f(X_{t}) - \E\left[\frac{1}{T} \sum_{t=1}^T f(X_{t})\right] + \left(\E\left[\frac{1}{T} \sum_{t=1}^T f(X_{t})\right] - \bpi \mathbf{f} \right)\,\right| \geq \varepsilon \right]\\
&amp;\leq \P\left[\left|\,\frac{1}{T} \sum_{t=1}^T f(X_{t}) - \E\left[\frac{1}{T} \sum_{t=1}^T f(X_{t})\right]\,\right| + \left|\,\E\left[\frac{1}{T} \sum_{t=1}^T f(X_{t})\right] - \bpi \mathbf{f} \,\right| \geq \varepsilon \right]\\
&amp;\leq \P\left[\left|\,\frac{1}{T} \sum_{t=1}^T f(X_{t}) - \E\left[\frac{1}{T} \sum_{t=1}^T f(X_{t})\right]\,\right|  \geq \varepsilon - \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty \gamma_\star^{-1}\frac{1}{T}\right].
\end{align*}\]</div>
<p>We can now apply <em>Chebyshev</em> to get</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\P\left[\left|\,\frac{1}{T} \sum_{t=1}^T f(X_{t}) - \bpi \mathbf{f}\,\right| \geq \varepsilon \right]
&amp;\leq \frac{9 \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty^2 \gamma_\star^{-1} \frac{1}{T}}{(\varepsilon - \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty \gamma_\star^{-1}\frac{1}{T})^2} \to 0,
\end{align*}\]</div>
<p>as <span class="math notranslate nohighlight">\(T \to +\infty\)</span>. <span class="math notranslate nohighlight">\(\square\)</span></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chap05_rwmc/06_adv"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../exercises/roch-mmids-rwmc-exercises.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">7.7. </span>Exercises</p>
      </div>
    </a>
    <a class="right-next"
       href="../../chap08_nn/00_intro/roch-mmids-nn-intro.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">8. </span>Neural networks, backpropagation and stochastic gradient descent</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#self-assessment-quizzes">7.8.1. Self-assessment quizzes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#just-the-code">7.8.2. Just the code</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#auto-quizzes">7.8.3. Auto-quizzes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#solutions-to-odd-numbered-warm-up-exercises">7.8.4. Solutions to odd-numbered warm-up exercises</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#proof-of-the-convergence-theorem">7.8.5. Proof of the convergence theorem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#random-walk-on-a-weighted-graph">7.8.6. Random walk on a weighted graph</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#spectral-techniques-for-random-walks-on-graphs">7.8.7. Spectral techniques for random walks on graphs</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Sebastien Roch
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>