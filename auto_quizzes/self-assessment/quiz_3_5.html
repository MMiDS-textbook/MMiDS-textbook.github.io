<!DOCTYPE html>
<html>

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MMiDS 3.5: Self-Assessment Quiz</title>
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@600&family=Open+Sans&display=swap"
        rel="stylesheet">
    <style>
        body {
            font-family: 'Arial', sans-serif;
            background-color: #f5f5f5;
            color: #333;
            margin: 0;
            padding: 0;
            display: flex;
            justify-content: center;
            align-items: center;
            min-height: 100vh;
        }

        #quiz-container {
            background-color: #fff;
            border-radius: 10px;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            padding: 30px;
            max-width: 800px;
            width: 100%;
            margin: 20px;
        }

        h1 {
            font-family: 'Montserrat', sans-serif;
            color: #673ab7;
            text-align: center;
            margin-bottom: 30px;
        }

        .question {
            background-color: #f5f0ff;
            border: 1px solid #d1c4e9;
            border-radius: 5px;
            padding: 20px;
            margin-bottom: 30px;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        }

        .question p {
            margin: 0 0 15px;
            font-weight: bold;
        }

        .options {
            list-style-type: none;
            padding: 0;
        }

        .options li {
            margin-bottom: 0px;
        }

        .options li label {
            display: block;
            padding: 12px;
            background-color: #f5f0ff;
            color: #333;
            border-radius: 5px;
            cursor: pointer;
            transition: background-color 0.3s;
        }

        .options li label:hover {
            background-color: #e0d6ff;
        }

        .feedback {
            margin-top: 15px;
            display: none;
            padding: 12px;
            border-radius: 5px;
        }

        .feedback.correct {
            background-color: #e0f0e5;
            color: #155724;
            display: block;
        }

        .feedback.incorrect {
            background-color: #f8e1e3;
            color: #721c24;
            display: block;
        }

        @media (max-width: 600px) {
            #quiz-container {
                padding: 20px;
            }

            h1 {
                font-size: 24px;
                margin-bottom: 20px;
            }

            .question {
                padding: 15px;
                margin-bottom: 20px;
            }
        }
    </style>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
    <div id="quiz-container">
        <h1>MMiDS 3.5: Self-Assessment Quiz</h1>
        <div class="question">
            <p>In gradient descent, the update rule for moving from \(x_t\) to \(x_{t+1}\) is given by:</p>
            <ul class="options">
                <li><label><input type="radio" name="q0" value="0"> a) \(x_{t+1} = x_t + \alpha_t \nabla f(x_t)\)</label></li>
                <li><label><input type="radio" name="q0" value="1"> b) \(x_{t+1} = x_t - \alpha_t \nabla f(x_t)\)</label></li>
                <li><label><input type="radio" name="q0" value="2"> c) \(x_{t+1} = x_t + \frac{\nabla f(x_t)}{\|\nabla f(x_t)\|}\)</label></li>
                <li><label><input type="radio" name="q0" value="3"> d) \(x_{t+1} = x_t - \frac{\nabla f(x_t)}{\|\nabla f(x_t)\|}\)</label></li>
            </ul>
            <div class="feedback" id="feedback0"></div>
        </div>

        <div class="question">
            <p>In the gradient descent update rule \(x_{t+1} = x_t - \alpha_t \nabla f(x_t)\), what does \(\alpha_t\) represent?</p>
            <ul class="options">
                <li><label><input type="radio" name="q1" value="0"> a) The gradient of \(f\) at \(x_t\)</label></li>
                <li><label><input type="radio" name="q1" value="1"> b) The step size or learning rate</label></li>
                <li><label><input type="radio" name="q1" value="2"> c) The direction of steepest ascent</label></li>
                <li><label><input type="radio" name="q1" value="3"> d) The Hessian matrix of \(f\) at \(x_t\)</label></li>
            </ul>
            <div class="feedback" id="feedback1"></div>
        </div>

        <div class="question">
            <p>A function \(f : \mathbb{R}^d \to \mathbb{R}\) is said to be \(L\)-smooth if:</p>
            <ul class="options">
                <li><label><input type="radio" name="q2" value="0"> a) \(\|\nabla f(x)\| \leq L\) for all \(x \in \mathbb{R}^d\)</label></li>
                <li><label><input type="radio" name="q2" value="1"> b) \(-LI_{d\times d} \preceq H_f(x) \preceq LI_{d\times d}\) for all \(x \in \mathbb{R}^d\)</label></li>
                <li><label><input type="radio" name="q2" value="2"> c) \(f(y) \leq f(x) + \nabla f(x)^T(y - x) + \frac{L}{2}\|y - x\|^2\) for all \(x, y \in \mathbb{R}^d\)</label></li>
                <li><label><input type="radio" name="q2" value="3"> d) Both b) and c)</label></li>
            </ul>
            <div class="feedback" id="feedback2"></div>
        </div>

        <div class="question">
            <p>What is the step size used in the convergence theorem for gradient descent in the smooth case?</p>
            <ul class="options">
                <li><label><input type="radio" name="q3" value="0"> a) \(\alpha_t = \frac{1}{L}\)</label></li>
                <li><label><input type="radio" name="q3" value="1"> b) \(\alpha_t = \frac{1}{t}\)</label></li>
                <li><label><input type="radio" name="q3" value="2"> c) \(\alpha_t = \frac{1}{L^2}\)</label></li>
                <li><label><input type="radio" name="q3" value="3"> d) \(\alpha_t = \frac{1}{\sqrt{t}}\)</label></li>
            </ul>
            <div class="feedback" id="feedback3"></div>
        </div>

        <div class="question">
            <p>Suppose \(f : \mathbb{R}^d \to \mathbb{R}\) is \(L\)-smooth and bounded from below. According to the "Convergence of Gradient Descent: Smooth Case" theorem, gradient descent with step size \(\alpha_t = \frac{1}{L}\) started from any \(x_0\) produces a sequence \(\{x_t\}\) such that</p>
            <ul class="options">
                <li><label><input type="radio" name="q4" value="0"> a) \(\lim_{t \to +\infty} f(x_t) = 0\)</label></li>
                <li><label><input type="radio" name="q4" value="1"> b) \(\lim_{t \to +\infty} \|\nabla f(x_t)\| = 0\)</label></li>
                <li><label><input type="radio" name="q4" value="2"> c) \(\min_{t=0,\ldots,S-1} \|\nabla f(x_t)\| \leq \sqrt{\frac{2L[f(x_0) - \bar{f}]}{S}}\) after \(S\) steps</label></li>
                <li><label><input type="radio" name="q4" value="3"> d) Both b) and c)</label></li>
            </ul>
            <div class="feedback" id="feedback4"></div>
        </div>

        <div class="question">
            <p>Suppose \(f : \mathbb{R}^d \to \mathbb{R}\) is \(L\)-smooth and \(m\)-strongly convex with a global minimizer at \(x^*\). According to the "Convergence of Gradient Descent: Strongly Convex Case" theorem, gradient descent with step size \(\alpha = \frac{1}{L}\) started from any \(x_0\) produces a sequence \(\{x_t\}\) such that after \(S\) steps:</p>
            <ul class="options">
                <li><label><input type="radio" name="q5" value="0"> a) \(f(x_S) - f(x^*) \leq \left(1 - \frac{m}{L}\right)^S[f(x_0) - f(x^*)]\)</label></li>
                <li><label><input type="radio" name="q5" value="1"> b) \(f(x_S) - f(x^*) \geq \left(1 - \frac{m}{L}\right)^S[f(x_0) - f(x^*)]\)</label></li>
                <li><label><input type="radio" name="q5" value="2"> c) \(f(x_S) - f(x^*) \leq \left(1 + \frac{m}{L}\right)^S[f(x_0) - f(x^*)]\)</label></li>
                <li><label><input type="radio" name="q5" value="3"> d) \(f(x_S) - f(x^*) \geq \left(1 + \frac{m}{L}\right)^S[f(x_0) - f(x^*)]\)</label></li>
            </ul>
            <div class="feedback" id="feedback5"></div>
        </div>

        <div class="question">
            <p>If a function \(f\) is \(m\)-strongly convex, what can we say about its global minimizer?</p>
            <ul class="options">
                <li><label><input type="radio" name="q6" value="0"> a) It may not exist.</label></li>
                <li><label><input type="radio" name="q6" value="1"> b) It exists and is unique.</label></li>
                <li><label><input type="radio" name="q6" value="2"> c) It exists but may not be unique.</label></li>
                <li><label><input type="radio" name="q6" value="3"> d) It always occurs at the origin.</label></li>
            </ul>
            <div class="feedback" id="feedback6"></div>
        </div>

        <div class="question">
            <p>What is the key property of strongly convex functions that allows us to establish a faster convergence rate for gradient descent compared to the smooth case?</p>
            <ul class="options">
                <li><label><input type="radio" name="q7" value="0"> a) Strong convexity implies smoothness.</label></li>
                <li><label><input type="radio" name="q7" value="1"> b) Strong convexity allows us to relate the function value at a point to the norm of the gradient at that point.</label></li>
                <li><label><input type="radio" name="q7" value="2"> c) Strong convexity guarantees the existence of a unique global minimum.</label></li>
                <li><label><input type="radio" name="q7" value="3"> d) Strong convexity ensures that the function is bounded from below.</label></li>
            </ul>
            <div class="feedback" id="feedback7"></div>
        </div>

        <div class="question">
            <p>What mathematical concept does the Descent Guarantee for Smooth Functions primarily rely on?</p>
            <ul class="options">
                <li><label><input type="radio" name="q8" value="0"> a) Taylor's Theorem</label></li>
                <li><label><input type="radio" name="q8" value="1"> b) Cauchy-Schwarz Inequality</label></li>
                <li><label><input type="radio" name="q8" value="2"> c) Mean Value Theorem</label></li>
                <li><label><input type="radio" name="q8" value="3"> d) Fundamental Theorem of Calculus</label></li>
            </ul>
            <div class="feedback" id="feedback8"></div>
        </div>
    </div>

    <script>
        document.querySelectorAll('.options').forEach((optionsList, index) => {
            const correctOptions = [1, 1, 3, 0, 3, 0, 1, 1, 0];

            optionsList.addEventListener('change', (event) => {
                const selectedOption = parseInt(event.target.value);
                const feedbackDiv = document.getElementById('feedback' + index);
                let feedbackText = '';

                if (selectedOption === correctOptions[index]) {
                    feedbackText = "Excellent! " + getCorrectFeedback(index);
                    feedbackDiv.classList.add('correct');
                    feedbackDiv.classList.remove('incorrect');
                } else {
                    feedbackText = "Try again.";
                    feedbackDiv.classList.add('incorrect');
                    feedbackDiv.classList.remove('correct');
                }

                feedbackDiv.innerHTML = feedbackText;

                MathJax.typesetPromise([feedbackDiv]);
            });
        });

        function getCorrectFeedback(index) {
            switch (index) {
                case 0:
                    return "The text defines the gradient descent update rule as \\(x_{t+1} = x_t - \\alpha_t \\nabla f(x_t)\\).";
                    
                case 1:
                    return "The text states 'At each iteration of gradient descent, we take a step in the direction of the negative of the gradient, that is, \\(x_{t+1} = x_t - \\alpha_t \\nabla f(x_t), t = 0, 1, 2, \\ldots\\) for a sequence of step sizes \\(\\alpha_t > 0\\).'";

                case 2:
                    return "The text provides both the definition in terms of the Hessian matrix (option b) and the equivalent characterization in terms of the quadratic bound (option c).";

                case 3:
                    return "The text specifies the step size as \\(\\alpha_t = \\alpha := \\frac{1}{L}\\) in the convergence theorem for the smooth case.";

                case 4:
                    return "The theorem states both the asymptotic convergence of the gradients to zero (option b) and the quantitative bound on the minimum gradient norm after \\(S\\) steps (option c).";

                case 5:
                    return "This is the convergence rate stated in the theorem.";

                case 6:
                    return "The text states that if \\(f\\) is \\(m\\)-strongly convex, then 'the global minimizer is unique.'";

                case 7:
                    return "The text emphasizes that strong convexity allows us to relate \\(f(x)\\) and \\(\\nabla f(x)\\) through the inequality \\(f(x) - f(x^*) \\le \\frac{\\|\\nabla f(x)\\|^2}{2m}\\), which is crucial for the faster convergence rate.";

                case 8:
                    return "The text mentions that the proof of the Descent Guarantee for Smooth Functions involves 'applying Taylor's Theorem, then bounding the second-order term.'";
                    
                default:
                    return "";
            }
        }
    </script>
</body>
</html>
