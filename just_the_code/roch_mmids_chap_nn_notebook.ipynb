{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c210e896",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell",
     "colab-keep"
    ]
   },
   "source": [
    "***\n",
    "\n",
    "*Course:* [Math 535](https://people.math.wisc.edu/~roch/mmids/) - Mathematical Methods in Data Science (MMiDS)  \n",
    "*Chapter:* 8-Neural networks, backpropagation and stochastic gradient descent   \n",
    "*Author:* [Sebastien Roch](https://people.math.wisc.edu/~roch/), Department of Mathematics, University of Wisconsin-Madison  \n",
    "*Updated:* July 16, 2024   \n",
    "*Copyright:* &copy; 2024 Sebastien Roch\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e36e2d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# You will need the files:\n",
    "#     * mmids.py\n",
    "#     * SAHeart.csv \n",
    "# from https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/tree/main/utils\n",
    "#\n",
    "# IF RUNNING ON GOOGLE COLAB (RECOMMENDED):\n",
    "# \"Upload to session storage\" from the Files tab on the left\n",
    "# Alternative instructions: https://colab.research.google.com/notebooks/io.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bf7892",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch\n",
    "import mmids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ab71d9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\newcommand{\\bmu}{\\boldsymbol{\\mu}}$ \n",
    "$\\newcommand{\\bSigma}{\\boldsymbol{\\Sigma}}$\n",
    "$\\newcommand{\\bfbeta}{\\boldsymbol{\\beta}}$ \n",
    "$\\newcommand{\\bflambda}{\\boldsymbol{\\lambda}}$\n",
    "$\\newcommand{\\bgamma}{\\boldsymbol{\\gamma}}$ \n",
    "$\\newcommand{\\bsigma}{{\\boldsymbol{\\sigma}}}$\n",
    "$\\newcommand{\\bpi}{\\boldsymbol{\\pi}}$\n",
    "$\\newcommand{\\btheta}{{\\boldsymbol{\\theta}}}$ \n",
    "$\\newcommand{\\bphi}{\\boldsymbol{\\phi}}$ \n",
    "$\\newcommand{\\balpha}{\\boldsymbol{\\alpha}}$\n",
    "$\\newcommand{\\blambda}{\\boldsymbol{\\lambda}}$\n",
    "$\\renewcommand{\\P}{\\mathbb{P}}$\n",
    "$\\newcommand{\\E}{\\mathbb{E}}$\n",
    "$\\newcommand{\\indep}{\\perp\\!\\!\\!\\perp} \\newcommand{\\bx}{\\mathbf{x}}$ \n",
    "$\\newcommand{\\bp}{\\mathbf{p}}$\n",
    "$\\renewcommand{\\bx}{\\mathbf{x}}$\n",
    "$\\newcommand{\\bX}{\\mathbf{X}}$ \n",
    "$\\newcommand{\\by}{\\mathbf{y}}$ \n",
    "$\\newcommand{\\bY}{\\mathbf{Y}}$ \n",
    "$\\newcommand{\\bz}{\\mathbf{z}}$\n",
    "$\\newcommand{\\bZ}{\\mathbf{Z}}$ \n",
    "$\\newcommand{\\bw}{\\mathbf{w}}$ \n",
    "$\\newcommand{\\bW}{\\mathbf{W}}$\n",
    "$\\newcommand{\\bv}{\\mathbf{v}}$ \n",
    "$\\newcommand{\\bV}{\\mathbf{V}}$\n",
    "$\\newcommand{\\bfg}{\\mathbf{g}}$ \n",
    "$\\newcommand{\\bfh}{\\mathbf{h}}$ \n",
    "$\\newcommand{\\horz}{\\rule[.5ex]{2.5ex}{0.5pt}}$\n",
    "$\\renewcommand{\\S}{\\mathcal{S}}$ \n",
    "$\\newcommand{\\X}{\\mathcal{X}}$ \n",
    "$\\newcommand{\\var}{\\mathrm{Var}}$ \n",
    "$\\newcommand{\\pa}{\\mathrm{pa}}$\n",
    "$\\newcommand{\\Z}{\\mathcal{Z}}$ \n",
    "$\\newcommand{\\bh}{\\mathbf{h}}$ \n",
    "$\\newcommand{\\bb}{\\mathbf{b}}$ \n",
    "$\\newcommand{\\bc}{\\mathbf{c}}$ \n",
    "$\\newcommand{\\cE}{\\mathcal{E}}$ \n",
    "$\\newcommand{\\cP}{\\mathcal{P}}$\n",
    "$\\newcommand{\\bbeta}{\\boldsymbol{\\beta}}$\n",
    "$\\newcommand{\\bLambda}{\\boldsymbol{\\Lambda}}$\n",
    "$\\newcommand{\\cov}{\\mathrm{Cov}}$\n",
    "$\\newcommand{\\bfk}{\\mathbf{k}}$\n",
    "$\\newcommand{\\idx}[1]{}$\n",
    "$\\newcommand{\\xdi}{}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76b2caa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Motivating example:  classifying natural images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8f9306",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "In this chapter, we return to the classification problem. This time we consider more complex datasets involving natural images. We have seen an example previously, the MNIST dataset. We use a related dataset known as Fashion-MNIST developed by the [Zalando Research](https://engineering.zalando.com/tags/zalando-research.html). Quoting from their [GitHub repository](https://github.com/zalandoresearch/fashion-mnist):\n",
    "\n",
    "> Fashion-MNIST is a dataset of Zalando's article images -- consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. We intend Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca79c983",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We first load the data and convert it to an appropriate matrix representation. The data can be accessed with [`torchvision.datasets.FashionMNIST`](https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58633523",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "fashion_mnist = datasets.FashionMNIST(root='./data', train=True, \n",
    "                                      download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af57df8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "For example, the first image and its label are the following. The [`squeeze()`](https://pytorch.org/docs/stable/generated/torch.Tensor.squeeze.html) below removes the color dimension in the image, which is grayscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed836110",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "img, label = fashion_mnist[0]\n",
    "plt.figure()\n",
    "plt.imshow(img.squeeze(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67a8f2b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7767ec",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def FashionMNIST_get_class_name(label):\n",
    "\n",
    "    class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \n",
    "    \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\n",
    "    return class_names[label]\n",
    "\n",
    "print(f\"{label}: '{FashionMNIST_get_class_name(label)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae23550f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The purpose of this chapter is to develop some of the mathematical tools used to solve this kind of classification problem:\n",
    "\n",
    "- neural networks,\n",
    "- backpropagation,\n",
    "- stochastic gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0429c6c1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Background: Jacobian, chain rule, and a brief introduction to automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4282901f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**Automatic differentiation in PyTorch** We will use [PyTorch](https://pytorch.org/tutorials/). It uses [tensors](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html)$\\idx{tensor}\\xdi$, which in many ways behave similarly to Numpy arrays. See [here](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html) for a quick introduction. We first initialize the tensors. Here each tensor corresponds to a single real variable. With the option [`requires_grad=True`](https://pytorch.org/docs/stable/generated/torch.Tensor.requires_grad.html#torch.Tensor.requires_grad), we indicate that these are variables with respect to which a gradient will be taken later. We initialize the tensors at the values where the derivatives will be computed. If derivatives need to be computed at different values, we need to repeat this process. The function [`.backward()`](https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html) computes the gradient using backpropagation, to which we will return later. The partial derivatives are accessed with [`.grad`](https://pytorch.org/docs/stable/generated/torch.Tensor.grad.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adeb746",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** This is better understood through an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95e3a1a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x1 = torch.tensor(1.0, requires_grad=True)\n",
    "x2 = torch.tensor(2.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9453fb9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We define the function. Note that we use\n",
    "[`torch.exp`](https://pytorch.org/docs/stable/generated/torch.exp.html), the PyTorch implementation of the (element-wise) exponential function. Moreover, as in NumPy, PyTorch allows the use of `**` for [taking a power](https://pytorch.org/docs/stable/generated/torch.pow.html). [Here](https://pytorch.org/docs/stable/name_inference.html) is a list of operations on tensors in PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26259ff",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "f = 3 * (x1 ** 2) + x2 + torch.exp(x1 * x2)\n",
    "\n",
    "f.backward()\n",
    "\n",
    "print(x1.grad)  # df/dx\n",
    "print(x2.grad)  # df/dy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe2c425",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The input parameters can also be vectors, which allows to consider functions of large numbers of variables. Here we use [`torch.sum`](https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum) for taking a sum of the arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5630c4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "z = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "\n",
    "g = torch.sum(z ** 2)\n",
    "g.backward()\n",
    "\n",
    "print(z.grad)  # gradient is (2 z_1, 2 z_2, 2 z_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6490e722",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Here is another typical example in a data science context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04fc69f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = torch.randn(3, 2)  # Random dataset (features)\n",
    "y = torch.tensor([[1., 0., 1.]])  # Dataset (labels)\n",
    "theta = torch.ones(2, 1, requires_grad=True)  # Parameter assignment\n",
    "\n",
    "predict = X @ theta  # Classifier with parameter vector theta\n",
    "loss = torch.sum((predict - y)**2)  # Loss function\n",
    "loss.backward()  # Compute gradients\n",
    "\n",
    "print(theta.grad)  # gradient of loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba97a08",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**CHAT & LEARN** Ask your favorite AI chatbot to explain how to compute a second derivative using PyTorch (it's bit tricky). Ask for code that you can apply to the previous examples. ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_nn_notebook.ipynb)) $\\ddagger$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26497615",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f86375f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**Implementing gradient descent in PyTorch** Rather than explicitly specifying the gradient function, we could use PyTorch to compute it automatically. This is done next. Note that the descent update is done within [`with torch.no_grad()`](https://pytorch.org/docs/stable/generated/torch.no_grad.html), which ensures that the update operation itself is not tracked for gradient computation. Here the input `x0` as well as the output `xk.numpy(force=True)` are Numpy arrays. The function [`torch.Tensor.numpy()`](https://pytorch.org/docs/stable/generated/torch.Tensor.numpy.html) converts a PyTorch tensor to a Numpy array (see the documentation for an explanation of the `force=True` option). Also, quoting ChatGPT:\n",
    "\n",
    "> In the given code, `.item()` is used to extract the scalar value from a tensor. In PyTorch, when you perform operations on tensors, you get back tensors as results, even if the result is a single scalar value. `.item()` is used to extract this scalar value from the tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7087241e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def gd_with_ad(f, x0, alpha=1e-3, niters=int(1e6)):\n",
    "    xk = torch.tensor(x0, requires_grad=True, dtype=torch.float)\n",
    "    \n",
    "    for _ in range(niters):\n",
    "        value = f(xk)\n",
    "        value.backward()\n",
    "\n",
    "        with torch.no_grad():  \n",
    "            xk -= alpha * xk.grad\n",
    "\n",
    "        xk.grad.zero_()\n",
    "\n",
    "    return xk.numpy(force=True), f(xk).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cc915c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We revisit a previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbb1b49",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**3\n",
    "\n",
    "print(gd_with_ad(f, 2, niters=int(1e4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd98701",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(gd_with_ad(f, -2, niters=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f429bd",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c21b72",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Building blocks of AI 1: backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caf9a93",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** To make things more concrete, we consider a specific example. We will use [`torch.linalg.vector_norm`](https://pytorch.org/docs/stable/generated/torch.linalg.vector_norm.html) to compute the Euclidean norm in PyTorch. Suppose $d=3$, $L=1$, $n_1 = 2$, and $K = 2$ with the following choices: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399d90ae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([1.,0.,-1.], requires_grad=True)\n",
    "y = torch.tensor([0.,1.])\n",
    "W0 = torch.tensor([[0.,1.,-1.],[2.,0.,1.]])\n",
    "W1 = torch.tensor([[-1.,0.],[2.,-1.]])\n",
    "\n",
    "z0 = x\n",
    "z1 = W0 @ z0\n",
    "z2 = W1 @ z1\n",
    "f = 0.5 * (torch.linalg.vector_norm(y-z2) ** 2)\n",
    "\n",
    "print(z0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327db4b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a644f4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560423f0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530f9be9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a738cee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We return to our concrete example. Using `.T` to convert a column vector into a row vector throws an error in PyTorch, as it is meant to be used only on 2D tensors. Instead, one can use [`torch.unsqueeze`](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html). Below, `(z2 - y).unsqueeze(0)` adds a dimension to `z2 - y`, making it a 2D tensor with shape $(1, 2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ad3593",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    F0 = W0\n",
    "    F1 = W1 @ F0\n",
    "    grad_f = (z2 - y).unsqueeze(0) @ F1\n",
    "    \n",
    "print(F0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d0502c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "print(F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9822ba54",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(grad_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32274a6f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We can check that we get the same outcome using AD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa283aa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "f.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ae6a60",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37be9fd8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We try our specific example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a06d180",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    G2 = (z2 - y).unsqueeze(0)\n",
    "    G1 = G2 @ W1\n",
    "    grad_f = G1 @ W0\n",
    "    \n",
    "print(G2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f366084c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "print(G1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7789c0ba",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "print(grad_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d54a72",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We indeed obtain the same answer yet again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce1024c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29497c9b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We return to the concrete example from the previous subsection. This time the matrices `W0` and `W1` require partial derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af476d7f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([1.,0.,-1.])\n",
    "y = torch.tensor([0.,1.])\n",
    "W0 = torch.tensor([[0.,1.,-1.],[2.,0.,1.]], requires_grad=True)\n",
    "W1 = torch.tensor([[-1.,0.],[2.,-1.]], requires_grad=True)\n",
    "\n",
    "z0 = x\n",
    "z1 = W0 @ z0\n",
    "z2 = W1 @ z1\n",
    "f = 0.5 * (torch.linalg.vector_norm(y-z2) ** 2)\n",
    "\n",
    "print(z0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be64ca8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80045c61",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "print(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ca5877",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb54bb58",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We compute the gradient $\\nabla f(\\mathbf{w})$ using AD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668597f8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "f.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82440a41",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "print(W0.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e18dc94",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "print(W1.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caf9bcb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "These are written in the form of matrix derivatives\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial \\mathcal{W}_0}\n",
    "= \\begin{pmatrix}\n",
    "\\frac{\\partial f}{\\partial w_0} &\n",
    "\\frac{\\partial f}{\\partial w_1} &\n",
    "\\frac{\\partial f}{\\partial w_2} \\\\\n",
    "\\frac{\\partial f}{\\partial w_3} &\n",
    "\\frac{\\partial f}{\\partial w_4} &\n",
    "\\frac{\\partial f}{\\partial w_5}\n",
    "\\end{pmatrix}\n",
    "\\quad\\text{and}\\quad\n",
    "\\frac{\\partial f}{\\partial \\mathcal{W}_1}\n",
    "= \\begin{pmatrix}\n",
    "\\frac{\\partial f}{\\partial w_6} &\n",
    "\\frac{\\partial f}{\\partial w_7} \\\\\n",
    "\\frac{\\partial f}{\\partial w_8} &\n",
    "\\frac{\\partial f}{\\partial w_9}\n",
    "\\end{pmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cd20be",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We use our formulas to confirm that they match these results. We need the Kronecker product, which in PyTorch is implemented as [`torch.kron`](https://pytorch.org/docs/stable/generated/torch.kron.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e3a635",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    grad_W0 = torch.kron((z2 - y).unsqueeze(0) @ W1, z0.unsqueeze(0))\n",
    "    grad_W1 = torch.kron((z2 - y).unsqueeze(0), z1.unsqueeze(0))\n",
    "\n",
    "print(grad_W0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d5a078",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "print(grad_W1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf97848",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Observe that this time these results are written in vectorized form (i.e., obtained by concatenating the rows). But they do match with the AD output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0a514c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6e713d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Building blocks of AI 2: stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785c409e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "For the mini-batch version of SGD, we pick a random sub-sample $\\mathcal{B}_t \\subseteq \\{1,\\ldots,n\\}$ of size $B$ and take the step\n",
    "\n",
    "$$\n",
    "\\mathbf{x}^{t+1}\n",
    "= \\mathbf{x}^{t} +\\beta \\frac{1}{B} \\sum_{i\\in \\mathcal{B}_t} (\n",
    "b_i - \\sigma(\\boldsymbol{\\alpha}_i^T \\mathbf{x}^t) \n",
    ") \\,\\boldsymbol{\\alpha}_i.\n",
    "$$\n",
    "\n",
    "We modify our previous code for logistic regression. The only change is to pick a random mini-batch which can be fed to the descent update sub-routine as dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f72ac6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "outputs": [],
   "source": [
    "def sigmoid(z): \n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def pred_fn(x, A): \n",
    "    return sigmoid(A @ x)\n",
    "\n",
    "def loss_fn(x, A, b): \n",
    "    return np.mean(-b*np.log(pred_fn(x, A)) - (1 - b)*np.log(1 - pred_fn(x, A)))\n",
    "\n",
    "def grad_fn(x, A, b):\n",
    "    return -A.T @ (b - pred_fn(x, A))/len(b)\n",
    "\n",
    "def desc_update_for_logreg(grad_fn, A, b, curr_x, beta):\n",
    "    gradient = grad_fn(curr_x, A, b)\n",
    "    return curr_x - beta*gradient\n",
    "\n",
    "def sgd_for_logreg(rng, loss_fn, grad_fn, A, b, \n",
    "                   init_x, beta=1e-3, niters=int(1e5), batch=40):\n",
    "    \n",
    "    curr_x = init_x\n",
    "    nsamples = len(b)\n",
    "    for _ in range(niters):\n",
    "        I = rng.integers(nsamples, size=batch)\n",
    "        curr_x = desc_update_for_logreg(\n",
    "            grad_fn, A[I,:], b[I], curr_x, beta)\n",
    "    \n",
    "    return curr_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be0411f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We analyze a dataset from [[ESL](https://web.stanford.edu/~hastie/ElemStatLearn/)], which can be downloaded [here](https://web.stanford.edu/~hastie/ElemStatLearn/data.html). Quoting [[ESL](https://web.stanford.edu/~hastie/ElemStatLearn/), Section 4.4.2] \n",
    "\n",
    "> The data [...] are a subset of the Coronary Risk-Factor Study (CORIS) baseline survey, carried out in three rural areas of the Western Cape, South Africa (Rousseauw et al., 1983). The aim of the study was to establish the intensity of ischemic heart disease risk factors in that high-incidence region. The data represent white males between 15 and 64, and the response variable is the presence or absence of myocardial infarction (MI) at the time of the survey (the overall prevalence of MI was 5.1% in this region). There are 160 cases in our data set, and a sample of 302 controls. These data are described in more detail in Hastie and Tibshirani (1987).\n",
    "\n",
    "We load the data, which we slightly reformatted and look at a summary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cb0a8b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('SAHeart.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b050772",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Our goal to predict `chd`, which stands for coronary heart disease, based on the other variables (which are briefly described [here](https://web.stanford.edu/~hastie/ElemStatLearn/datasets/SAheart.info.txt)). We use logistic regression again. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c643803d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We first construct the data matrices. We only use three of the predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178538b2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "feature = data[['tobacco', 'ldl', 'age']].to_numpy()\n",
    "print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cd274b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "label = data['chd'].to_numpy()\n",
    "A = np.concatenate((np.ones((len(label),1)),feature),axis=1)\n",
    "b = label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c511008d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We try mini-batch SGD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2731679d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "seed = 535\n",
    "rng = np.random.default_rng(seed)\n",
    "init_x = np.zeros(A.shape[1])\n",
    "best_x = sgd_for_logreg(rng, loss_fn, grad_fn, A, b, init_x, beta=1e-3, niters=int(1e6))\n",
    "print(best_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af711d4a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The outcome is harder to vizualize. To get a sense of how accurate the result is, we compare our predictions to the true labels. By prediction, let us say that we mean that we predict label $1$ whenever $\\sigma(\\boldsymbol{\\alpha}^T \\mathbf{x}) > 1/2$. We try this on the training set. (A better approach would be to split the data into training and testing sets, but we will not do this here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edde4da6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def logis_acc(x, A, b):\n",
    "    return np.sum((pred_fn(x, A) > 0.5) == b)/len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b613b3a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "logis_acc(best_x, A, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6861dcfd",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d3c952",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We will use the Fashion MNIST dataset. This example is inspired by [these](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html) [tutorials](https://www.tensorflow.org/tutorials/keras/classification). We first check for the availability of GPUs and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a0f1b9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                      else (\"mps\" if torch.backends.mps.is_available() \n",
    "                            else \"cpu\"))\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30dd8ab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "if device.type == 'cuda': # device-specific seeding and settings\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # for multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "elif device.type == 'mps':\n",
    "    torch.mps.manual_seed(seed)  # MPS-specific seeding\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, \n",
    "                               download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, \n",
    "                              download=True, transform=transforms.ToTensor())\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, generator=g)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad21f74",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We used [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader), which provides utilities to load the data in batches for training. We took mini-batches of size `BATCH_SIZE = 32` and we apply a random permutation of the samples on every pass over the training data (with the option `shuffle=True`). The function [`torch.manual_seed()`](https://pytorch.org/docs/stable/generated/torch.manual_seed.html) is used to set the global seed for PyTorch operations (e.g., weight initialization). The shuffling in `DataLoader` uses its own separate random number generator, which we initialize with [`torch.Generator()`](https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator) and [`manual_seed()`](https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator.manual_seed). (You can tell from the fact that `seed=42` that Claude explained that one to me...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e78eb7",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**CHAT & LEARN** Ask your favorite AI chatbot to explain the lines:\n",
    "\n",
    "```\n",
    "if device.type == 'cuda': # device-specific seeding and settings\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # for multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "elif device.type == 'mps':\n",
    "    torch.mps.manual_seed(seed)  # MPS-specific seeding\n",
    "```\n",
    "\n",
    "$\\ddagger$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d7e470",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We implement multinomial logistic regression to learn a classifier for the MNIST data. In PyTorch, composition of functions can be achieved with [`torch.nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html). Our model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392e91ed",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28 * 28, 10)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7958ed",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The [`torch.nn.Flatten`](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html) layer turns each input image into a vector of size $784$ (where $784 = 28^2$ is the number of pixels in each image). After the flattening, we have an affine map from $\\mathbb{R}^{784}$ to $\\mathbb{R}^{10}$. Note that there is no need to pre-process the inputs by adding $1$s. A constant term (or \"bias variable\") is automatically added by PyTorch (unless one chooses the option [`bias=False`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)). The final output is $10$-dimensional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27050a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Finally, we are ready to run an optimization method of our choice on the loss function, which are specified next. There are many [optimizers](https://pytorch.org/docs/stable/optim.html#algorithms) available. (See this [post](https://hackernoon.com/demystifying-different-variants-of-gradient-descent-optimization-algorithm-19ae9ba2e9bc) for a brief explanation of many common optimizers.) Here we use SGD as the optimizer. A quick tutorial is [here](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html). The loss function is the [cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy), as implemented by [`torch.nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html), which first takes the softmax and expects the labels to be class names rather than their one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf579a20",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abebee9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We implement special functions for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a20ab40",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)    \n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def training_loop(train_loader, model, loss_fn, optimizer, device, epochs=3):\n",
    "    for epoch in range(epochs):\n",
    "        train(train_loader, model, loss_fn, optimizer, device)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba3b443",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "An epoch is one training iteration where all samples are iterated once (in a randomly shuffled order). In the interest of time, we train for 10 epochs only. But it does better if you train it longer (try it!). On each pass, we compute the output of the current model, use `backward()` to obtain the gradient, and then perform a descent update with `step()`. We also have to reset the gradients first (otherwise they add up by default). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ce91ce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "training_loop(train_loader, model, loss_fn, optimizer, device, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd3848c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Because of the issue of [overfitting](https://en.wikipedia.org/wiki/Overfitting), we use the *test* images to assess the performance of the final classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3331ae40",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    correct = 0    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            correct += (pred.argmax(dim=1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    print(f\"Test error: {(100*(correct / size)):>0.1f}% accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62cdd7a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "test(test_loader, model, loss_fn, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3378fdf0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To make a prediction, we take a [`torch.nn.functional.softmax`](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html) of the output of our model. Recall that it is implicitly included in `torch.nn.CrossEntropyLoss`, but is not actually part of `model`. (Note that the softmax itself has no parameter.) \n",
    "\n",
    "As an illustration, we do this for each test image. We use [`torch.cat`](https://pytorch.org/docs/stable/generated/torch.cat.html) to concatenate a sequence of tensors into a single tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b135c76",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def predict_softmax(dataloader, model, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            probabilities = F.softmax(pred, dim=1)\n",
    "            predictions.append(probabilities.cpu())\n",
    "            \n",
    "    return torch.cat(predictions, dim=0)\n",
    "\n",
    "predictions = predict_softmax(test_loader, model, device).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a43314d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The result for the first test image is shown below. To make a prediction, we choose the label with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02299eb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a7e7b9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "predictions[0].argmax(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb5e2a6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The truth is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b421d3b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "images, labels = next(iter(test_loader))\n",
    "images = images.squeeze().numpy()\n",
    "labels = labels.numpy()\n",
    "\n",
    "print(f\"{labels[0]}: '{mmids.FashionMNIST_get_class_name(labels[0])}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06caea51",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Above, `next(iter(test_loader))` loads the first batch of test images. (See [here](https://docs.python.org/3/tutorial/classes.html#iterators) for background on iterators in Python.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6333857d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2473f242",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Building blocks of AI 3: neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52243361",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We return to the concrete example from the previous section. We re-write the gradient as\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla f(\\mathbf{w})^T\n",
    "&= \\begin{pmatrix}\n",
    "[(\\mathbf{z}_2 - \\mathbf{y})^T\n",
    "\\mathcal{W}_{1} \\mathrm{diag}(\\mathbf{z}_1 \\odot (\\mathbf{1} - \\mathbf{z}_1))] \\otimes \\mathbf{z}_0^T &\n",
    "(\\mathbf{z}_2 - \\mathbf{y})^T\n",
    "\\otimes \\mathbf{z}_1^T\n",
    "\\end{pmatrix}.\n",
    "\\end{align*}\n",
    "\n",
    "We will use [`torch.nn.functional.sigmoid`](https://pytorch.org/docs/stable/generated/torch.nn.functional.sigmoid.html) and\n",
    "[`torch.nn.functional.softmax`](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html) for the sigmoid and softmax functions respectively. We also use [`torch.dot`](https://pytorch.org/docs/stable/generated/torch.dot.html) for the inner product (i.e., dot product) of two vectors (as tensors) and [`torch.diag`](https://pytorch.org/docs/stable/generated/torch.diag.html) for the creation of a diagonal matrix with specified entries on its diagonal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50f4f04",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "x = torch.tensor([1.,0.,-1.])\n",
    "y = torch.tensor([0.,1.])\n",
    "W0 = torch.tensor([[0.,1.,-1.],[2.,0.,1.]], requires_grad=True)\n",
    "W1 = torch.tensor([[-1.,0.],[2.,-1.]], requires_grad=True)\n",
    "\n",
    "z0 = x\n",
    "z1 = F.sigmoid(W0 @ z0)\n",
    "z2 = F.softmax(W1 @ z1, dim=0)\n",
    "f = -torch.dot(torch.log(z2), y)\n",
    "\n",
    "print(z0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd99f9a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a01de24",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3743ed9d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402154aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We compute the gradient $\\nabla f(\\mathbf{w})$ using AD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42336267",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "f.backward()\n",
    "print(W0.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5decfea9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "print(W1.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828b6494",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We use our formulas to confirm that they match these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51302c14",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    grad_W0 = torch.kron((z2 - y).unsqueeze(0) @ W1 @ torch.diag(z1 * (1-z1)), z0.unsqueeze(0))\n",
    "    grad_W1 = torch.kron((z2 - y).unsqueeze(0), z1.unsqueeze(0))\n",
    "\n",
    "print(grad_W0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38859da0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "print(grad_W1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21def0db",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The results match with the AD output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b0b0bc",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a6b332",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We implement the training of a neural network in PyTorch. We use the Fashion MNIST dataset again. We first load it again. We also check for the availability of GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431340ef",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() \n",
    "                      else ('mps' if torch.backends.mps.is_available() \n",
    "                            else 'cpu'))\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfe7a8e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "if device.type == 'cuda': # device-specific seeding and settings\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # for multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "elif device.type == 'mps':\n",
    "    torch.mps.manual_seed(seed)  # MPS-specific seeding\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, \n",
    "                               download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, \n",
    "                              download=True, transform=transforms.ToTensor())\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, generator=g)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f9c772",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We construct a two-layer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5194fb5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Flatten(),                      # Flatten the input\n",
    "    nn.Linear(28 * 28, 32),            # First Linear layer with 32 nodes\n",
    "    nn.Sigmoid(),                      # Sigmoid activation function\n",
    "    nn.Linear(32, 10)                  # Second Linear layer with 10 nodes (output layer)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f01f37",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "As we did for multinomial logistic regression, we use the Adam optimizer and the cross-entropy loss (which in PyTorch includes the softmax function and expects labels to be class names rather than one-hot encoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaec83e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d545909",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We train for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0754ca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "mmids.training_loop(train_loader, model, loss_fn, optimizer, device, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685800c9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "On the test data, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c269a2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mmids.test(test_loader, model, loss_fn, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c72270",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Disappointingly, this is significantly less accurate model than what we obtained using multinomial logistic regression. It turns out that using a different optimizer gives much better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880a54c9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.Adam(model.parameters())\n",
    "mmids.training_loop(train_loader, model, loss_fn, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc6f8c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "mmids.test(test_loader, model, loss_fn, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53308c46",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**CHAT & LEARN** The text mentions that there are many optimizers available in PyTorch besides SGD. Ask your favorite AI chatbot to explain and implement a different optimizer, such as Adam, Adagrad or RMSprop, for the linear regression problem. Compare the results with those obtained using SGD. $\\ddagger$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c806c8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
