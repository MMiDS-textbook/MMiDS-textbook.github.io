{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b8cd87c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell",
     "colab-keep"
    ]
   },
   "source": [
    "***\n",
    "\n",
    "*Course:* [Math 535](https://people.math.wisc.edu/~roch/mmids/) - Mathematical Methods in Data Science (MMiDS)  \n",
    "*Chapter:* 7-Probabilistic models   \n",
    "*Author:* [Sebastien Roch](https://people.math.wisc.edu/~roch/), Department of Mathematics, University of Wisconsin-Madison  \n",
    "*Updated:* Jan 22, 2024   \n",
    "*Copyright:* &copy; 2024 Sebastien Roch\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09deaf7",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# You will need the files:\n",
    "#     * mmids.py\n",
    "# from https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/tree/main/utils\n",
    "#\n",
    "# IF RUNNING ON GOOGLE COLAB (RECOMMENDED):\n",
    "# \"Upload to session storage\" from the Files tab on the left\n",
    "# Alternative instructions: https://colab.research.google.com/notebooks/io.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104f066c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# PYTHON 3\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import mmids\n",
    "seed = 535\n",
    "rng = np.random.default_rng(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57adf933",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Motivating example: tracking location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852b8ed3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Suppose we let loose a cyborg corgi in a large park. We would like to know where it is at all time. For this purpose, it has an implanted location device that sends a signal to a tracking app.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d282a1f3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Here is an example of the data we might have. The red dots are recorded locations at reguler time intervals. The dotted line helps keep track of the time order of the recordings. (We will explain later how this dataset is generated.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5010209",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "ss = 4\n",
    "os = 2\n",
    "F = np.array([[1., 0., 1., 0.],[0., 1., 0., 1.],[0., 0., 1., 0.],[0., 0., 0., 1.]]) \n",
    "H = np.array([[1., 0., 0., 0.],[0., 1, 0., 0.]])\n",
    "Q = 0.1 * np.diag(np.ones(ss))\n",
    "R = 10 * np.diag(np.ones(os))\n",
    "x_0 = np.array([0., 0., 1., 1.])\n",
    "T = 50\n",
    "x, y = mmids.lgSamplePath(ss, os, F, H, Q, R, x_0, T)\n",
    "plt.plot(y[0,:], y[1,:], marker='o', c='r', linestyle='dotted')\n",
    "plt.xlim((np.min(y[0,:])-5, np.max(y[0,:])+5)) \n",
    "plt.ylim((np.min(y[1,:])-5, np.max(y[1,:])+5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d95df3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "By convention, we start at $(0,0)$. Notice how squiggly the trajectory is. One issue might be that the times  at which the location is recorded are too far between. But, in fact, there is another issue: the tracking device is *inaccurate*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b819fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To get a better estimate of the true trajectory, it is natural to try to model the noise in the measurement as well as the dynamics itself. Probabilistic models are perfectly suite for this. \n",
    "\n",
    "In this chapter, we will encounter of variety of such models and show how to take advantage of them to estimate unknown states (or parameters). In particular, conditional independence will play a key role.\n",
    "\n",
    "We will come back to location tracking later in the chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c26f6cc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\newcommand{\\P}{\\mathbb{P}}$ $\\newcommand{\\E}{\\mathbb{E}}$ $\\newcommand{\\S}{\\mathcal{S}}$ $\\newcommand{\\X}{\\mathcal{X}}$ $\\newcommand{\\var}{\\mathrm{Var}}$ $\\newcommand{\\btheta}{\\boldsymbol{\\theta}}$ $\\newcommand{\\bbeta}{\\boldsymbol{\\beta}}$ $\\newcommand{\\bphi}{\\boldsymbol{\\phi}}$ $\\newcommand{\\bpi}{\\boldsymbol{\\pi}}$ $\\newcommand{\\bmu}{\\boldsymbol{\\mu}}$ $\\newcommand{\\bSigma}{\\boldsymbol{\\Sigma}}$ $\\newcommand{\\balpha}{\\boldsymbol{\\alpha}}$ $\\newcommand{\\indep}{\\perp\\!\\!\\!\\perp}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b19729",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Background: introduction to parametric families and maximum likelihood estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30fced7",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER** In Numpy, the module [numpy.random](https://numpy.org/doc/stable/reference/random/index.html) provides a way to sample from a variety of standard distributions. We first initialize the [pseudorandom number generator](https://en.wikipedia.org/wiki/Pseudorandom_number_generator) with a [random seed](https://en.wikipedia.org/wiki/Random_seed). In particular it allows the results to be reproducible: using the same seed produces the same results again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e166ae",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "seed = 535\n",
    "rng = np.random.default_rng(535)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a96874",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We then set the distribution and its parameters. Here's are lists of available [probability distributions](https://numpy.org/doc/stable/reference/random/generator.html#distributions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc76de97",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "p = 0.1 # probability of success\n",
    "N = 5 # number of samples\n",
    "rng.binomial(1,p,size=N) # Bernoulli is special case of binomial with 1 trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad440a33",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Here are a few other examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0b8d62",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "p = [0.1, 0.2, 0.7]\n",
    "n = 100\n",
    "rng.multinomial(n,p,size=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf6d402",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "mu = np.array([0.1, -0.3])\n",
    "sig = np.array([[2., 0.],[0., 3.]])\n",
    "rng.multivariate_normal(mu,sig,size=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23104e1a",
   "metadata": {
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71f1588",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\newcommand{\\P}{\\mathbb{P}}$ $\\newcommand{\\E}{\\mathbb{E}}$ $\\newcommand{\\S}{\\mathcal{S}}$ $\\newcommand{\\var}{\\mathrm{Var}}$ $\\newcommand{\\btheta}{\\boldsymbol{\\theta}}$ $\\newcommand{\\bphi}{\\boldsymbol{\\phi}}$ $\\newcommand{\\bpi}{\\boldsymbol{\\pi}}$ $\\newcommand{\\bmu}{\\boldsymbol{\\mu}}$ $\\newcommand{\\blambda}{\\boldsymbol{\\lambda}}$ $\\newcommand{\\bSigma}{\\boldsymbol{\\Sigma}}$ $\\newcommand{\\balpha}{\\boldsymbol{\\alpha}}$ $\\newcommand{\\indep}{\\perp\\!\\!\\!\\perp}$ $\\newcommand{\\bp}{\\mathbf{p}}$ $\\newcommand{\\bx}{\\mathbf{x}}$ $\\newcommand{\\bX}{\\mathbf{X}}$ $\\newcommand{\\by}{\\mathbf{y}}$ $\\newcommand{\\bY}{\\mathbf{Y}}$ $\\newcommand{\\bz}{\\mathbf{z}}$ $\\newcommand{\\bZ}{\\mathbf{Z}}$ $\\newcommand{\\bw}{\\mathbf{w}}$ $\\newcommand{\\bW}{\\mathbf{W}}$ $\\newcommand{\\pa}{\\mathrm{pa}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d45141",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Modeling correlated random variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29946f4",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER** In Numpy, the module [numpy.random](https://numpy.org/doc/stable/reference/random/index.html) also provides a way to sample from mixture models by using [numpy.random.Generator.choice](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.choice.html). This is better seen on an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6566fab7",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "seed = 535 # setting the seed\n",
    "rng = np.random.default_rng(seed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f9966a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# constructs a mixture of three normal distributions,\n",
    "# with prior probabilities [0.2, 0.5, 0.3]\n",
    "mus = np.array([-2.0, 0.0, 3.0])\n",
    "sigmas = np.array([1.2, 1.0, 2.5])\n",
    "coeffs = np.array([0.2, 0.5, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db83e446",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "N = 5 # number of samples\n",
    "which = rng.choice(3,N,p=coeffs)\n",
    "rng.normal(mus[which],sigmas[which])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aee867",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ccfb00",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER** We implement the Naive Bayes model. We use [Laplace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing) to avoid overfitting issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ea5b5f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We use a simple example from [Towards Data Science](https://towardsdatascience.com/all-about-naive-bayes-8e13cef044cf):\n",
    "\n",
    "> **Example:** let’s say we have data on 1000 pieces of fruit. The fruit being a Banana, Orange or some other fruit and imagine we know 3 features of each fruit, whether it’s long or not, sweet or not and yellow or not, as displayed in the table below.\n",
    "\n",
    "![Table](https://miro.medium.com/max/1066/1*B_uXqox7nHfwTa1HH4Fc_A.png)\n",
    "\n",
    "> [...] Which should provide enough evidence to predict the class of another fruit as it’s introduced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1935b06",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We encode the data into a table, where the rows are the classes and the columns are the features. The entries are the corresponding $N_{km}$'s. In addition we provide the vector $N_k$, which is the last column above, and the value $N$, which is the sum of the entries of $N_k$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28459e64",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "N_km = np.array([[400., 350., 450.],[0., 150., 300.],[100., 150., 50.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125a3308",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def nb_fit_table(N_km, alpha=1., beta=1.):\n",
    "    \n",
    "    K, M = N_km.shape\n",
    "    N_k = np.sum(N_km,axis=-1)\n",
    "    N = np.sum(N_k)\n",
    "    \n",
    "    # MLE for pi_k's\n",
    "    pi_k = (N_k+alpha) / (N+K*alpha)\n",
    "    \n",
    "    # MLE for p_km's\n",
    "    p_km = (N_km+beta) / (N_k[:,None]+2*beta)\n",
    "\n",
    "    return pi_k, p_km"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16424d15",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We run it on our simple dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd29b08",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pi_k, p_km = nb_fit_table(N_km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb8ceab",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(pi_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edc489c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(p_km)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ae9138",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Continuing on with our previous example:\n",
    "\n",
    "> So let’s say we’re given the features of a piece of fruit and we need to predict the class. If we’re told that the additional fruit is Long, Sweet and Yellow, we can classify it using the [prediction] formula and subbing in the values for each outcome, whether it’s a Banana, an Orange or Other Fruit. The one with the highest probability (score) being the winner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c90911a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The next function computes the negative logarithm of $\\pi_k \\prod_{m=1}^M p_{km}^{x_m} (1-p_{km})^{1-x_m}$, that is, the score of $k$, and outputs a $k$ achieving the minimum score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebd8b70",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def nb_predict(pi_k, p_km, x, label_set):\n",
    "   \n",
    "    K = len(pi_k)\n",
    "    \n",
    "    # Computing the score for each k\n",
    "    score_k = np.zeros(K)\n",
    "    for k in range(K):\n",
    "       \n",
    "        score_k[k] += - np.log(pi_k[k])\n",
    "        score_k[k] += - np.sum(x * np.log(p_km[k,:]) + (1 - x)*np.log(1 - p_km[k,:]))\n",
    "    \n",
    "    # Computing the minimum\n",
    "    argmin = np.argmin(score_k, axis=0)\n",
    "    minscr = np.max(score_k, axis=0)\n",
    "\n",
    "    return label_set[argmin]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b722c9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We run it on our dataset with the additional fruit from the quote above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fb8df6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "label_set = ['Banana', 'Orange', 'Other']\n",
    "x = np.array([1., 1., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a734d412",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nb_predict(pi_k, p_km, x, label_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d8df87",
   "metadata": {
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8bde62",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER** We implement the EM algorithm for mixtures of multivariate Bernoullis. For this purpose, we adapt our previous Naive Bayes routines. We also allow for the possibility of using Laplace smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72df7ac5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def responsibility(pi_k, p_km, x):\n",
    "   \n",
    "    K = len(pi_k)\n",
    "        \n",
    "    # Computing the score for each k\n",
    "    score_k = np.zeros(K)\n",
    "    for k in range(K):\n",
    "       \n",
    "        score_k[k] += - np.log(pi_k[k])\n",
    "        score_k[k] += - np.sum(x*np.log(p_km[k,:]) + (1 - x)*np.log(1 - p_km[k,:]))\n",
    "    \n",
    "    # Computing responsibilities for each k\n",
    "    r_k = np.exp(-score_k)/(np.sum(np.exp(-score_k)))\n",
    "        \n",
    "    return r_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0139b243",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def update_parameters(eta_km, eta_k, eta, alpha, beta):\n",
    "\n",
    "    K = len(eta_k)\n",
    "    \n",
    "    # MLE for pi_k's\n",
    "    pi_k = (eta_k+alpha) / (eta+K*alpha)\n",
    "    \n",
    "    # MLE for p_km's\n",
    "    p_km = (eta_km+beta) / (eta_k[:,None]+2*beta)\n",
    "\n",
    "    return pi_k, p_km"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8859f4b6",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We implement the E and M Step next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52191a1c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def em_bern(X, K, pi_0, p_0, maxiters = 10, alpha=0., beta=0.):\n",
    "    \n",
    "    n, M = X.shape\n",
    "    pi_k = pi_0\n",
    "    p_km = p_0\n",
    "    \n",
    "    for _ in range(maxiters):\n",
    "    \n",
    "        # E Step\n",
    "        r_ki = np.zeros((K,n))\n",
    "        for i in range(n):\n",
    "            r_ki[:,i] = responsibility(pi_k, p_km, X[i,:])\n",
    "        \n",
    "        # M Step     \n",
    "        eta_km = np.zeros((K,M))\n",
    "        eta_k = np.sum(r_ki, axis=-1)\n",
    "        eta = np.sum(eta_k)\n",
    "        for k in range(K):\n",
    "            for m in range(M):\n",
    "                eta_km[k,m] = np.sum(X[:,m] * r_ki[k,:]) \n",
    "        pi_k, p_km = update_parameters(eta_km, eta_k, eta, alpha, beta)\n",
    "        \n",
    "    return pi_k, p_km   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5656267",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We test the algorithm on a very simple dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241ecdea",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([[1., 1., 1.],\n",
    "              [1., 1., 1.],\n",
    "              [1., 1., 1.],\n",
    "              [1., 0., 1.],\n",
    "              [0., 1., 1.],\n",
    "              [0., 0., 0.],\n",
    "              [0., 0., 0.],\n",
    "              [0., 0., 1.]])\n",
    "n, M = X.shape\n",
    "K = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59ec167",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "seed = 535\n",
    "rng = np.random.default_rng(seed)\n",
    "pi_0 = np.ones(K)/K\n",
    "p_0 = rng.random((K,M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e95d6ee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pi_k, p_km = em_bern(X, K, pi_0, p_0, maxiters=100, alpha=0.01, beta=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65263162",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(pi_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2a5026",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(p_km)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4470584",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We compute the probability that the vector $(0, 0, 1)$ is in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed22beb5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x_test = np.array([0., 0., 1.])\n",
    "responsibility(pi_k, p_km, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757f6c14",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To give a more involved example, we return to the MNIST dataset. There are two common ways to write a $2$. Let's see if a mixture of multivariate Bernoullis can find them. We load the dataset and extract the images labelled $2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b589ac",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Download and load the MNIST dataset\n",
    "mnist = datasets.MNIST(root='./data', \n",
    "                       train=True, \n",
    "                       download=True, \n",
    "                       transform=transforms.ToTensor())\n",
    "\n",
    "# Convert the dataset to a PyTorch DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(mnist, \n",
    "                                           batch_size=len(mnist), \n",
    "                                           shuffle=False)\n",
    "\n",
    "# Extract images and labels from the DataLoader\n",
    "imgs, labels = next(iter(train_loader))\n",
    "imgs = imgs.squeeze().numpy()\n",
    "labels = labels.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704b7bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out images with label 2\n",
    "mask = labels == 2\n",
    "imgs2 = imgs[mask]\n",
    "labels2 = labels[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecad77a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Next, we transform the images into vectors and convert into black and white by rounding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba74b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.round(imgs2.reshape(len(imgs2), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118dcfd2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We can convert back as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6974cea5",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(X[0,:].reshape((28,28)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba41d75",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "In this example, the probabilities involved are very small and the responsibilities are close to $0$ or $1$. We use a variant, called hard EM, which replaces responsibilities with the one-hot encoding of the largest responsibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1b92cd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def hard_responsibility(pi_k, p_km, x):\n",
    "   \n",
    "    K = len(pi_k)\n",
    "        \n",
    "    # Computing the score for each k\n",
    "    score_k = np.zeros(K)\n",
    "    for k in range(K):\n",
    "       \n",
    "        score_k[k] += - np.log(pi_k[k])\n",
    "        score_k[k] += - np.sum(x*np.log(p_km[k,:]) + (1 - x)*np.log(1 - p_km[k,:]))\n",
    "    \n",
    "    # Computing responsibilities for each k\n",
    "    argmin = np.argmin(score_k, axis=0)\n",
    "    r_k = np.zeros(K)\n",
    "    r_k[argmin] = 1\n",
    "\n",
    "    return r_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298b37a6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def hard_em_bern(X, K, pi_0, p_0, maxiters = 10, alpha=0., beta=0.):\n",
    "    \n",
    "    n, M = X.shape\n",
    "    pi_k = pi_0\n",
    "    p_km = p_0\n",
    "    \n",
    "    for _ in range(maxiters):\n",
    "    \n",
    "        # E Step\n",
    "        r_ki = np.zeros((K,n))\n",
    "        for i in range(n):\n",
    "            r_ki[:,i] = hard_responsibility(pi_k, p_km, X[i,:])\n",
    "        \n",
    "        # M Step     \n",
    "        eta_km = np.zeros((K,M))\n",
    "        eta_k = np.sum(r_ki, axis=-1)\n",
    "        eta = np.sum(eta_k)\n",
    "        for k in range(K):\n",
    "            for m in range(M):\n",
    "                eta_km[k,m] = np.sum(X[:,m] * r_ki[k,:]) \n",
    "        pi_k, p_km = update_parameters(eta_km, eta_k, eta, alpha, beta)\n",
    "        \n",
    "    return pi_k, p_km   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c76add",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We run the algorithm with $2$ clusters. You may have to run it a few times to get a meaningful clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b8450a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n, M = X.shape\n",
    "K = 2\n",
    "seed = 535\n",
    "rng = np.random.default_rng(seed)\n",
    "pi_0 = np.ones(K)/K\n",
    "p_0 = rng.random((K,M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e24062",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pi_k, p_km = hard_em_bern(X, K, pi_0, p_0, maxiters=10, alpha=1., beta=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4abc58",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(pi_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd4c743",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We vizualize the two uncovered clusters by rendering their means as an image. Here is one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dafe47c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(p_km[0,:].reshape((28,28)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d017c4f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Here is the other one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b403237",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(p_km[1,:].reshape((28,28)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69710be3",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345c62b9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\newcommand{\\P}{\\mathbb{P}}$ $\\newcommand{\\E}{\\mathbb{E}}$ $\\newcommand{\\S}{\\mathcal{S}}$ $\\newcommand{\\var}{\\mathrm{Var}}$ $\\newcommand{\\bmu}{\\boldsymbol{\\mu}}$ $\\newcommand{\\bSigma}{\\boldsymbol{\\Sigma}}$ $\\newcommand{\\btheta}{\\boldsymbol{\\theta}}$ $\\newcommand{\\bpi}{\\boldsymbol{\\pi}}$ $\\newcommand{\\indep}{\\perp\\!\\!\\!\\perp}$ $\\newcommand{\\bp}{\\mathbf{p}}$ $\\newcommand{\\bx}{\\mathbf{x}}$ $\\newcommand{\\bX}{\\mathbf{X}}$ $\\newcommand{\\by}{\\mathbf{y}}$ $\\newcommand{\\bY}{\\mathbf{Y}}$ $\\newcommand{\\bz}{\\mathbf{z}}$ $\\newcommand{\\bZ}{\\mathbf{Z}}$ $\\newcommand{\\bw}{\\mathbf{w}}$ $\\newcommand{\\bW}{\\mathbf{W}}$ $\\newcommand{\\bv}{\\mathbf{v}}$ $\\newcommand{\\bV}{\\mathbf{V}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401173a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Linear-Gaussian models and Kalman filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27370b0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**Implementing the Kalman filter** We implement the Kalman filter as described above with known covariance matrices. We take $\\Delta = 1$ for simplicity. The code is adapted from [[Mur](https://github.com/probml)]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d5d483",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We will test Kalman filtering on a simulated path drawn from the linear-Gaussian model above. The following function creates such a path and its noisy observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f492d289",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "seed = 535\n",
    "rng = np.random.default_rng(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d56485",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def lgSamplePath(ss, os, F, H, Q, R, x_0, T):\n",
    "    x = np.zeros((ss,T)) \n",
    "    y = np.zeros((os,T))\n",
    "    x[:,0] = x_0\n",
    "    ey = np.zeros(os)\n",
    "    ey = rng.multivariate_normal(np.zeros(os),R) \n",
    "    y[:,0] = H @ x[:,0] + ey\n",
    "    \n",
    "    for t in range(1,T):\n",
    "        ex = np.zeros(ss)\n",
    "        ex = rng.multivariate_normal(np.zeros(ss),Q) # noise on x_t\n",
    "        x[:,t] = F @ x[:,t-1] + ex\n",
    "        ey = np.zeros(os)\n",
    "        ey = rng.multivariate_normal(np.zeros(os),R) # noise on y_t\n",
    "        y[:,t] = H @ x[:,t] + ey\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99a4bef",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Here is an example. Here $\\bSigma$ is denoted as $V$. In the plot, the blue crosses are the unobserved true path and the orange dots are the noisy observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f25172c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ss = 4 # state size\n",
    "os = 2 # observation size\n",
    "F = np.array([[1., 0., 1., 0.],[0., 1., 0., 1.],[0., 0., 1., 0.],[0., 0., 0., 1.]]) \n",
    "H = np.array([[1., 0., 0., 0.],[0., 1, 0., 0.]])\n",
    "Q = 0.1 * np.diag(np.ones(ss))\n",
    "R = 10 * np.diag(np.ones(os))\n",
    "x_0 = np.array([0., 0., 1., 1.]) # initial state\n",
    "T = 50\n",
    "x, y = lgSamplePath(ss, os, F, H, Q, R, x_0, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25eaa21b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.plot(y[0,:], y[1,:], marker='o', c='r', linestyle='dotted')\n",
    "plt.xlim((np.min(y[0,:])-5, np.max(y[0,:])+5)) \n",
    "plt.ylim((np.min(y[1,:])-5, np.max(y[1,:])+5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2e4814",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.plot(x[0,:], x[1,:], marker='x', c='g', linestyle='dashed', alpha=0.5)\n",
    "plt.xlim((np.min(x[0,:])-5, np.max(x[0,:])+5)) \n",
    "plt.ylim((np.min(x[1,:])-5, np.max(x[1,:])+5))\n",
    "plt.scatter(y[0,:], y[1,:], c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67104a40",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The following function implements the Kalman filter. Here $A$ is $F$ and $C$ is $H$. The full recursion is broken up into several steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec320bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def kalmanUpdate(ss, A, C, Q, R, y_t, mu_prev, Sig_prev):\n",
    "    mu_pred = A @ mu_prev\n",
    "    Sig_pred = A @ Sig_prev @ A.T + Q\n",
    "    e_t = y_t - C @ mu_pred # error at time t\n",
    "    S = C @ Sig_pred @ C.T + R\n",
    "    Sinv = LA.inv(S)\n",
    "    K = Sig_pred @ C.T @ Sinv # Kalman gain matrix\n",
    "    mu_new = mu_pred + K @ e_t\n",
    "    Sig_new = (np.diag(np.ones(ss)) - K @ C) @ Sig_pred\n",
    "    return mu_new, Sig_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022ab114",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def kalmanFilter(ss, os, y, A, C, Q, R, init_mu, init_Sig, T):\n",
    "    mu = np.zeros((ss, T))\n",
    "    Sig = np.zeros((ss, ss, T))\n",
    "    mu[:,0] = init_mu\n",
    "    Sig[:,:,0] = init_Sig\n",
    "\n",
    "    for t in range(1,T):\n",
    "        mu[:,t], Sig[:,:,t] = kalmanUpdate(ss, A, C, Q, R, y[:,t], mu[:,t-1], Sig[:,:,t-1])\n",
    "\n",
    "    return mu, Sig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea67aed",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We apply this to the example above. The inferred unobserved states are in green."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa1b462",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "init_mu = x_0\n",
    "init_Sig = 1 * np.diag(np.ones(ss))\n",
    "mu, Sig = kalmanFilter(ss, os, y, F, H, Q, R, init_mu, init_Sig, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996ea325",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.plot(x[0,:], x[1,:], marker='x', c='g', linestyle='dashed', alpha=0.5)\n",
    "plt.xlim((np.min(x[0,:])-5, np.max(x[0,:])+5)) \n",
    "plt.ylim((np.min(x[1,:])-5, np.max(x[1,:])+5))\n",
    "plt.scatter(y[0,:], y[1,:], c='r')\n",
    "plt.plot(mu[0,:], mu[1,:], marker='s', linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a16a0a5",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To quantify the improvement in the inference compared to the observations, we compute the mean squared error in both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aeb7e8d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dobs = x[0:1,:] - y[0:1,:]\n",
    "mse_obs = np.sqrt(np.sum(dobs**2))\n",
    "print(mse_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f70ab0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dfilt = x[0:1,:] - mu[0:1,:]\n",
    "mse_filt = np.sqrt(np.sum(dfilt**2))\n",
    "print(mse_filt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9dc03d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We indeed observe a reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f88858",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**Missing data** We can also allow for the possibility that some observations are missing. Imagine for instance losing GPS signal while going through a tunnel. The recursions above are still valid, with the only modification that the $\\bY_t$ and $H$ terms are dropped at those times $t$ where there is no observation. In Numpy, we can use [`NaN`](https://numpy.org/doc/stable/reference/constants.html#numpy.nan). (Alternatively, one can use the [numpy.ma](https://numpy.org/doc/stable/reference/maskedarray.generic.html) module.) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c008b1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We use a same sample path as above, but mask observations at times $t=10,\\ldots,20$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b620cae",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ss = 4\n",
    "os = 2\n",
    "F = np.array([[1., 0., 1., 0.],[0., 1., 0., 1.],[0., 0., 1., 0.],[0., 0., 0., 1.]]) \n",
    "H = np.array([[1., 0., 0., 0.],[0., 1, 0., 0.]])\n",
    "Q = 0.01 * np.diag(np.ones(ss))\n",
    "R = 10 * np.diag(np.ones(os))\n",
    "x_0 = np.array([0., 0., 1., 1.])\n",
    "T = 30\n",
    "x, y = lgSamplePath(ss, os, F, H, Q, R, x_0, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2456c07a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(10,20):\n",
    "    y[0,i] = np.nan\n",
    "    y[1,i] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77053c4e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Here is the sample we are aiming to infer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec57bd5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.plot(x[0,:], x[1,:], marker='x', c='g', linestyle='dashed', alpha=0.5)\n",
    "plt.xlim((np.min(x[0,:])-5, np.max(x[0,:])+5)) \n",
    "plt.ylim((np.min(x[1,:])-5, np.max(x[1,:])+5))\n",
    "plt.scatter(y[0,:], y[1,:], c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6725c0fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We modify the recursion accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dbdb4a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def kalmanUpdate(ss, A, C, Q, R, y_t, mu_prev, Sig_prev):\n",
    "    mu_pred = A @ mu_prev\n",
    "    Sig_pred = A @ Sig_prev @ A.T + Q\n",
    "    if np.isnan(y_t[0]) or np.isnan(y_t[1]):\n",
    "        return mu_pred, Sig_pred\n",
    "    else:\n",
    "        e_t = y_t - C @ mu_pred # error at time t\n",
    "        S = C @ Sig_pred @ C.T + R\n",
    "        Sinv = LA.inv(S)\n",
    "        K = Sig_pred @ C.T @ Sinv # Kalman gain matrix\n",
    "        mu_new = mu_pred + K @ e_t\n",
    "        Sig_new = (np.diag(np.ones(ss)) - K @ C) @ Sig_pred\n",
    "        return mu_new, Sig_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ca9018",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "init_mu = x_0\n",
    "init_Sig = 1 * np.diag(np.ones(ss))\n",
    "mu, Sig = kalmanFilter(ss, os, y, F, H, Q, R, init_mu, init_Sig, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9036d7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.plot(x[0,:], x[1,:], marker='x', c='g', alpha=0.5)\n",
    "plt.xlim((np.min(x[0,:])-5, np.max(x[0,:])+5)) \n",
    "plt.ylim((np.min(x[1,:])-5, np.max(x[1,:])+5))\n",
    "plt.scatter(y[0,:], y[1,:], c='r', alpha=0.5)\n",
    "plt.plot(mu[0,:], mu[1,:], marker='s', linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56193e4d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\newcommand{\\P}{\\mathbb{P}}$ $\\newcommand{\\E}{\\mathbb{E}}$ $\\newcommand{\\S}{\\mathcal{S}}$ $\\newcommand{\\var}{\\mathrm{Var}}$ $\\newcommand{\\bmu}{\\boldsymbol{\\mu}}$ $\\newcommand{\\bSigma}{\\boldsymbol{\\Sigma}}$ $\\newcommand{\\btheta}{\\boldsymbol{\\theta}}$ $\\newcommand{\\bpi}{\\boldsymbol{\\pi}}$ $\\newcommand{\\indep}{\\perp\\!\\!\\!\\perp}$ $\\newcommand{\\bp}{\\mathbf{p}}$ $\\newcommand{\\bx}{\\mathbf{x}}$ $\\newcommand{\\bX}{\\mathbf{X}}$ $\\newcommand{\\by}{\\mathbf{y}}$ $\\newcommand{\\bY}{\\mathbf{Y}}$ $\\newcommand{\\bz}{\\mathbf{z}}$ $\\newcommand{\\bZ}{\\mathbf{Z}}$ $\\newcommand{\\bw}{\\mathbf{w}}$ $\\newcommand{\\bW}{\\mathbf{W}}$ $\\newcommand{\\bv}{\\mathbf{v}}$ $\\newcommand{\\bV}{\\mathbf{V}}$ $\\newcommand{\\Z}{\\mathcal{Z}}$ $\\newcommand{\\bh}{\\mathbf{h}}$ $\\newcommand{\\bb}{\\mathbf{b}}$ $\\newcommand{\\bc}{\\mathbf{c}}$ $\\newcommand{\\cE}{\\mathcal{E}}$ $\\newcommand{\\cP}{\\mathcal{P}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ead6713",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Gibbs sampling with application to generating images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fafc02",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** Recall how this works. We first initialize the random number generator and use a `seed` for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d765306",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "seed = 535\n",
    "rng = np.random.default_rng(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4647a3b2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To generate, say $1000$, samples from a multivariate normal, say with mean $(0, 0)$ and covariance $\\begin{pmatrix}5 & 0\\\\0 & 1\\end{pmatrix}$, we use [`numpy.random.Generator.multivariate_normal`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.multivariate_normal.html#numpy.random.Generator.multivariate_normal) as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2e7161",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mean = [0, 0]\n",
    "cov = [[5, 0], [0, 1]]\n",
    "x, y = rng.multivariate_normal(mean, cov, 1000).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e762a7b3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Plotting the result we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3a1b6e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.scatter(x, y, marker='x')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0716ce26",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Computing the mean of each component we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d487597f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(np.mean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbfe45d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(np.mean(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6931c421",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "This is somewhat close to the expected answer: $(0,0)$. \n",
    "\n",
    "Using a larger number of samples, say $10,000$, gives a better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476784ed",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x, y = rng.multivariate_normal(mean, cov, 10000).T\n",
    "print(np.mean(x))\n",
    "print(np.mean(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ad9572",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e601c9a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Sampling from an arbitrary distribution on a finite set is also straightforward -- as long as the set is not too big. This can be done using [`numpy.random.Generator.choice`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.choice.html). Borrowing the example from the documentation, the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80e1744",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "aa_milne_arr = ['pooh', 'rabbit', 'piglet', 'christopher']\n",
    "rng.choice(aa_milne_arr, 5, p=[0.5, 0.1, 0.1, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4610b7dc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "generates $5$ samples from the set $\\S = \\{\\tt{pooh}, \\tt{rabbit}, \\tt{piglet}, \\tt{christopher}\\}$ with respective probabilities $0.5, 0.1, 0.1, 0.3$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc3a21a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "But this may not be practical when the state space $\\S$ is very large. As an example, later in this section, we will learn a \"realistic\" distribution of handwritten digits. We will do so using the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database), which we have encountered previously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b21c4b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We load it from PyTorch and turn the grayscale images (more precisely, each pixel is an integer between $0$ and $255$) into a black-and-white images by rounding the pixels (after dividing by $255$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d418e94",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Download and load the MNIST dataset\n",
    "mnist = datasets.MNIST(root='./data', \n",
    "                       train=True, \n",
    "                       download=True, \n",
    "                       transform=transforms.ToTensor())\n",
    "\n",
    "# Convert the dataset to a PyTorch DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(mnist, \n",
    "                                           batch_size=len(mnist), \n",
    "                                           shuffle=False)\n",
    "\n",
    "# Extract images and labels from the DataLoader\n",
    "imgs, labels = next(iter(train_loader))\n",
    "imgs = imgs.squeeze().numpy()\n",
    "labels = labels.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d0cd62",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "imgs = np.round(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1d7eb3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The first image is the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cac03f9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.imshow(imgs[0], cmap=plt.cm.gray_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b0857c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "It is $28 \\times 28$, so the total number of pixels is $784$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fa58af",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nx_pixels, ny_pixels = imgs[0].shape\n",
    "nx_pixels, ny_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f8d497",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n_pixels = nx_pixels * ny_pixels\n",
    "n_pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac797a3e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To specify the a distribution over all possible black and white images of this size, we need in principle to assign a probability to a very large number of states. Our space here is $\\S = \\{0,1\\}^{784}$, imagining that $0$ encodes white and $1$ encodes black and that we have ordered the pixels in some arbitrary way. How big is this space?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82215296",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Answer: $2^{784}$.\n",
    "\n",
    "Or in base $10$, we compute $\\log_{10}(2^{784})$, which is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02c02df",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "784 * np.log(2) / np.log(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae78ebe0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "So a little more than $10^{236}$. \n",
    "\n",
    "This is much too large to naively plug into `rng.choice`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211f21da",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**EXAMPLE:** Suppose $\\S = \\{1,\\cdots, n\\} = [n]$ for some positive integer $n$ and $\\bpi$ is proportional to a Poisson distribution with mean $\\lambda > 0$. That is, \n",
    "\n",
    "$$\n",
    "\\pi_i = C e^{-\\lambda} \\frac{\\lambda^i}{i!}, \\quad \\forall i \\in \\S\n",
    "$$\n",
    "\n",
    "for some constant $C$ chosen so that $\\sum_{i=1}^{n} \\pi_i = 1$. Recall that we do not need to determine $C$ as it is enough to know the target distribution up to a scaling factor by the previous remark. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ee93f0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To apply Metropolis-Hastings, we need a proposal chain. Consider the following choice. For each $1 < i < n$, move to $i+1$ or $i-1$ with probability $1/2$ each. For $i=1$ (respectively $i = n$), move to $2$ (respectively $n-1$) with probability $1/2$, otherwise stay where you are. For instance, if $n = 4$, then \n",
    "\n",
    "$$\n",
    "Q\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "1/2 & 1/2 & 0 & 0\\\\\n",
    "1/2 & 0 & 1/2 & 0\\\\\n",
    "0 & 1/2 & 0 & 1/2\\\\\n",
    "0 & 0 & 1/2 & 1/2\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "\n",
    "which is indeed a stochastic matrix. It is also symmetric, so it does not enter into the acceptance probability by the previous remark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd52039c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To compute the acceptance probability, we only need to consider pairs of adjacent integers as they are the only one that have non-zero probability under $Q$. Consider state $1 < i < n$. Observe that\n",
    "\n",
    "$$\n",
    "\\frac{\\pi_{i+1}}{\\pi_{i}}\n",
    "= \\frac{C e^{-\\lambda} \\lambda^{i+1}/(i+1)!}{C e^{-\\lambda} \\lambda^{i}/i!}\n",
    "= \\frac{\\lambda}{i+1}\n",
    "$$\n",
    "\n",
    "so a move to $i+1$ happens with probability\n",
    "\n",
    "$$\n",
    "\\frac{1}{2} \\min\\left\\{1, \\frac{\\lambda}{i+1}\\right\\},\n",
    "$$\n",
    "\n",
    "where the $1/2$ factor from the proposal distribution.\n",
    "Similarly, it can be checked (try it!) that a move to $i-1$ occurs with probability\n",
    "\n",
    "$$\n",
    "\\frac{1}{2} \\min\\left\\{1, \\frac{i}{\\lambda}\\right\\}.\n",
    "$$\n",
    "\n",
    "And we stay at $i$ with probability $1 - \\frac{1}{2} \\min\\left\\{1, \\frac{\\lambda}{i+1}\\right\\} - \\frac{1}{2} \\min\\left\\{1, \\frac{i}{\\lambda}\\right\\}$. (Why is this guaranteed to be a probability?) \n",
    "\n",
    "A similar formula applies to $i = 1, n$. (Try it!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4841bc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We are ready to apply Metropolis-Hastings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a629ac7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def mh_transition_poisson(lmbd, n):\n",
    "    P = np.zeros((n,n))\n",
    "    for idx in range(n):\n",
    "        i = idx + 1 # index starts at 0 rather than 1\n",
    "        if (i > 1 and i < n):\n",
    "            P[idx, idx+1] = (1/2) * np.min(np.array([1, lmbd/(i+1)]))\n",
    "            P[idx, idx-1] = (1/2) * np.min(np.array([1, i/lmbd]))\n",
    "            P[idx, idx] = 1 - P[idx, idx+1] - P[idx, idx-1]\n",
    "        elif i == 1:\n",
    "            P[idx, idx+1] = (1/2) * np.min(np.array([1, lmbd/(i+1)]))\n",
    "            P[idx, idx] = 1 - P[idx, idx+1]\n",
    "        elif i == n:\n",
    "            P[idx, idx-1] = (1/2) * np.min(np.array([1, i/lmbd]))\n",
    "            P[idx, idx] = 1 - P[idx, idx-1]\n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ed72fa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Take $\\lambda = 1$ and $n = 6$. We get the following transition matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00c8b5f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lmbd = 1\n",
    "n = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b845e10e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "P = mh_transition_poisson(lmbd, n)\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0333308",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We use our simulator from a previous chapter. We start from the uniform distribution and take $100$ steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9970e77",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mu = np.ones(n) / n\n",
    "T = 100\n",
    "X = mmids.SamplePath(mu, P, T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d287989b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Our sample is the final state of the trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c64370",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X[T]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed1069d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We repeat $1000$ times and plot the frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f908f4a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "N_samples = 1000 # number of repetitions\n",
    "\n",
    "freq_z = np.zeros(n) # init of frequencies sampled\n",
    "for i in range(N_samples):\n",
    "    X = mmids.SamplePath(mu, P, T)\n",
    "    freq_z[int(X[T])-1] += 1 # adjust for index starting at 0\n",
    "    \n",
    "freq_z = freq_z/N_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e4b89d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.bar(range(1,n+1),freq_z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b628753",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "If we increase the parameter $\\lambda$ (which is not quite the mean; why?), we expect the sampled distribution to shift to the right. We must recompute the transition matrix first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c3fec0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lmbd = 10\n",
    "P = mh_transition_poisson(lmbd, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e5d10c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "freq_z = np.zeros(n) # init of frequencies sampled\n",
    "for i in range(N_samples):\n",
    "    X = mmids.SamplePath(mu, P, T)\n",
    "    freq_z[int(X[T])-1] += 1 # adjust for index starting at 0\n",
    "    \n",
    "freq_z = freq_z/N_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31025bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.bar(range(1,n+1),freq_z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3d4e86",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**TRY IT!** Redo the simulations, but this time implement a general Metropolis-Hastings algorithm rather than specifying the transition matrix directly. That is, implement the algorithm for an arbitrary $\\bpi$ and $Q$. Assume the state space is $[n]$. ([Open in Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502c79fe",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\lhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6325671",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "*Gibbs sampling:* We sample from the joint distribution $\\pi$ and observe only $\\bv$.\n",
    "\n",
    "We need to compute the conditional probabilities given every other variable. The sigmoid function, which we have encountered previously, \n",
    "\n",
    "$$\n",
    "\\sigma(x)\n",
    "=\n",
    "\\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "will once again make an appearance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330343f9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x): \n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e78db3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "grid = np.linspace(-5, 5, 100)\n",
    "plt.plot(grid,sigmoid(grid),'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5476e0a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We implement the Gibbs sampler for an RBM. Rather than updating the units at random, we use a block approach. Specifically, we update all hidden units independently, given the visible units; then we update all visible units independently, given the hidden units. In each case, this is warranted by the conditional independence structure revealed above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790425ba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We first implement the conditional means using the formulas previously derived. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748b5d3e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def rbm_mean_hidden(v, W, c):\n",
    "    return sigmoid(W @ v + c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8312412f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def rbm_mean_visible(h, W, b):\n",
    "    return sigmoid(W.T @ h + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89daa8f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We next implement one step of the sampler, which consists in updating all hidden units, followed by updating all visible units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9433ce3b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def rbm_gibbs_update(v, W, b, c):\n",
    "    p_hidden = rbm_mean_hidden(v, W, c)\n",
    "    h = rng.binomial(1, p_hidden, p_hidden.shape)\n",
    "    p_visible = rbm_mean_visible(h, W, b)\n",
    "    v = rng.binomial(1, p_visible, p_visible.shape)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b17be4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Finally, we repeat these steps `k` times. We only return the visible units `v`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbfe20a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def rbm_gibbs_sampling(k, v_0, W, b, c):\n",
    "    counter = 0\n",
    "    v = v_0\n",
    "    while counter < k:\n",
    "        v = rbm_gibbs_update(v, W, b, c)\n",
    "        counter += 1\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875a2944",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Here `v_0` is the initial visible unit states. We do not need to initialize the hidden ones as this is done automatically in the first update step. In the next subsection, we will take the initial distribution of $\\bv$ to be independent Bernoullis with success probability $1/2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be5a99c",
   "metadata": {
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee89b5a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**Trainging the model** We first need to train the model on the data. We will not show how this is done here, but instead use [`sklearn.neural_network.BernoulliRBM`](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.BernoulliRBM.html). (Some details of how this training is done is provided [here](https://scikit-learn.org/stable/modules/neural_networks_unsupervised.html#stochastic-maximum-likelihood-learning).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84de512",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import BernoulliRBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dbd88e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rbm = BernoulliRBM(random_state=seed, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d3e6c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To simplify the analysis and speed up the training, we only keep digits $0$, $1$ and $5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba3edd0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Filter out images with labels 0, 1, or 5\n",
    "mask = (labels == 0) | (labels == 1) | (labels == 5)\n",
    "imgs = imgs[mask]\n",
    "labels = labels[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4aeed01",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We flatten the images (which have already been \"rounded\" to black-and-white; see the first subsection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1e8200",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = imgs.reshape(len(imgs), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dceb131d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We now fit the model. Choosing the hyperparameters of the training algorithm is tricky. The following seem to work reasonably well. (For a more systematic approach to tuning hyperparameters, see [here](https://scikit-learn.org/stable/modules/grid_search.html).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252d7b1f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rbm.n_components = 100\n",
    "rbm.learning_rate = 0.02\n",
    "rbm.batch_size = 50\n",
    "rbm.n_iter = 20\n",
    "rbm.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520f4e5b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We plot the learned weight matrix using a script [adapted from here](https://scikit-learn.org/stable/auto_examples/neural_networks/plot_rbm_logistic_classification.html). Each image shows the weights associated to all visible units by on hidden unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8972f02",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_imgs(z, n_imgs, nx_pixels, ny_pixels):\n",
    "    nx_imgs = np.floor(np.sqrt(n_imgs))\n",
    "    ny_imgs = np.ceil(np.sqrt(n_imgs))\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    for i, comp in enumerate(z):\n",
    "        plt.subplot(int(nx_imgs), int(ny_imgs), i + 1)\n",
    "        plt.imshow(comp.reshape((nx_pixels, ny_pixels)), \n",
    "                   cmap=plt.cm.gray_r)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1369ace2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plot_imgs(rbm.components_, rbm.n_components, \n",
    "          nx_pixels, ny_pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e7b75a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**Back to Gibbs sampling** We are ready to sample from the trained RBM. We extract the learned parameters from `rbm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c0e624",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "W = rbm.components_\n",
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59506b23",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "b = rbm.intercept_visible_\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08216c2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "c = rbm.intercept_hidden_\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eb21be",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To generate $25$ samples, we first generate $25$ independent initial states. We stack them into a matrix, where each row is a different flattened random noise image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911515e1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n_samples = 25 # number of samples\n",
    "z = rng.binomial(1, 0.5, (n_samples, n_pixels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6290667",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plot_imgs(z, n_samples, nx_pixels, ny_pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70547729",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To process all samples simultaneously, we make a small change to the code. We [`numpy.reshape`](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html)\n",
    "to make the offsets into column vectors, which are then automatically added to all columns of the resulting weighted sum. \n",
    "(This is known as [broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de8ef06",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def rbm_mean_hidden(v, W, c):\n",
    "    return sigmoid(W @ v + c.reshape(len(c),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1123140",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def rbm_mean_visible(h, W, b):\n",
    "    return sigmoid(W.T @ h + b.reshape(len(b),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8edef61",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We are now ready to run our Gibbs sampler. The outcome depends on the number of steps we take. For instance, after one step, the result is still very noisy -- although note that the fraction of white pixels is already more realistic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb4f05a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "v_0 = z.T\n",
    "gen_v = rbm_gibbs_sampling(1, v_0, W, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f959d9d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plot_imgs(gen_v.T, n_samples, nx_pixels, ny_pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a85f839",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "After $10$ steps, we already see shadows of digits appearing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70008c27",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "v_0 = z.T\n",
    "gen_v = rbm_gibbs_sampling(10, v_0, W, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8013fbc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plot_imgs(gen_v.T, n_samples, nx_pixels, ny_pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9731f638",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "After $100$ steps, the outcome is quite realistic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c26a16c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "v_0 = z.T\n",
    "gen_v = rbm_gibbs_sampling(100, v_0, W, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cbf4dd",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plot_imgs(gen_v.T, n_samples, nx_pixels, ny_pixels)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
