
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>3.8. Online supplementary material &#8212; MMiDS Textbook</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=87e54e7c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-P5E8DW088F"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-P5E8DW088F');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-P5E8DW088F');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chap03_opt/supp/roch-mmids-opt-supp';</script>
    <link rel="canonical" href="https://mmids-textbook.github.io/chap03_opt/supp/roch-mmids-opt-supp.html" />
    <link rel="icon" href="https://mmids-textbook.github.io/favicon-32x32.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="4. Singular value decomposition" href="../../chap04_svd/00_intro/roch-mmids-svd-intro.html" />
    <link rel="prev" title="3.7. Exercises" href="../exercises/roch-mmids-opt-exercises.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/mmids-cover-small.jpg" class="logo__image only-light" alt="MMiDS Textbook - Home"/>
    <script>document.write(`<img src="../../_static/mmids-cover-small.jpg" class="logo__image only-dark" alt="MMiDS Textbook - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../index.html">
                    <b>MATHEMATICAL METHODS in DATA SCIENCE (with Python)</b>
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap01_intro/00_intro/roch-mmids-intro-intro.html">1. Introduction: a first data science problem</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap01_intro/01_motiv/roch-mmids-intro-motiv.html">1.1. Motivating example: identifying penguin species</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap01_intro/02_review/roch-mmids-intro-review.html">1.2. Background: quick refresher of matrix algebra, differential calculus, and elementary probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap01_intro/03_clustering/roch-mmids-intro-clustering.html">1.3. Clustering: an objective, an algorithm and a guarantee</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap01_intro/04_highdim/roch-mmids-intro-highdim.html">1.4. Some observations about high-dimensional data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap01_intro/exercises/roch-mmids-intro-exercises.html">1.5. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap01_intro/supp/roch-mmids-intro-supp.html">1.6. Online supplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap02_ls/00_intro/roch-mmids-ls-intro.html">2. Least squares: geometric, algebraic, and numerical aspects</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/01_motiv/roch-mmids-ls-motiv.html">2.1. Motivating example: predicting sales</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/02_spaces/roch-mmids-ls-spaces.html">2.2. Background: review of vector spaces and matrix inverses</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/03_orthog/roch-mmids-ls-orthog.html">2.3. Geometry of least squares: the orthogonal projection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/04_qr/roch-mmids-ls-qr.html">2.4. QR decomposition and Householder transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/05_regression/roch-mmids-ls-regression.html">2.5. Application: regression analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/exercises/roch-mmids-ls-exercises.html">2.6. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/supp/roch-mmids-ls-supp.html">2.7. Online supplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../00_intro/roch-mmids-opt-intro.html">3. Optimization theory and algorithms</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../01_motiv/roch-mmids-opt-motiv.html">3.1. Motivating example:  analyzing customer satisfaction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02_several/roch-mmids-opt-several.html">3.2. Background: review of differentiable functions of several variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03_optimality/roch-mmids-opt-optimality.html">3.3. Optimality conditions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04_convexity/roch-mmids-opt-convexity.html">3.4. Convexity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../05_gd/roch-mmids-opt-gd.html">3.5. Gradient descent and its convergence analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../06_logistic/roch-mmids-opt-logistic.html">3.6. Application: logistic regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../exercises/roch-mmids-opt-exercises.html">3.7. Exercises</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">3.8. Online supplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap04_svd/00_intro/roch-mmids-svd-intro.html">4. Singular value decomposition</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_svd/01_motiv/roch-mmids-svd-motiv.html">4.1. Motivating example: visualizing viral evolution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_svd/02_spectral/roch-mmids-svd-spectral.html">4.2. Background: review of matrix rank  and spectral decomposition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_svd/03_svd/roch-mmids-svd-svd.html">4.3. Approximating subspaces and the SVD</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_svd/04_power/roch-mmids-svd-power.html">4.4. Power iteration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_svd/05_pca/roch-mmids-svd-pca.html">4.5. Application: principal components analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_svd/06_further/roch-mmids-svd-further.html">4.6. Further applications of the SVD: low-rank approximations and ridge regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_svd/exercises/roch-mmids-svd-exercises.html">4.7. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_svd/supp/roch-mmids-svd-supp.html">4.8. Online supplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap05_specgraph/00_intro/roch-mmids-specgraph-intro.html">5. Spectral graph theory</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap05_specgraph/01_motiv/roch-mmids-specgraph-motiv.html">5.1. Motivating example: uncovering social groups</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap05_specgraph/02_graph/roch-mmids-specgraph-graph.html">5.2. Background: basic concepts in graph theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap05_specgraph/03_extremal/roch-mmids-specgraph-extremal.html">5.3. Variational characterization of eigenvalues</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap05_specgraph/04_laplacian/roch-mmids-specgraph-laplacian.html">5.4. Spectral properties of the Laplacian matrix</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap05_specgraph/05_partitioning/roch-mmids-specgraph-partitioning.html">5.5. Application: graph partitioning via spectral clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap05_specgraph/06_sbm/roch-mmids-specgraph-sbm.html">5.6. Erdős-Rényi random graph and stochastic blockmodel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap05_specgraph/exercises/roch-mmids-specgraph-exercises.html">5.7. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap05_specgraph/supp/roch-mmids-specgraph-supp.html">5.8. Online supplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap06_prob/00_intro/roch-mmids-prob-intro.html">6. Probabilistic models: from simple to complex</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap06_prob/01_motiv/roch-mmids-prob-motiv.html">6.1. Motivating example: tracking location</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap06_prob/02_parametric/roch-mmids-prob-parametric.html">6.2. Background: introduction to parametric families and maximum likelihood estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap06_prob/03_joint/roch-mmids-prob-joint.html">6.3. Modeling more complex dependencies 1: using conditional independence</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap06_prob/04_em/roch-mmids-prob-em.html">6.4. Modeling more complex dependencies 2: marginalizing out an unobserved variable</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap06_prob/05_kalman/roch-mmids-prob-kalman.html">6.5. Application: linear-Gaussian models and Kalman filtering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap06_prob/exercises/roch-mmids-prob-exercises.html">6.6. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap06_prob/supp/roch-mmids-prob-supp.html">6.7. Online supplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap07_rwmc/00_intro/roch-mmids-rwmc-intro.html">7. Random walks on graphs and Markov chains</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap07_rwmc/01_motiv/roch-mmids-rwmc-motiv.html">7.1. Motivating example: discovering mathematical topics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap07_rwmc/02_mcdefs/roch-mmids-rwmc-mcdefs.html">7.2. Background: elements of finite Markov chains</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap07_rwmc/03_stat/roch-mmids-rwmc-stat.html">7.3. Limit behavior 1: stationary distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap07_rwmc/04_mclimit/roch-mmids-rwmc-mclimit.html">7.4. Limit behavior 2: convergence to equilibrium</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap07_rwmc/05_pagerank/roch-mmids-rwmc-pagerank.html">7.5. Application: random walks on graphs and PageRank</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap07_rwmc/06_gibbs/roch-mmids-rwmc-gibbs.html">7.6. Further applications: Gibbs sampling and generating images</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap07_rwmc/exercises/roch-mmids-rwmc-exercises.html">7.7. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap07_rwmc/supp/roch-mmids-rwmc-supp.html">7.8. Online supplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap08_nn/00_intro/roch-mmids-nn-intro.html">8. Neural networks, backpropagation and stochastic gradient descent</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/01_motiv/roch-mmids-nn-motiv.html">8.1. Motivating example:  classifying natural images</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/02_chain/roch-mmids-nn-chain.html">8.2. Background: Jacobian, chain rule, and a brief introduction to automatic differentiation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/03_backprop/roch-mmids-nn-backprop.html">8.3. Building blocks of AI 1: backpropagation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/04_sgd/roch-mmids-nn-sgd.html">8.4. Building blocks of AI 2: stochastic gradient descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/05_nn/roch-mmids-nn-nn.html">8.5. Building blocks of AI 3: neural networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/exercises/roch-mmids-nn-exercises.html">8.6. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/supp/roch-mmids-nn-supp.html">8.7. Online supplementary material</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/issues/new?title=Issue%20on%20page%20%2Fchap03_opt/supp/roch-mmids-opt-supp.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/chap03_opt/supp/roch-mmids-opt-supp.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Online supplementary material</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quizzes-solutions-code-etc">3.8.1. Quizzes, solutions, code, etc.</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#just-the-code">3.8.1.1. Just the code</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#self-assessment-quizzes">3.8.1.2. Self-assessment quizzes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#auto-quizzes">3.8.1.3. Auto-quizzes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#solutions-to-odd-numbered-warm-up-exercises">3.8.1.4. Solutions to odd-numbered warm-up exercises</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-outcomes">3.8.1.5. Learning outcomes</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-sections">3.8.2. Additional sections</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-illustration-of-convergence-result">3.8.2.1. Logistic regression: illustration of convergence result</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-another-dataset">3.8.2.2. Logistic regression: another dataset</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <p><span class="math notranslate nohighlight">\(\newcommand{\bSigma}{\boldsymbol{\Sigma}}\)</span> <span class="math notranslate nohighlight">\(\newcommand{\bmu}{\boldsymbol{\mu}}\)</span> <span class="math notranslate nohighlight">\(\newcommand{\blambda}{\boldsymbol{\lambda}}\)</span></p>
<section id="online-supplementary-material">
<h1><span class="section-number">3.8. </span>Online supplementary material<a class="headerlink" href="#online-supplementary-material" title="Link to this heading">#</a></h1>
<section id="quizzes-solutions-code-etc">
<h2><span class="section-number">3.8.1. </span>Quizzes, solutions, code, etc.<a class="headerlink" href="#quizzes-solutions-code-etc" title="Link to this heading">#</a></h2>
<section id="just-the-code">
<h3><span class="section-number">3.8.1.1. </span>Just the code<a class="headerlink" href="#just-the-code" title="Link to this heading">#</a></h3>
<p>An interactive Jupyter notebook featuring the code in this chapter can be accessed below (Google Colab recommended). You are encouraged to tinker with it. Some suggested computational exercises are scattered throughout. The notebook is also available as a slideshow.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb">Notebook</a> (<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb">Open In Colab</a>)</p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_opt_notebook_slides.slides.html">Slideshow</a></p></li>
</ul>
</section>
<section id="self-assessment-quizzes">
<h3><span class="section-number">3.8.1.2. </span>Self-assessment quizzes<a class="headerlink" href="#self-assessment-quizzes" title="Link to this heading">#</a></h3>
<p>A more extensive web version of the self-assessment quizzes is available by following the links below.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_3_2.html">Section 3.2</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_3_3.html">Section 3.3</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_3_4.html">Section 3.4</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_3_5.html">Section 3.5</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_3_6.html">Section 3.6</a></p></li>
</ul>
</section>
<section id="auto-quizzes">
<h3><span class="section-number">3.8.1.3. </span>Auto-quizzes<a class="headerlink" href="#auto-quizzes" title="Link to this heading">#</a></h3>
<p>Automatically generated quizzes for this chapter can be accessed here (Google Colab recommended).</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-opt-autoquiz.ipynb">Auto-quizzes</a>
(<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-opt-autoquiz.ipynb">Open In Colab</a>)</p></li>
</ul>
</section>
<section id="solutions-to-odd-numbered-warm-up-exercises">
<h3><span class="section-number">3.8.1.4. </span>Solutions to odd-numbered warm-up exercises<a class="headerlink" href="#solutions-to-odd-numbered-warm-up-exercises" title="Link to this heading">#</a></h3>
<p><em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p>Answer and justification for E3.2.1:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\nabla f(x_1, x_2) &amp;= \left(\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}\right) \\
&amp;= (6x_1 - 2x_2 - 5, -2x_1 + 8x_2 + 2).
\end{align*}\]</div>
<p>At the point <span class="math notranslate nohighlight">\((1, -1)\)</span>, we have:</p>
<div class="math notranslate nohighlight">
\[
\nabla f(1, -1) = (6(1) - 2(-1) - 5, -2(1) + 8(-1) + 2) = (3, -8).
\]</div>
<p>Answer and justification for E3.2.3:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{\partial f}{\partial x_1} &amp;= \cos(x_1) \cos(x_2), \\
\frac{\partial f}{\partial x_2} &amp;= -\sin(x_1) \sin(x_2).
\end{align*}\]</div>
<p>At the point <span class="math notranslate nohighlight">\((\frac{\pi}{4}, \frac{\pi}{3})\)</span>, we have:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{\partial f}{\partial x_1}(\frac{\pi}{4}, \frac{\pi}{3}) &amp;= \cos(\frac{\pi}{4}) \cos(\frac{\pi}{3}) = \frac{\sqrt{2}}{2} \cdot \frac{1}{2} = \frac{\sqrt{2}}{4}, \\
\frac{\partial f}{\partial x_2}(\frac{\pi}{4}, \frac{\pi}{3}) &amp;= -\sin(\frac{\pi}{4}) \sin(\frac{\pi}{3}) = -\frac{\sqrt{2}}{2} \cdot \frac{\sqrt{3}}{2} = -\frac{\sqrt{6}}{4}.
\end{align*}\]</div>
<p>Answer and justification for E3.2.5: The Hessian matrix of <span class="math notranslate nohighlight">\(f(x_1, x_2)\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{H}_f(x_1, x_2) = \begin{pmatrix}
6x_1 &amp; 6x_2 \\
6x_2 &amp; 6x_1 - 12x_2
\end{pmatrix}.
\end{split}\]</div>
<p>At the point <span class="math notranslate nohighlight">\((1, 2)\)</span>, we have:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{H}_f(1, 2) = \begin{pmatrix}
6 &amp; 12 \\
12 &amp; -18
\end{pmatrix}.
\end{split}\]</div>
<p>We can see that <span class="math notranslate nohighlight">\(\frac{\partial^2 f}{\partial x_1 \partial x_2}(1, 2) = \frac{\partial^2 f}{\partial x_2 \partial x_1}(1, 2) = 12\)</span>, confirming the Symmetry of the Hessian Theorem.</p>
<p>Answer and justification for E3.2.7:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{\partial^2 f}{\partial x_1^2} &amp;= 2 \sin(x_2), \\
\frac{\partial^2 f}{\partial x_1 \partial x_2} &amp;= 2x_1 \cos(x_2), \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} &amp;= 2x_1 \cos(x_2), \\
\frac{\partial^2 f}{\partial x_2^2} &amp;= -x_1^2 \sin(x_2).
\end{align*}\]</div>
<p>Answer and justification for E3.2.9: The Hessian matrix is given by:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{H}_f(x_1, x_2, x_3) = \begin{pmatrix}
2 &amp; -2 &amp; 4 \\
-2 &amp; 4 &amp; -6 \\
4 &amp; -6 &amp; 6
\end{pmatrix}.
\end{split}\]</div>
<p>Answer and justification for E3.2.11: <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial x} = 3x^2y^2 - 2y^3\)</span> and <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial y} = 2x^3y - 6xy^2 + 4y^3\)</span>, obtained by differentiating <span class="math notranslate nohighlight">\(f\)</span> with respect to each variable while holding the other constant.</p>
<p>Answer and justification for E3.2.13: The Hessian matrix is given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{H}_g(x, y) = \begin{pmatrix}
-\sin(x) \cos(y) &amp; -\cos(x) \sin(y) \\
-\cos(x) \sin(y) &amp; -\sin(x) \cos(y)
\end{pmatrix}.
\end{split}\]</div>
<p>Answer and justification for E3.2.15: <span class="math notranslate nohighlight">\(\frac{\partial^2 q}{\partial x^2} = 6x\)</span> and <span class="math notranslate nohighlight">\(\frac{\partial^2 q}{\partial y^2} = -6x\)</span>. Adding these gives <span class="math notranslate nohighlight">\(6x - 6x = 0\)</span>, so <span class="math notranslate nohighlight">\(q\)</span> satisfies Laplace’s equation.</p>
<p>Answer and justification for E3.2.17: By the chain rule, the rate of change of temperature experienced by the particle is</p>
<div class="math notranslate nohighlight">
\[
\frac{d}{dt}u(\mathbf{c}(t)) = \nabla u(\mathbf{c}(t))^T \mathbf{c}'(t).
\]</div>
<p>We have <span class="math notranslate nohighlight">\(\nabla u(x, y) = (-2xe^{-x^2 - y^2}, -2ye^{-x^2 - y^2})\)</span> and <span class="math notranslate nohighlight">\(\mathbf{c}'(t) = (2t, 3t^2)\)</span>. Evaluating at <span class="math notranslate nohighlight">\(t = 1\)</span> gives</p>
<div class="math notranslate nohighlight">
\[
\frac{d}{dt}u(\mathbf{c}(1)) = (-2e^{-2}, -2e^{-2})^T (2, 3) = -10e^{-2}.
\]</div>
<p>Answer and justification for E3.2.19: <span class="math notranslate nohighlight">\(\frac{d}{dt} f(\mathbf{g}(t)) = 2t \cos t - t^2 \sin t\)</span>. Justification: <span class="math notranslate nohighlight">\(\nabla f = (y, x)\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{g}'(t) = (2t, -\sin t)\)</span>. Then, <span class="math notranslate nohighlight">\(\frac{d}{dt} f(\mathbf{g}(t)) = \cos t \cdot 2t + t^2 \cdot (-\sin t)\)</span>.</p>
<p>Answer and justification for E3.3.1: <span class="math notranslate nohighlight">\(\nabla f(x_1, x_2) = (2x_1, 4x_2)\)</span>. Setting this equal to zero yields <span class="math notranslate nohighlight">\(2x_1 = 0\)</span> and <span class="math notranslate nohighlight">\(4x_2 = 0\)</span>, which implies <span class="math notranslate nohighlight">\(x_1 = 0\)</span> and <span class="math notranslate nohighlight">\(x_2 = 0\)</span>. Thus, the only point where <span class="math notranslate nohighlight">\(\nabla f(x_1, x_2) = 0\)</span> is <span class="math notranslate nohighlight">\((0, 0)\)</span>.</p>
<p>Answer and justification for E3.3.3: The second directional derivative is given by <span class="math notranslate nohighlight">\(\frac{\partial^2 f(\mathbf{x}_0)}{\partial \mathbf{v}^2} = \mathbf{v}^T \mathbf{H}_f(\mathbf{x}_0) \mathbf{v}\)</span>. We have <span class="math notranslate nohighlight">\(\mathbf{H}_f(x_1, x_2) = \begin{pmatrix}
2 &amp; 2 \\
2 &amp; 2
\end{pmatrix}\)</span>. Thus,
$<span class="math notranslate nohighlight">\(\frac{\partial^2 f(1, 1)}{\partial \mathbf{v}^2} = (\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}})^T \begin{pmatrix}
2 &amp; 2 \\
2 &amp; 2
\end{pmatrix} (\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}) = \frac{1}{2}(2 + 2 + 2 + 2) = 4.\)</span>$</p>
<p>Answer and justification for E3.3.5: The first-order necessary conditions are:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\nabla_{x_1, x_2} L(x_1, x_2, \lambda) &amp;= 0, \\
h(x_1, x_2) &amp;= 0.
\end{align*}\]</div>
<p>Computing the gradients:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{\partial L}{\partial x_1} &amp;= 2x_1 + \lambda = 0, \\
\frac{\partial L}{\partial x_2} &amp;= 2x_2 + \lambda = 0, \\
x_1 + x_2 &amp;= 1.
\end{align*}\]</div>
<p>From the first two equations, we have <span class="math notranslate nohighlight">\(x_1 = x_2 = -\frac{\lambda}{2}\)</span>. Substituting into the third equation:</p>
<div class="math notranslate nohighlight">
\[
-\frac{\lambda}{2} - \frac{\lambda}{2} = 1 \implies \lambda = -1.
\]</div>
<p>Thus, <span class="math notranslate nohighlight">\(x_1 = x_2 = \frac{1}{2}\)</span>, and the only point satisfying the first-order necessary conditions is <span class="math notranslate nohighlight">\((\frac{1}{2}, \frac{1}{2}, -1)\)</span>.</p>
<p>Answer and justification for E3.3.7: The Lagrangian is <span class="math notranslate nohighlight">\(L(x_1, x_2, x_3, \lambda) = x_1^2 + x_2^2 + x_3^2 + \lambda(x_1 + 2x_2 + 3x_3 - 6)\)</span>. The first-order necessary conditions are:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
2x_1 + \lambda &amp;= 0, \\
2x_2 + 2\lambda &amp;= 0, \\
2x_3 + 3\lambda &amp;= 0, \\
x_1 + 2x_2 + 3x_3 &amp;= 6.
\end{align*}\]</div>
<p>From the first three equations, we have <span class="math notranslate nohighlight">\(x_1 = -\frac{\lambda}{2}\)</span>, <span class="math notranslate nohighlight">\(x_2 = -\lambda\)</span>, <span class="math notranslate nohighlight">\(x_3 = -\frac{3\lambda}{2}\)</span>. Substituting into the fourth equation:</p>
<div class="math notranslate nohighlight">
\[
-\frac{\lambda}{2} - 2\lambda - \frac{9\lambda}{2} = 6 \implies -6\lambda = 6 \implies \lambda = -1.
\]</div>
<p>Thus, <span class="math notranslate nohighlight">\(x_1 = \frac{1}{2}\)</span>, <span class="math notranslate nohighlight">\(x_2 = 1\)</span>, <span class="math notranslate nohighlight">\(x_3 = \frac{3}{2}\)</span>, and the only point satisfying the first-order necessary conditions is <span class="math notranslate nohighlight">\((\frac{1}{2}, 1, \frac{3}{2}, -1)\)</span>.</p>
<p>Answer and justification for E3.3.9: The gradient of <span class="math notranslate nohighlight">\(f\)</span> is <span class="math notranslate nohighlight">\(\nabla f(x_1, x_2) = (3x_1^2 - 3x_2^2, -6x_1x_2)\)</span>. At the point <span class="math notranslate nohighlight">\((1, 0)\)</span>, the gradient is <span class="math notranslate nohighlight">\(\nabla f(1, 0) = (3, 0)\)</span>. The directional derivative of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\((1, 0)\)</span> in the direction <span class="math notranslate nohighlight">\(v = (1, 1)\)</span> is <span class="math notranslate nohighlight">\(\nabla f(1, 0)^T v = (3, 0)^T (1, 1) = 3\)</span>. Since this is positive, <span class="math notranslate nohighlight">\(v\)</span> is not a descent direction.</p>
<p>Answer and justification for E3.3.11: The Hessian matrix of <span class="math notranslate nohighlight">\(f\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
H_f(x_1, x_2) = \begin{bmatrix} -2 &amp; 0 \\ 0 &amp; -2 \end{bmatrix}.
\end{split}\]</div>
<p>Therefore, the second directional derivative of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\((0, 0)\)</span> in the direction <span class="math notranslate nohighlight">\(v = (1, 0)\)</span> is <span class="math notranslate nohighlight">\(v^T Hf(0, 0) v = (1, 0) \begin{bmatrix} -2 &amp; 0 \\ 0 &amp; -2 \end{bmatrix} \begin{pmatrix} 1 \\ 0 \end{pmatrix} = -2\)</span>.</p>
<p>Answer and justification for E3.3.13: The Hessian matrix is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
H_f(1,1) = \begin{pmatrix}
6 &amp; -3 \\
-3 &amp; 6
\end{pmatrix}.
\end{split}\]</div>
<p>Justification: Compute the second partial derivatives:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial^2 f}{\partial x^2} = 6x, \quad \frac{\partial^2 f}{\partial y^2} = 6y, \quad \frac{\partial^2 f}{\partial x \partial y} = \frac{\partial^2 f}{\partial y \partial x} = -3.
\]</div>
<p>At <span class="math notranslate nohighlight">\((1,1)\)</span>, these values are <span class="math notranslate nohighlight">\(6\)</span>, <span class="math notranslate nohighlight">\(6\)</span>, and <span class="math notranslate nohighlight">\(-3\)</span>, respectively.</p>
<p>Answer and justification for E3.4.1: The convex combination is:</p>
<div class="math notranslate nohighlight">
\[
(1 - \alpha)(2, 3) + \alpha(4, 5) = 0.7(2, 3) + 0.3(4, 5) = (0.7 \cdot 2 + 0.3 \cdot 4, 0.7 \cdot 3 + 0.3 \cdot 5) = (2.6, 3.6).
\]</div>
<p>Answer and justification for E3.4.3: <span class="math notranslate nohighlight">\(S_1\)</span> and <span class="math notranslate nohighlight">\(S_2\)</span> are halfspaces, which are convex sets. By the lemma in the text, the intersection of convex sets is also convex. Therefore, <span class="math notranslate nohighlight">\(S_1 \cap S_2\)</span> is a convex set.</p>
<p>Answer and justification for E3.4.5: The function <span class="math notranslate nohighlight">\(f\)</span> is a quadratic function with <span class="math notranslate nohighlight">\(P = 2\)</span>, <span class="math notranslate nohighlight">\(q = 2\)</span>, and <span class="math notranslate nohighlight">\(r = 1\)</span>. Since <span class="math notranslate nohighlight">\(P &gt; 0\)</span>, <span class="math notranslate nohighlight">\(f\)</span> is strictly convex. The unique global minimizer is found by setting the gradient to zero:</p>
<div class="math notranslate nohighlight">
\[
\nabla f(x) = 2x + 2 = 0 \implies x^* = -1.
\]</div>
<p>Answer and justification for E3.4.7: The Hessian matrix of <span class="math notranslate nohighlight">\(f\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla^2 f(x, y) = \begin{pmatrix}
2 &amp; 0 \\
0 &amp; 4
\end{pmatrix}.
\end{split}\]</div>
<p>For any <span class="math notranslate nohighlight">\((x, y) \in \mathbb{R}^2\)</span>, we have:</p>
<div class="math notranslate nohighlight">
\[
\nabla^2 f(x, y) \succeq 2I_{2 \times 2},
\]</div>
<p>so <span class="math notranslate nohighlight">\(f\)</span> is strongly convex with <span class="math notranslate nohighlight">\(m = 2\)</span>.</p>
<p>Answer and justification for E3.4.9: <span class="math notranslate nohighlight">\(f\)</span> is not convex. We have <span class="math notranslate nohighlight">\(f''(x) = 12x^2 - 4\)</span>, which is negative for <span class="math notranslate nohighlight">\(x \in (-\frac{1}{\sqrt{3}}, \frac{1}{\sqrt{3}})\)</span>. Since the second derivative is not always nonnegative, <span class="math notranslate nohighlight">\(f\)</span> is not convex.</p>
<p>Answer and justification for E3.4.11: <span class="math notranslate nohighlight">\(f\)</span> is strongly convex. We have <span class="math notranslate nohighlight">\(f''(x) = 2 &gt; 0\)</span>, so <span class="math notranslate nohighlight">\(f\)</span> is 2-strongly convex.</p>
<p>Answer and justification for E3.4.13: <span class="math notranslate nohighlight">\(D\)</span> is convex. To show this, let <span class="math notranslate nohighlight">\((x_1, y_1), (x_2, y_2) \in D\)</span> and <span class="math notranslate nohighlight">\(\alpha \in (0, 1)\)</span>. We need to show that <span class="math notranslate nohighlight">\((1 - \alpha)(x_1, y_1) + \alpha(x_2, y_2) \in D\)</span>. Compute:</p>
<div class="math notranslate nohighlight">
\[
(1 - \alpha)(x_1, y_1) + \alpha(x_2, y_2) = ((1 - \alpha)x_1 + \alpha x_2, (1 - \alpha)y_1 + \alpha y_2).
\]</div>
<p>Now,</p>
<div class="math notranslate nohighlight">
\[
((1 - \alpha)x_1 + \alpha x_2)^2 + ((1 - \alpha)y_1 + \alpha y_2)^2 &lt; (1 - \alpha)(x_1^2 + y_1^2) + \alpha(x_2^2 + y_2^2) &lt; 4.
\]</div>
<p>Hence, <span class="math notranslate nohighlight">\(D\)</span> is convex.</p>
<p>Answer and justification for E3.4.15: <span class="math notranslate nohighlight">\(D\)</span> is not convex. For example, let <span class="math notranslate nohighlight">\((x_1, y_1) = (2, \sqrt{3})\)</span> and <span class="math notranslate nohighlight">\((x_2, y_2) = (2, -\sqrt{3})\)</span>, both of which are in <span class="math notranslate nohighlight">\(D\)</span>. For <span class="math notranslate nohighlight">\(\alpha = \frac{1}{2}\)</span>,</p>
<div class="math notranslate nohighlight">
\[
(1 - \alpha)(2, \sqrt{3}) + \alpha(2, -\sqrt{3}) = \left(2, 0\right).
\]</div>
<p>Now,</p>
<div class="math notranslate nohighlight">
\[
\left(2\right)^2 - \left(0\right)^2 = 4 &gt; 1.
\]</div>
<p>Answer and justification for E3.5.1: The gradient is <span class="math notranslate nohighlight">\(\nabla f(x, y) = (2x, 8y)\)</span>. At <span class="math notranslate nohighlight">\((1, 1)\)</span>, it is <span class="math notranslate nohighlight">\((2, 8)\)</span>. The direction of steepest descent is <span class="math notranslate nohighlight">\(-\nabla f(1, 1) = (-2, -8)\)</span>.</p>
<p>Answer and justification for E3.5.3: <span class="math notranslate nohighlight">\(\nabla f(x) = 3x^2 - 12x + 9\)</span>. At <span class="math notranslate nohighlight">\(x_0 = 0\)</span>, <span class="math notranslate nohighlight">\(\nabla f(0) = 9\)</span>. The first iteration gives <span class="math notranslate nohighlight">\(x_1 = x_0 - \alpha \nabla f(x_0) = 0 - 0.1 \cdot 9 = -0.9\)</span>. At <span class="math notranslate nohighlight">\(x_1 = -0.9\)</span>, <span class="math notranslate nohighlight">\(\nabla f(-0.9) = 3 \cdot (-0.9)^2 - 12 \cdot (-0.9) + 9 = 2.43 + 10.8 + 9 = 22.23\)</span>. The second iteration gives <span class="math notranslate nohighlight">\(x_2 = x_1 - \alpha \nabla f(x_1) = -0.9 - 0.1 \cdot 22.23 = -3.123\)</span>.</p>
<p>Answer and justification for E3.5.5: We have <span class="math notranslate nohighlight">\(\nabla f(x) = 2x\)</span>, so <span class="math notranslate nohighlight">\(\nabla f(2) = 4\)</span>. Thus, the gradient descent update is <span class="math notranslate nohighlight">\(x_1 = x_0 - \alpha \nabla f(x_0) = 2 - 0.1 \cdot 4 = 1.6\)</span>.</p>
<p>Answer and justification for E3.5.7: <span class="math notranslate nohighlight">\(f''(x) = 4\)</span> for all <span class="math notranslate nohighlight">\(x \in \mathbb{R}\)</span>. Therefore, <span class="math notranslate nohighlight">\(f''(x) \geq 4\)</span> for all <span class="math notranslate nohighlight">\(x \in \mathbb{R}\)</span>, which implies that <span class="math notranslate nohighlight">\(f\)</span> is <span class="math notranslate nohighlight">\(4\)</span>-strongly convex.</p>
<p>Answer and justification for E3.5.9: No. We have <span class="math notranslate nohighlight">\(f''(x) = 12x^2\)</span>, which can be arbitrarily large as <span class="math notranslate nohighlight">\(x\)</span> increases. Thus, there is no constant <span class="math notranslate nohighlight">\(L\)</span> such that <span class="math notranslate nohighlight">\(-L \le f''(x) \le L\)</span> for all <span class="math notranslate nohighlight">\(x \in \mathbb{R}\)</span>.</p>
<p>Answer and justification for E3.5.11: We have <span class="math notranslate nohighlight">\(f''(x) = 2\)</span> for all <span class="math notranslate nohighlight">\(x \in \mathbb{R}\)</span>. Thus, we can take <span class="math notranslate nohighlight">\(m = 2\)</span>, and we have <span class="math notranslate nohighlight">\(f''(x) \ge 2\)</span> for all <span class="math notranslate nohighlight">\(x \in \mathbb{R}\)</span>, which is the condition for <span class="math notranslate nohighlight">\(m\)</span>-strong convexity.</p>
<p>Answer and justification for E3.5.13: The gradient is <span class="math notranslate nohighlight">\(\nabla f(x, y) = (2x, 2y)\)</span>. At <span class="math notranslate nohighlight">\((1, 1)\)</span>, it is <span class="math notranslate nohighlight">\((2, 2)\)</span>. The first update is <span class="math notranslate nohighlight">\((0.8, 0.8)\)</span>. The gradient at <span class="math notranslate nohighlight">\((0.8, 0.8)\)</span> is <span class="math notranslate nohighlight">\((1.6, 1.6)\)</span>. The second update is <span class="math notranslate nohighlight">\((0.64, 0.64)\)</span>.</p>
<p>Answer and justification for E3.6.1: The log-odds is given by <span class="math notranslate nohighlight">\(\log \frac{p}{1-p} = \log \frac{0.25}{0.75} = \log \frac{1}{3} = -\log 3\)</span>.</p>
<p>Answer and justification for E3.6.3:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\alpha}^T \mathbf{x} = (-0.2 \cdot 1) + (0.4 \cdot 3) = -0.2 + 1.2 = 1.0
\]</div>
<div class="math notranslate nohighlight">
\[
p(\mathbf{x}; \boldsymbol{\alpha}) = \sigma(1.0) = \frac{1}{1 + e^{-1}} \approx 0.731
\]</div>
<p>Answer and justification for E3.6.5: By the quotient rule,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\sigma'(z) &amp;= \frac{e^{-z}}{(1 + e^{-z})^2} = \frac{1}{1 + e^{-z}} \cdot \frac{e^{-z}}{1 + e^{-z}} \\
&amp;= \sigma(z) (1 - \sigma(z)).
\end{align*}\]</div>
<p>Answer and justification for E3.6.7: We have <span class="math notranslate nohighlight">\(b_1 - \sigma(\boldsymbol{\alpha}_1^T \mathbf{x}) \approx 0.05\)</span>, <span class="math notranslate nohighlight">\(b_2 - \sigma(\boldsymbol{\alpha}_2^T \mathbf{x}) \approx -0.73\)</span>, <span class="math notranslate nohighlight">\(b_3 - \sigma(\boldsymbol{\alpha}_3^T \mathbf{x}) \approx 0.73\)</span>. Therefore,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\nabla \ell(\mathbf{x}; A, b) &amp;= -\frac{1}{3} \sum_{i=1}^3 (b_i - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})) \boldsymbol{\alpha}_i \\
&amp;\approx -\frac{1}{3} \{(0.05)(1, 2) + (-0.73)(-1, 1) + 0.73(0, -1)\}.
\end{align*}\]</div>
<p>Answer and justification for E3.6.9: We have <span class="math notranslate nohighlight">\(b_1 - \sigma(\boldsymbol{\alpha}_1^T \mathbf{x}^0) = 1 - \sigma(0) = 0.5\)</span>, <span class="math notranslate nohighlight">\(b_2 - \sigma(\boldsymbol{\alpha}_2^T \mathbf{x}^0) = -0.5\)</span>, <span class="math notranslate nohighlight">\(b_3 - \sigma(\boldsymbol{\alpha}_3^T \mathbf{x}^0) = 0.5\)</span>. Therefore,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{x}^1 &amp;= \mathbf{x}^0 - \beta \nabla \ell(\mathbf{x}^0; A, \mathbf{b}) \\
&amp;= (0, 0) + \frac{0.1}{3} \{0.5(1, 2) + (-0.5)(-1, 1) + 0.5(0, -1)\} \\
&amp;= (0.1, 0.05).
\end{align*}\]</div>
</section>
<section id="learning-outcomes">
<h3><span class="section-number">3.8.1.5. </span>Learning outcomes<a class="headerlink" href="#learning-outcomes" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Define and calculate partial derivatives and gradients for functions of several variables.</p></li>
<li><p>Compute second-order partial derivatives and construct the Hessian matrix for twice continuously differentiable functions.</p></li>
<li><p>Apply the Chain Rule to compute derivatives of composite functions of several variables.</p></li>
<li><p>State and prove the multivariable version of the Mean Value Theorem using the Chain Rule.</p></li>
<li><p>Calculate gradients and Hessians for affine and quadratic functions of several variables.</p></li>
<li><p>Define global and local minimizers for unconstrained optimization problems.</p></li>
<li><p>Derive the first-order necessary conditions for a local minimizer using the gradient.</p></li>
<li><p>Explain the concept of a descent direction and its relation to the gradient.</p></li>
<li><p>Explain the concept of directional derivatives and compute directional derivatives using the gradient.</p></li>
<li><p>State the second-order necessary and sufficient conditions for a local minimizer using the Hessian matrix.</p></li>
<li><p>Compute the gradient and Hessian matrix for a given multivariable function.</p></li>
<li><p>Formulate optimization problems with equality constraints.</p></li>
<li><p>Apply the method of Lagrange multipliers to derive the first-order necessary conditions for a constrained local minimizer.</p></li>
<li><p>Verify the second-order sufficient conditions for a constrained local minimizer using the Hessian matrix and Lagrange multipliers.</p></li>
<li><p>Analyze the regularity condition in the context of constrained optimization problems.</p></li>
<li><p>Solve a constrained optimization problem by finding points that satisfy the first-order necessary conditions and checking the second-order sufficient conditions.</p></li>
<li><p>Define convex sets and convex functions, and provide examples of each.</p></li>
<li><p>Identify operations that preserve convexity of sets and functions.</p></li>
<li><p>Characterize convex functions using the first-order convexity condition based on the gradient.</p></li>
<li><p>Determine the convexity of a function using the second-order convexity condition based on the Hessian matrix.</p></li>
<li><p>Explain the relationship between convexity and optimization, particularly how local minimizers of convex functions are also global minimizers.</p></li>
<li><p>State and prove the first-order optimality condition for convex functions on R^d and on convex sets.</p></li>
<li><p>Define strong convexity and its implications for the existence and uniqueness of global minimizers.</p></li>
<li><p>Analyze the convexity and strong convexity of quadratic functions and least-squares objectives.</p></li>
<li><p>Apply the concept of convexity to solve optimization problems, such as finding the projection of a point onto a convex set.</p></li>
<li><p>Define gradient descent and explain its motivation as a numerical optimization method.</p></li>
<li><p>Prove that the negative gradient is the steepest descent direction for a continuously differentiable function.</p></li>
<li><p>Analyze the convergence of gradient descent for smooth functions, proving that it produces a sequence of points with decreasing objective values and vanishing gradients.</p></li>
<li><p>Derive the convergence rate of gradient descent for smooth functions in terms of the number of iterations.</p></li>
<li><p>Define strong convexity for twice continuously differentiable functions and relate it to the function value and gradient.</p></li>
<li><p>Prove faster convergence rates for gradient descent when applied to smooth and strongly convex functions, showing exponential convergence to the global minimum.</p></li>
<li><p>Implement gradient descent in Python and apply it to simple examples to illustrate the theoretical convergence results.</p></li>
<li><p>Explain the role of the sigmoid function in transforming a linear function of features into a probability.</p></li>
<li><p>Derive the gradient and Hessian of the logistic regression objective function (cross-entropy loss).</p></li>
<li><p>Prove that the logistic regression objective function is convex and smooth.</p></li>
<li><p>Implement gradient descent to minimize the logistic regression objective function in Python.</p></li>
<li><p>Apply logistic regression to real-world datasets.</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\aleph\)</span></p>
</section>
</section>
<section id="additional-sections">
<h2><span class="section-number">3.8.2. </span>Additional sections<a class="headerlink" href="#additional-sections" title="Link to this heading">#</a></h2>
<section id="logistic-regression-illustration-of-convergence-result">
<h3><span class="section-number">3.8.2.1. </span>Logistic regression: illustration of convergence result<a class="headerlink" href="#logistic-regression-illustration-of-convergence-result" title="Link to this heading">#</a></h3>
<p>We return to our proof of convergence for smooth functions using a special case of logistic regression. We first define the functions <span class="math notranslate nohighlight">\(\hat{f}\)</span>, <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> and <span class="math notranslate nohighlight">\(\frac{\partial}{\partial x}\mathcal{L}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fhat</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">))</span> <span class="p">)</span>

<span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">):</span> 
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="n">b</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">fhat</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">))</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">fhat</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">b</span> <span class="o">-</span> <span class="n">fhat</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">))</span><span class="o">*</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We illustrate GD on a random dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">seed</span> <span class="o">=</span> <span class="mi">535</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">rng</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We plot the upper and lower bounds in the <em>Quadratic Bound for Smooth Functions</em> around <span class="math notranslate nohighlight">\(x = x_0\)</span>. It turns out we can take <span class="math notranslate nohighlight">\(L=1\)</span> because all features are uniformly random between <span class="math notranslate nohighlight">\(-1\)</span> and <span class="math notranslate nohighlight">\(1\)</span>. Observe that minimizing the upper quadratic bound leads to a decrease in <span class="math notranslate nohighlight">\(\mathcal{L}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x0</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.3</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x0</span><span class="o">-</span><span class="mf">0.05</span><span class="p">,</span><span class="n">x0</span><span class="o">+</span><span class="mf">0.05</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="n">upper</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x0</span><span class="p">)</span><span class="o">*</span><span class="n">grad</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="n">lower</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x0</span><span class="p">)</span><span class="o">*</span><span class="n">grad</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">upper</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;upper&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;lower&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/324fe7586bd9a9025def6d4dad22a04d4771badbb9de6c87d2f507e57fba625f.png" src="../../_images/324fe7586bd9a9025def6d4dad22a04d4771badbb9de6c87d2f507e57fba625f.png" />
</div>
</div>
</section>
<section id="logistic-regression-another-dataset">
<h3><span class="section-number">3.8.2.2. </span>Logistic regression: another dataset<a class="headerlink" href="#logistic-regression-another-dataset" title="Link to this heading">#</a></h3>
<p>Recall that to run gradient descent, we first implement a function computing a descent update. It takes as input a function <code class="docutils literal notranslate"><span class="pre">grad_fn</span></code> computing the gradient itself, as well as a current iterate and a step size. We now also feed a dataset as additional input.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">):</span> 
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">desc_update_for_logreg</span><span class="p">(</span><span class="n">grad_fn</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">curr_x</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
    <span class="n">gradient</span> <span class="o">=</span> <span class="n">grad_fn</span><span class="p">(</span><span class="n">curr_x</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">curr_x</span> <span class="o">-</span> <span class="n">beta</span><span class="o">*</span><span class="n">gradient</span>

<span class="k">def</span> <span class="nf">gd_for_logreg</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">grad_fn</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">init_x</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">niters</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e5</span><span class="p">)):</span>
    <span class="n">curr_x</span> <span class="o">=</span> <span class="n">init_x</span>
    
    <span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">niters</span><span class="p">):</span>
        <span class="n">curr_x</span> <span class="o">=</span> <span class="n">desc_update_for_logreg</span><span class="p">(</span><span class="n">grad_fn</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">curr_x</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">curr_x</span>

<span class="k">def</span> <span class="nf">pred_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">):</span> 
    <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">A</span> <span class="o">@</span> <span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span> 
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="n">b</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">pred_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">))</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">pred_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">)))</span>

<span class="k">def</span> <span class="nf">grad_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">A</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">pred_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">))</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">stepsize_for_logreg</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">L</span> <span class="o">=</span> <span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">A</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="n">L</span>
</pre></div>
</div>
</div>
</div>
<p>We analyze with a simple dataset from UC Berkeley’s <a class="reference external" href="http://www.ds100.org">DS100</a> course. The file <code class="docutils literal notranslate"><span class="pre">lebron.csv</span></code> is available <a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/tree/main/utils/datasets">here</a>. Quoting a previous version of the course’s textbook:</p>
<blockquote>
<div><p>In basketball, players score by shooting a ball through a hoop. One such player, LeBron James, is widely considered one of the best basketball players ever for his incredible ability to score. LeBron plays in the National Basketball Association (NBA), the United States’s premier basketball league. We’ve collected a dataset of all of LeBron’s attempts in the 2017 NBA Playoff Games using the NBA statistics website (<a class="reference external" href="https://stats.nba.com/">https://stats.nba.com/</a>).</p>
</div></blockquote>
<p>We first load the data and look at its summary.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;lebron.csv&#39;</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>game_date</th>
      <th>minute</th>
      <th>opponent</th>
      <th>action_type</th>
      <th>shot_type</th>
      <th>shot_distance</th>
      <th>shot_made</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>20170415</td>
      <td>10</td>
      <td>IND</td>
      <td>Driving Layup Shot</td>
      <td>2PT Field Goal</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>20170415</td>
      <td>11</td>
      <td>IND</td>
      <td>Driving Layup Shot</td>
      <td>2PT Field Goal</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>20170415</td>
      <td>14</td>
      <td>IND</td>
      <td>Layup Shot</td>
      <td>2PT Field Goal</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>20170415</td>
      <td>15</td>
      <td>IND</td>
      <td>Driving Layup Shot</td>
      <td>2PT Field Goal</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>20170415</td>
      <td>18</td>
      <td>IND</td>
      <td>Alley Oop Dunk Shot</td>
      <td>2PT Field Goal</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>game_date</th>
      <th>minute</th>
      <th>shot_distance</th>
      <th>shot_made</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>3.840000e+02</td>
      <td>384.00000</td>
      <td>384.000000</td>
      <td>384.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>2.017052e+07</td>
      <td>24.40625</td>
      <td>10.695312</td>
      <td>0.565104</td>
    </tr>
    <tr>
      <th>std</th>
      <td>6.948501e+01</td>
      <td>13.67304</td>
      <td>10.547586</td>
      <td>0.496390</td>
    </tr>
    <tr>
      <th>min</th>
      <td>2.017042e+07</td>
      <td>1.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>2.017050e+07</td>
      <td>13.00000</td>
      <td>1.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>2.017052e+07</td>
      <td>25.00000</td>
      <td>6.500000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>2.017060e+07</td>
      <td>35.00000</td>
      <td>23.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>2.017061e+07</td>
      <td>48.00000</td>
      <td>31.000000</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The two columns we will be interested in are <code class="docutils literal notranslate"><span class="pre">shot_distance</span></code> (LeBron’s distance from the basket when the shot was attempted (ft)) and <code class="docutils literal notranslate"><span class="pre">shot_made</span></code> (0 if the shot missed, 1 if the shot went in). As the summary table above indicates, the average distance was <code class="docutils literal notranslate"><span class="pre">10.6953</span></code> and the frequency of shots made was <code class="docutils literal notranslate"><span class="pre">0.565104</span></code>. We extract those two columns and display them on a scatter plot.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">feature</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;shot_distance&#39;</span><span class="p">]</span>
<span class="n">label</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;shot_made&#39;</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/ccf5a77835e56e6a43556d4f958a46f41dcda89b707e8c6937654150eafed13d.png" src="../../_images/ccf5a77835e56e6a43556d4f958a46f41dcda89b707e8c6937654150eafed13d.png" />
</div>
</div>
<p>As you can see, this kind of data is hard to vizualize because of the superposition of points with the same <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>-values. One trick is to jiggle the <span class="math notranslate nohighlight">\(y\)</span>’s a little bit by adding Gaussian noise. We do this next and plot again.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">seed</span> <span class="o">=</span> <span class="mi">535</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="n">label_jitter</span> <span class="o">=</span> <span class="n">label</span> <span class="o">+</span> <span class="mf">0.05</span><span class="o">*</span><span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">label</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">label_jitter</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/0108a98a440f644ac9feb81bb3700cbea2ffcdf8fa56a7d2f355b1d652ae7cff.png" src="../../_images/0108a98a440f644ac9feb81bb3700cbea2ffcdf8fa56a7d2f355b1d652ae7cff.png" />
</div>
</div>
<p>We apply GD to logistic regression. We first construct the data matrices <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b}\)</span>. To allow an affine function of the features, we add a column of <span class="math notranslate nohighlight">\(1\)</span>’s as we have done before.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">label</span><span class="p">)),</span><span class="n">feature</span><span class="p">),</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">label</span>
</pre></div>
</div>
</div>
</div>
<p>We run GD starting from <span class="math notranslate nohighlight">\((0,0)\)</span> with a step size computed from the smoothness of the objective as above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stepsize</span> <span class="o">=</span> <span class="n">stepsize_for_logreg</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">stepsize</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.017671625306319678
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">init_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">best_x</span> <span class="o">=</span> <span class="n">gd_for_logreg</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">grad_fn</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">init_x</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="n">stepsize</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">best_x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[ 0.90959003 -0.05890828]
</pre></div>
</div>
</div>
</div>
<p>Finally we plot the results.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">feature</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">feature</span><span class="p">),</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">feature_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grid</span><span class="p">)),</span><span class="n">grid</span><span class="p">),</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">predict_grid</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">feature_grid</span> <span class="o">@</span> <span class="n">best_x</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">label_jitter</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span><span class="n">predict_grid</span><span class="p">,</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/e630ab57029ca45289fcf7e97e68879d9893d32390e62c3bf04d6b2295b8c0b2.png" src="../../_images/e630ab57029ca45289fcf7e97e68879d9893d32390e62c3bf04d6b2295b8c0b2.png" />
</div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chap03_opt/supp"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../exercises/roch-mmids-opt-exercises.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">3.7. </span>Exercises</p>
      </div>
    </a>
    <a class="right-next"
       href="../../chap04_svd/00_intro/roch-mmids-svd-intro.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">4. </span>Singular value decomposition</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quizzes-solutions-code-etc">3.8.1. Quizzes, solutions, code, etc.</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#just-the-code">3.8.1.1. Just the code</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#self-assessment-quizzes">3.8.1.2. Self-assessment quizzes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#auto-quizzes">3.8.1.3. Auto-quizzes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#solutions-to-odd-numbered-warm-up-exercises">3.8.1.4. Solutions to odd-numbered warm-up exercises</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-outcomes">3.8.1.5. Learning outcomes</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-sections">3.8.2. Additional sections</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-illustration-of-convergence-result">3.8.2.1. Logistic regression: illustration of convergence result</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-another-dataset">3.8.2.2. Logistic regression: another dataset</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Sebastien Roch
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>