<!DOCTYPE html>
<html>
<head>
    <title>Self-Assessment Questions for Section 4.3</title>
    <style>
        .question {
            margin-bottom: 20px;
        }
        .options {
            list-style-type: none;
            padding: 0;
        }
        .options li {
            margin-bottom: 10px;
        }
        .feedback {
            margin-top: 10px;
            font-weight: bold;
        }
    </style>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <h1>Self-Assessment Questions for Section 4.3</h1>

    <p>Before moving on, you may wish to quickly test your knowledge by going over the following review questions.</p>

    <div id="quiz-container">
        <div class="question">
            <p>Let \( \alpha_1, \dots, \alpha_n \) be data points in \( \mathbb{R}^m \). What is the objective of the best approximating subspace problem?</p>
            <ul class="options">
                <li><label><input type="radio" name="q0" value="0"> a) To find a linear subspace \( Z \) of \( \mathbb{R}^m \) that minimizes the sum of the distances between the \( \alpha_i \)'s and \( Z \).</label></li>
                <li><label><input type="radio" name="q0" value="1"> b) To find a linear subspace \( Z \) of \( \mathbb{R}^m \) that minimizes the sum of the squared distances between the \( \alpha_i \)'s and their orthogonal projections onto \( Z \).</label></li>
                <li><label><input type="radio" name="q0" value="2"> c) To find a linear subspace \( Z \) of \( \mathbb{R}^m \) that maximizes the sum of the squared norms of the orthogonal projections of the \( \alpha_i \)'s onto \( Z \).</label></li>
                <li><label><input type="radio" name="q0" value="3"> d) Both b and c.</label></li>
            </ul>
            <div class="feedback" id="feedback0"></div>
        </div>

        <div class="question">
            <p>Consider the data points \( \alpha_1 = (-2,2) \) and \( \alpha_2 = (3,-3) \). For \( k=1 \), what is the solution of the best approximating subspace problem?</p>
            <ul class="options">
                <li><label><input type="radio" name="q1" value="0"> a) \( Z = \{(x,y) \in \mathbb{R}^2 : y = x\} \)</label></li>
                <li><label><input type="radio" name="q1" value="1"> b) \( Z = \{(x,y) \in \mathbb{R}^2 : y = -x\} \)</label></li>
                <li><label><input type="radio" name="q1" value="2"> c) \( Z = \{(x,y) \in \mathbb{R}^2 : y = x+1\} \)</label></li>
                <li><label><input type="radio" name="q1" value="3"> d) \( Z = \{(x,y) \in \mathbb{R}^2 : y = x-1\} \)</label></li>
            </ul>
            <div class="feedback" id="feedback1"></div>
        </div>

        <div class="question">
            <p>Let \( A \in \mathbb{R}^{n \times m} \) with rows \( \alpha_i^T \), \( i=1,\dots,n \). A solution to the best approximating subspace problem is obtained by solving:</p>
            <ul class="options">
                <li><label><input type="radio" name="q2" value="0"> a) \( \max_{w_1,\dots,w_k} \sum_{j=1}^k \| Aw_j \|^2 \) over all orthonormal lists \( w_1,\dots,w_k \) of length \( k \).</label></li>
                <li><label><input type="radio" name="q2" value="1"> b) \( \min_{w_1,\dots,w_k} \sum_{j=1}^k \| Aw_j \|^2 \) over all orthonormal lists \( w_1,\dots,w_k \) of length \( k \).</label></li>
                <li><label><input type="radio" name="q2" value="2"> c) \( \max_{w_1,\dots,w_k} \sum_{j=1}^k \| Aw_j \| \) over all orthonormal lists \( w_1,\dots,w_k \) of length \( k \).</label></li>
                <li><label><input type="radio" name="q2" value="3"> d) \( \min_{w_1,\dots,w_k} \sum_{j=1}^k \| Aw_j \| \) over all orthonormal lists \( w_1,\dots,w_k \) of length \( k \).</label></li>
            </ul>
            <div class="feedback" id="feedback2"></div>
        </div>

        <div class="question">
            <p>What is the significance of the leading right singular vectors in the SVD?</p>
            <ul class="options">
                <li><label><input type="radio" name="q3" value="0"> a) They form an orthonormal basis for the column space of \( A \).</label></li>
                <li><label><input type="radio" name="q3" value="1"> b) They form an orthonormal basis for the row space of \( A \).</label></li>
                <li><label><input type="radio" name="q3" value="2"> c) They span the subspace that best approximates the data points.</label></li>
                <li><label><input type="radio" name="q3" value="3"> d) They maximize the sum of the Euclidean distances from the data points.</label></li>
            </ul>
            <div class="feedback" id="feedback3"></div>
        </div>

        <div class="question">
            <p>In the singular value decomposition \( A = \sum_{j=1}^r \sigma_j u_j v_j^T \), the vectors \( u_j \) are called:</p>
            <ul class="options">
                <li><label><input type="radio" name="q4" value="0"> a) Left singular vectors</label></li>
                <li><label><input type="radio" name="q4" value="1"> b) Right singular vectors</label></li>
                <li><label><input type="radio" name="q4" value="2"> c) Singular values</label></li>
                <li><label><input type="radio" name="q4" value="3"> d) None of the above</label></li>
            </ul>
            <div class="feedback" id="feedback4"></div>
        </div>

        <div class="question">
            <p>Which of the following is true about the SVD of a matrix \( A \)?</p>
            <ul class="options">
                <li><label><input type="radio" name="q5" value="0"> a) The SVD of \( A \) is unique.</label></li>
                <li><label><input type="radio" name="q5" value="1"> b) The left singular vectors of \( A \) are the eigenvectors of \( A^TA \).</label></li>
                <li><label><input type="radio" name="q5" value="2"> c) The right singular vectors of \( A \) are the eigenvectors of \( AA^T \).</label></li>
                <li><label><input type="radio" name="q5" value="3"> d) Both b and c.</label></li>
            </ul>
            <div class="feedback" id="feedback5"></div>
        </div>

        <div class="question">
            <p>What is the difference between a compact SVD and a full SVD?</p>
            <ul class="options">
                <li><label><input type="radio" name="q6" value="0"> a) A compact SVD only includes the non-zero singular values, while a full SVD includes all singular values, including zeros.</label></li>
                <li><label><input type="radio" name="q6" value="1"> b) A compact SVD has orthogonal matrices \( U \) and \( V \), while a full SVD has orthonormal matrices \( U \) and \( V \).</label></li>
                <li><label><input type="radio" name="q6" value="2"> c) A compact SVD is unique, while a full SVD is not.</label></li>
                <li><label><input type="radio" name="q6" value="3"> d) A compact SVD can be computed for any matrix, while a full SVD can only be computed for square matrices.</label></li>
            </ul>
            <div class="feedback" id="feedback6"></div>
        </div>

        <div class="question">
            <p>Let \( A = U \Sigma V^T \) be an SVD of \( A \). Which of the following is true?</p>
            <ul class="options">
                <li><label><input type="radio" name="q7" value="0"> a) \( A v_i = \sigma_i u_i \) for all \( i \).</label></li>
                <li><label><input type="radio" name="q7" value="1"> b) \( A^T u_i = \sigma_i v_i \) for all \( i \).</label></li>
                <li><label><input type="radio" name="q7" value="2"> c) \( \|Av_i\| = \sigma_i \) for all \( i \).</label></li>
                <li><label><input type="radio" name="q7" value="3"> d) All of the above.</label></li>
            </ul>
            <div class="feedback" id="feedback7"></div>
        </div>

        <div class="question">
            <p>Which of the following is NOT a property of the matrix \( A^TA \) in the context of the SVD?</p>
            <ul class="options">
                <li><label><input type="radio" name="q8" value="0"> a) \( A^T A \) is symmetric.</label></li>
                <li><label><input type="radio" name="q8" value="1"> b) \( A^T A \) is positive semidefinite.</label></li>
                <li><label><input type="radio" name="q8" value="2"> c) The eigenvectors of \( A^T A \) are the right singular vectors of \( A \).</label></li>
                <li><label><input type="radio" name="q8" value="3"> d) The eigenvalues of \( A^T A \) are the squares of the singular values of \( A \).</label></li>
            </ul>
            <div class="feedback" id="feedback8"></div>
        </div>

        <div class="question">
            <p>The columns of \( U \) in the compact SVD form an orthonormal basis for:</p>
            <ul class="options">
                <li><label><input type="radio" name="q9" value="0"> a) \( \mathrm{col}(A) \)</label></li>
                <li><label><input type="radio" name="q9" value="1"> b) \( \mathrm{row}(A) \)</label></li>
                <li><label><input type="radio" name="q9" value="2"> c) \( \mathrm{null}(A) \)</label></li>
                <li><label><input type="radio" name="q9" value="3"> d) \( \mathrm{null}(A^T) \)</label></li>
            </ul>
            <div class="feedback" id="feedback9"></div>
        </div>

        <div class="question">
            <p>Let \( A \in \mathbb{R}^{n \times m} \) have compact SVD \( A = U_1 \Sigma_1 V_1^T \). To obtain a full SVD, we complete \( U_1 \) to an orthonormal basis \( U = (U_1 \ U_2) \) of \( \mathbb{R}^n \) and \( V_1 \) to an orthonormal basis \( V = (V_1 \ V_2) \) of \( \mathbb{R}^m \). Then the columns of \( U_2 \) form an orthonormal basis of:</p>
            <ul class="options">
                <li><label><input type="radio" name="q10" value="0"> a) \( \mathrm{col}(A) \)</label></li>
                <li><label><input type="radio" name="q10" value="1"> b) \( \mathrm{row}(A) \)</label></li>
                <li><label><input type="radio" name="q10" value="2"> c) \( \mathrm{null}(A) \)</label></li>
                <li><label><input type="radio" name="q10" value="3"> d) \( \mathrm{null}(A^T) \)</label></li>
            </ul>
            <div class="feedback" id="feedback10"></div>
        </div>
    </div>

    <script>
        document.querySelectorAll('.options').forEach((optionsList, index) => {
            const correctOptions = [3, 1, 0, 2, 0, 3, 0, 3, 2, 0, 3];

            optionsList.addEventListener('change', (event) => {
                const selectedOption = parseInt(event.target.value);
                const feedbackDiv = document.getElementById('feedback' + index);
                let feedbackText = '';

                if (selectedOption === correctOptions[index]) {
                    feedbackText = "Excellent! " + getCorrectFeedback(index);
                } else {
                    feedbackText = "Try again.";
                }

                feedbackDiv.innerHTML = feedbackText;
                feedbackDiv.style.color = selectedOption === correctOptions[index] ? 'green' : 'red';

                MathJax.typesetPromise([feedbackDiv]);
            });
        });

        function getCorrectFeedback(index) {
            switch (index) {
                case 0:
                    return "The text defines the best approximating subspace problem as minimizing the sum of squared distances between the data points and their projections onto the subspace, and it also states a lemma that this problem is equivalent to maximizing the sum of squared norms of the projections.";
                case 1:
                    return "By symmetry, the best approximating line must pass through the origin and bisect the angle between the two points. This is the line \\( y=-x \\).";
                case 2:
                    return "The text states this result in Lemma (Best Subspace in Matrix Form).";
                case 3:
                    return "The text indicates that the leading right singular vectors span the subspace that best approximates the data points.";
                case 4:
                    return "The text states: 'Here the \\( u_j \\)\\'s are the columns of \\( U \\) and are referred to as left singular vectors.'";
                case 5:
                    return "The SVD is not unique in general. The other two statements are true and are mentioned in the text.";
                case 6:
                    return "The text defines the compact and full SVDs and explains that the compact SVD only includes the non-zero singular values.";
                case 7:
                    return "This is a lemma stated in the text.";
                case 8:
                    return "The eigenvectors of \\( A^T A \\) are the right singular vectors of \\( A \\). The left singular vectors of \\( A \\) are the eigenvectors of \\( AA^T \\). The other statements are true.";
                case 9:
                    return "The text states in the SVD and Rank Lemma: 'the columns of \\( U \\) form an orthonormal basis of \\( \\mathrm{col}(A) \\).'";
                case 10:
                    return "The text states: 'Because \\( \\mathrm{col}(A)^\\perp = \\mathrm{null}(A^T) \\), the columns of \\( U_2 \\) form an orthonormal basis of \\( \\mathrm{null}(A^T) \\).'";
                default:
                    return "";
            }
        }
    </script>
</body>
</html>
