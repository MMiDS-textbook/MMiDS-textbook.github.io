
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>1.4. Some observations about high-dimensional data &#8212; MMiDS Textbook</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=87e54e7c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-P5E8DW088F"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-P5E8DW088F');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-P5E8DW088F');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chap01_intro/04_highdim/roch-mmids-intro-highdim';</script>
    <link rel="canonical" href="https://mmids-textbook.github.io/chap01_intro/04_highdim/roch-mmids-intro-highdim.html" />
    <link rel="icon" href="https://mmids-textbook.github.io/favicon-32x32.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="1.5. Exercises" href="../exercises/roch-mmids-intro-exercises.html" />
    <link rel="prev" title="1.3. Clustering: an objective, an algorithm and a guarantee" href="../03_clustering/roch-mmids-intro-clustering.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/mmids-cover-small.jpg" class="logo__image only-light" alt="MMiDS Textbook - Home"/>
    <script>document.write(`<img src="../../_static/mmids-cover-small.jpg" class="logo__image only-dark" alt="MMiDS Textbook - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../index.html">
                    <b>MATHEMATICAL METHODS in DATA SCIENCE (with Python)</b>
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../00_intro/roch-mmids-intro-intro.html">1. Introduction: a first data science problem</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../01_motiv/roch-mmids-intro-motiv.html">1.1. Motivating example: identifying penguin species</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02_review/roch-mmids-intro-review.html">1.2. Background: quick refresher of matrix algebra, differential calculus, and elementary probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03_clustering/roch-mmids-intro-clustering.html">1.3. Clustering: an objective, an algorithm and a guarantee</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">1.4. Some observations about high-dimensional data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../exercises/roch-mmids-intro-exercises.html">1.5. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../05_adv/roch-mmids-intro-adv.html">1.6. Online suppplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap02_ls/00_intro/roch-mmids-ls-intro.html">2. Least squares: geometric, algebraic, and numerical aspects</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/01_motiv/roch-mmids-ls-motiv.html">2.1. Motivating example: predicting sales</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/02_subspaces/roch-mmids-ls-spaces.html">2.2. Background: review of vector spaces and matrix inverses</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/03_orthog/roch-mmids-ls-orthog.html">2.3. Geometry of least squares: the orthogonal projection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/04_qr/roch-mmids-ls-qr.html">2.4. QR decomposition and Householder transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/05_regression/roch-mmids-ls-regression.html">2.5. Application: regression analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/exercises/roch-mmids-ls-exercises.html">2.6. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/06_adv/roch-mmids-ls-adv.html">2.7. Online suppplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap06_opt/00_intro/roch-mmids-opt-intro.html">3. Optimization theory and algorithms</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap06_opt/01_motiv/roch-mmids-opt-motiv.html">3.1. Motivating example:  analyzing customer satisfaction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap06_opt/02_several/roch-mmids-opt-several.html">3.2. Background: review of differentiable functions of several variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap06_opt/03_optimality/roch-mmids-opt-optimality.html">3.3. Optimality conditions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap06_opt/03part2_convexity/roch-mmids-opt-convexity.html">3.4. Convexity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap06_opt/04_gd/roch-mmids-opt-gd.html">3.5. Gradient descent and its convergence analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap06_opt/04part2_logistic/roch-mmids-opt-logistic.html">3.6. Application: logistic regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap06_opt/exercises/roch-mmids-opt-exercises.html">3.7. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap06_opt/06_adv/roch-mmids-opt-adv.html">3.8. Online suppplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap03_svd/00_intro/roch-mmids-svd-intro.html">4. Singular value decomposition</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap03_svd/01_motiv/roch-mmids-svd-motiv.html">4.1. Motivating example: visualizing viral evolution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap03_svd/02_spectral/roch-mmids-svd-spectral.html">4.2. Background: review of matrix rank  and spectral decomposition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap03_svd/03_svd/roch-mmids-svd-svd.html">4.3. Approximating subspaces and the SVD</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap03_svd/04_power/roch-mmids-svd-power.html">4.4. Power iteration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap03_svd/05_pca/roch-mmids-svd-pca.html">4.5. Application: principal components analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap03_svd/06_further/roch-mmids-svd-further.html">4.6. Further applications of the SVD: low-rank approximations and ridge regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap03_svd/exercises/roch-mmids-svd-exercises.html">4.7. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap03_svd/07_adv/roch-mmids-svd-adv.html">4.8. Online suppplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap04_specgraph/00_intro/roch-mmids-specgraph-intro.html">5. Spectral graph theory</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_specgraph/01_motiv/roch-mmids-specgraph-motiv.html">5.1. Motivating example: uncovering social groups</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_specgraph/02_graph/roch-mmids-specgraph-graph.html">5.2. Background: basic concepts in graph theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_specgraph/03_extremal/roch-mmids-specgraph-extremal.html">5.3. Variational characterization of eigenvalues</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_specgraph/04_laplacian/roch-mmids-specgraph-laplacian.html">5.4. Spectral properties of the Laplacian matrix</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_specgraph/05_partitioning/roch-mmids-specgraph-partitioning.html">5.5. Application: graph partitioning via spectral clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_specgraph/05part2_sbm/roch-mmids-specgraph-sbm.html">5.6. Erdős-Rényi random graph and stochastic blockmodel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_specgraph/exercises/roch-mmids-specgraph-exercises.html">5.7. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_specgraph/06_adv/roch-mmids-specgraph-adv.html">5.8. Online supplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap07_prob/00_intro/roch-mmids-prob-intro.html">6. Probabilistic models: from simple to complex</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap07_prob/01_motiv/roch-mmids-prob-motiv.html">6.1. Motivating example: tracking location</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap07_prob/02_parametric/roch-mmids-prob-parametric.html">6.2. Background: introduction to parametric families and maximum likelihood estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap07_prob/03_joint/roch-mmids-prob-joint.html">6.3. Modeling more complex dependencies 1: using conditional independence</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap07_prob/03part2_em/roch-mmids-prob-em.html">6.4. Modeling more complex dependencies 2: marginalizing out an unobserved variable</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap07_prob/04_kalman/roch-mmids-prob-kalman.html">6.5. Application: linear-Gaussian models and Kalman filtering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap07_prob/exercises/roch-mmids-prob-exercises.html">6.6. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap07_prob/06_adv/roch-mmids-prob-adv.html">6.7. Online supplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap05_rwmc/00_intro/roch-mmids-rwmc-intro.html">7. Random walks on graphs and Markov chains</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap05_rwmc/01_motiv/roch-mmids-rwmc-motiv.html">7.1. Motivating example: discovering mathematical topics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap05_rwmc/03_mcdefs/roch-mmids-rwmc-mcdefs.html">7.2. Background: elements of finite Markov chains</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap05_rwmc/04_stat/roch-mmids-rwmc-stat.html">7.3. Limit behavior 1: stationary distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap05_rwmc/04part2_mclimit/roch-mmids-rwmc-mclimit.html">7.4. Limit behavior 2: convergence to equilibrium</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap05_rwmc/05_pagerank/roch-mmids-rwmc-pagerank.html">7.5. Application: random walks on graphs and PageRank</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap05_rwmc/05part2_gibbs/roch-mmids-rwmc-gibbs.html">7.6. Further applications: Gibbs sampling and generating images</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap05_rwmc/exercises/roch-mmids-rwmc-exercises.html">7.7. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap05_rwmc/06_adv/roch-mmids-rwmc-adv.html">7.8. Online supplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap08_nn/00_intro/roch-mmids-nn-intro.html">8. Neural networks, backpropagation and stochastic gradient descent</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/01_motiv/roch-mmids-nn-motiv.html">8.1. Motivating example:  classifying natural images</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/02_chain/roch-mmids-nn-chain.html">8.2. Background: Jacobian, chain rule, and a brief introduction to automatic differentiation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/03_backprop/roch-mmids-nn-backprop.html">8.3. Building blocks of AI 1: backpropagation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/04_sgd/roch-mmids-nn-sgd.html">8.4. Building blocks of AI 2: stochastic gradient descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/05_nn/roch-mmids-nn-nn.html">8.5. Building blocks of AI 3: neural networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/exercises/roch-mmids-nn-exercises.html">8.6. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/06_adv/roch-mmids-nn-adv.html">8.7. Online supplementary material</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/issues/new?title=Issue%20on%20page%20%2Fchap01_intro/04_highdim/roch-mmids-intro-highdim.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/chap01_intro/04_highdim/roch-mmids-intro-highdim.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Some observations about high-dimensional data</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clustering-in-high-dimension">1.4.1. Clustering in high dimension</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#surprising-phenomena-in-high-dimension">1.4.2. Surprising phenomena in high dimension</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <p><span class="math notranslate nohighlight">\(\newcommand{\bmu}{\boldsymbol{\mu}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bSigma}{\boldsymbol{\Sigma}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bfbeta}{\boldsymbol{\beta}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bflambda}{\boldsymbol{\lambda}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bgamma}{\boldsymbol{\gamma}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bsigma}{{\boldsymbol{\sigma}}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bpi}{\boldsymbol{\pi}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\btheta}{{\boldsymbol{\theta}}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bphi}{\boldsymbol{\phi}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\balpha}{\boldsymbol{\alpha}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\blambda}{\boldsymbol{\lambda}}\)</span>
<span class="math notranslate nohighlight">\(\renewcommand{\P}{\mathbb{P}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\E}{\mathbb{E}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\indep}{\perp\!\!\!\perp} \newcommand{\bx}{\mathbf{x}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bp}{\mathbf{p}}\)</span>
<span class="math notranslate nohighlight">\(\renewcommand{\bx}{\mathbf{x}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bX}{\mathbf{X}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\by}{\mathbf{y}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bY}{\mathbf{Y}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bz}{\mathbf{z}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bZ}{\mathbf{Z}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bw}{\mathbf{w}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bW}{\mathbf{W}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bv}{\mathbf{v}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bV}{\mathbf{V}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bfg}{\mathbf{g}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bfh}{\mathbf{h}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\horz}{\rule[.5ex]{2.5ex}{0.5pt}}\)</span>
<span class="math notranslate nohighlight">\(\renewcommand{\S}{\mathcal{S}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\X}{\mathcal{X}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\var}{\mathrm{Var}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\pa}{\mathrm{pa}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\Z}{\mathcal{Z}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bh}{\mathbf{h}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bb}{\mathbf{b}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bc}{\mathbf{c}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\cE}{\mathcal{E}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\cP}{\mathcal{P}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bbeta}{\boldsymbol{\beta}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bLambda}{\boldsymbol{\Lambda}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\cov}{\mathrm{Cov}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bfk}{\mathbf{k}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\idx}[1]{}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\xdi}{}\)</span></p>
<section id="some-observations-about-high-dimensional-data">
<h1><span class="section-number">1.4. </span>Some observations about high-dimensional data<a class="headerlink" href="#some-observations-about-high-dimensional-data" title="Link to this heading">#</a></h1>
<p>In this section, we first apply <span class="math notranslate nohighlight">\(k\)</span>-means clustering to a high-dimensional example to illustrate the issues that arise in that context. We then discuss some surprising phenomena in high dimensions.</p>
<section id="clustering-in-high-dimension">
<h2><span class="section-number">1.4.1. </span>Clustering in high dimension<a class="headerlink" href="#clustering-in-high-dimension" title="Link to this heading">#</a></h2>
<p>In this section, we test our implementation of <span class="math notranslate nohighlight">\(k\)</span>-means on a simple simulated dataset in high dimension.</p>
<p>The following function generates <span class="math notranslate nohighlight">\(n\)</span> data points from a mixture of two equally likely, spherical <span class="math notranslate nohighlight">\(d\)</span>-dimensional Gaussians with variance <span class="math notranslate nohighlight">\(1\)</span>, one with mean <span class="math notranslate nohighlight">\(-w\mathbf{e}_1\)</span> and one with mean <span class="math notranslate nohighlight">\(w \mathbf{e}_1\)</span>. We use <code class="docutils literal notranslate"><span class="pre">gmm2</span></code> from a previous section. It is found in <code class="docutils literal notranslate"><span class="pre">mmids.py</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">two_mixed_clusters</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="n">mu0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">(([</span><span class="n">w</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">d</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
    <span class="n">mu1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">(([</span><span class="o">-</span><span class="n">w</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">d</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">mmids</span><span class="o">.</span><span class="n">gmm2spherical</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">mu0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">mu1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>NUMERICAL CORNER:</strong> We start with <span class="math notranslate nohighlight">\(d=2\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">seed</span> <span class="o">=</span> <span class="mi">535</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mf">3.</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">two_mixed_clusters</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s run <span class="math notranslate nohighlight">\(k\)</span>-means on this dataset using <span class="math notranslate nohighlight">\(k=2\)</span>. We use <code class="docutils literal notranslate"><span class="pre">kmeans()</span></code> from the <code class="docutils literal notranslate"><span class="pre">mmids.py</span></code> file.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">assign</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">kmeans</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1044.8267883490312
208.5284166285488
204.02397716710018
204.02397716710018
204.02397716710018
</pre></div>
</div>
</div>
</div>
<p>Our default of <span class="math notranslate nohighlight">\(10\)</span> iterations seem to have been enough for the algorithm to converge. We can visualize the result by <a class="reference external" href="https://matplotlib.org/stable/api/_as_gen/matplotlib.lines.Line2D.html">coloring</a> the points according to the assignment.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">assign</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;brg&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/041b6d2a2e2458487a6e9892f422269d4b0cee8344ecadafad0f96617c853d2c.png" src="../../_images/041b6d2a2e2458487a6e9892f422269d4b0cee8344ecadafad0f96617c853d2c.png" />
</div>
</div>
<p>Let’s see what happens in higher dimension. We repeat our experiment with <span class="math notranslate nohighlight">\(d=1000\)</span>.</p>
<div class="cell tag_colab-keep docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mf">3.</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">two_mixed_clusters</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Again, we observe two clearly delineated clusters.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/eebaeb647ff9ddc453faa262a97b8269df24daff4223b31e5cba64e78d86844a.png" src="../../_images/eebaeb647ff9ddc453faa262a97b8269df24daff4223b31e5cba64e78d86844a.png" />
</div>
</div>
<p>This dataset is in <span class="math notranslate nohighlight">\(1000\)</span> dimensions, but we’ve plotted the data in only the first two dimensions. If we plot in any two dimensions not including the first one instead, we see only one cluster.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">2</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/da3214138fe48892de8ea209710a01b6089704f591faad197971fa5723ac1466.png" src="../../_images/da3214138fe48892de8ea209710a01b6089704f591faad197971fa5723ac1466.png" />
</div>
</div>
<p>Let’s see how <span class="math notranslate nohighlight">\(k\)</span>-means fares on this dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">assign</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">kmeans</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>99518.03165136592
99518.03165136592
99518.03165136592
99518.03165136592
99518.03165136592
</pre></div>
</div>
</div>
</div>
<p>Our attempt at clustering does not appear to have been successful.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">assign</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;brg&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/d6154a3919d9829cd26cbb9709f164d25053a5c995bdf10a2d8164d3d315d4be.png" src="../../_images/d6154a3919d9829cd26cbb9709f164d25053a5c995bdf10a2d8164d3d315d4be.png" />
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p>What happened? While the clusters are easy to tease apart <em>if we know to look at the first coordinate only</em>, in the full space the within-cluster and between-cluster distances become harder to distinguish: the noise overwhelms the signal.</p>
<p>As the dimension increases, the distributions of intra-cluster and inter-cluster distances overlap significantly and become more or less indistinguishable. That provides some insights into why clustering may fail here. Note that we used the same offset for all simulations. On the other hand, if the separation between the clusters is sufficiently large, one would expect clustering to work even in high dimension.</p>
<p><img alt="Histograms of within-cluster and between-cluster distances for a sample of size  in  (left) and  (right) dimensions with a given offset . As  increases, the two distributions become increasingly indistinguishable." src="../../_images/cluster-distances.png" /></p>
<p><strong>TRY IT!</strong> What precedes (and what follows in the next subsection) is not a formal proof that <span class="math notranslate nohighlight">\(k\)</span>-means clustering will be unsuccessful here. The behavior of the algorithm is quite complex and depends, in particular, on the initialization and the density of points. Here, increasing the number of data points eventually leads to a much better performance. Explore this behavior on your own by modifying the code. (For some theoretical justifications (beyond this course), see <a class="reference external" href="https://arxiv.org/pdf/0912.0086.pdf">here</a> and <a class="reference external" href="http://www.stat.yale.edu/~pollard/Papers/Pollard81AS.pdf">here</a>.)</p>
<!--ONLINE ONLY

**CHAT & LEARN** According to Claude, here is how a cat might summarize the situation: 

**Figure:** A kitten in space (*Credit:* Made with [Midjourney](https://www.midjourney.com/))  

![Kitty in space](./figs/kitten_astronaut-small.png)

$\bowtie$

"I iz in high-dimensional space

All dese data points, everywheres

But no matter how far I roams

They all look the sames to me!"


$\ddagger$

--></section>
<section id="surprising-phenomena-in-high-dimension">
<h2><span class="section-number">1.4.2. </span>Surprising phenomena in high dimension<a class="headerlink" href="#surprising-phenomena-in-high-dimension" title="Link to this heading">#</a></h2>
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">a high-dimensional space is a lonely place</p>&mdash; Bernhard Schölkopf (@bschoelkopf) <a href="https://twitter.com/bschoelkopf/status/503554842829549568?ref_src=twsrc%5Etfw">August 24, 2014</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> <p>In the previous section, we saw how the contribution from a large number of “noisy dimensions” can overwhelm the “signal” in the context of clustering. In this section we discuss further properties of high-dimensional space that are relevant to data science problems.</p>
<p>Applying <em>Chebyshev’s Inequality</em> to sums of independent random variables has useful statistical implications: it shows that, with a large enough number of samples <span class="math notranslate nohighlight">\(n\)</span>, the sample mean is close to the population mean. Hence it allows us to infer properties of a population from samples. Interestingly, one can apply a similar argument to a different asymptotic regime: the limit of large dimension <span class="math notranslate nohighlight">\(d\)</span>. But as we will see in this section, the statistical implications are quite different.</p>
<p>To start explaining the quote above, we consider a simple experiment. Let <span class="math notranslate nohighlight">\(\mathcal{C} = [-1/2,1/2]^d\)</span> be the <span class="math notranslate nohighlight">\(d\)</span>-cube with side lengths <span class="math notranslate nohighlight">\(1\)</span> centered at the origin and let <span class="math notranslate nohighlight">\(\mathcal{B} = \{\mathbf{x} \in \mathbb{R}^d : \|\mathbf{x}\|\leq 1/2\}\)</span> be the inscribed <span class="math notranslate nohighlight">\(d\)</span>-ball.</p>
<p>Now pick a point <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> uniformly at random in <span class="math notranslate nohighlight">\(\mathcal{C}\)</span>. What is the probability that it falls in <span class="math notranslate nohighlight">\(\mathcal{B}\)</span>?</p>
<p>To generate <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>, we pick <span class="math notranslate nohighlight">\(d\)</span> independent random variables <span class="math notranslate nohighlight">\(X_1, \ldots, X_d \sim \mathrm{U}[-1/2, 1/2]\)</span>, and form the vector <span class="math notranslate nohighlight">\(\mathbf{X} = (X_1, \ldots, X_d)\)</span>. Indeed, the PDF of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is then <span class="math notranslate nohighlight">\(f_{\mathbf{X}}(\mathbf{x})= 1^d = 1\)</span> if <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathcal{C}\)</span> and <span class="math notranslate nohighlight">\(0\)</span> otherwise.</p>
<p>The event we are interested in is <span class="math notranslate nohighlight">\(A = \left\{\|\mathbf{X}\| \leq 1/2\right\}\)</span>. The uniform distribution over the set <span class="math notranslate nohighlight">\(\mathcal{C}\)</span> has the property that <span class="math notranslate nohighlight">\(\mathbb{P}[A]\)</span> is the volume of <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> divided by the volume of <span class="math notranslate nohighlight">\(\mathcal{C}\)</span>. In this case, the volume of <span class="math notranslate nohighlight">\(\mathcal{C}\)</span> is <span class="math notranslate nohighlight">\(1^d = 1\)</span> and the volume of <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> has an <a class="reference external" href="https://en.wikipedia.org/wiki/Volume_of_an_n-ball">explicit formula</a>.</p>
<p>This leads to the following surprising fact:</p>
<p><strong>THEOREM</strong> <strong>(High-dimensional Cube)</strong> <span class="math notranslate nohighlight">\(\idx{high-dimensional cube theorem}\xdi\)</span> Let
<span class="math notranslate nohighlight">\(\mathcal{B} = \{\mathbf{x} \in \mathbb{R}^d \,:\, \|\mathbf{x}\|\leq 1/2\}\)</span> and
<span class="math notranslate nohighlight">\(\mathcal{C} = [-1/2,1/2]^d\)</span>. Pick <span class="math notranslate nohighlight">\(\mathbf{X} \sim \mathrm{U}[\mathcal{C}]\)</span>. Then, as <span class="math notranslate nohighlight">\(d \to +\infty\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}[\mathbf{X} \in \mathcal{B}]
\to 0.
\]</div>
<p><span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p>In words, in high dimension if one picks a point at random from the cube, it is unlikely to be close to the origin. Instead it is likely to be in the corners. A geometric interpretation is that a high-dimensional cube is a bit like a “spiky ball.”</p>
<p><strong>Figure:</strong> Visualization of a high-dimensional cube as a spiky ball (<em>Credit:</em> Made with <a class="reference external" href="https://www.midjourney.com/">Midjourney</a>)</p>
<p><img alt="Visualization of a high-dimensional cube" src="../../_images/ball_with_spikes.png" /></p>
<p><span class="math notranslate nohighlight">\(\bowtie\)</span></p>
<p>We give a proof based on <em>Chebyshev’s Inequality</em>. It has the advantage of providing some insight into this counter-intuitive phenomenon by linking it to the concentration of sums of independent random variables, in this case the squared norm of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>.</p>
<p><em>Proof idea:</em> We think of <span class="math notranslate nohighlight">\(\|\mathbf{X}\|^2\)</span> as a sum of independent random variables and apply <em>Chebyshev’s Inequality</em>. It implies that the norm of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is concentrated around its mean, which grows like <span class="math notranslate nohighlight">\(\sqrt{d}\)</span>. The latter is larger than <span class="math notranslate nohighlight">\(1/2\)</span> for <span class="math notranslate nohighlight">\(d\)</span> large.</p>
<p><em>Proof:</em> To see the relevance of <em>Chebyshev’s Inequality</em>, we compute the mean and standard deviation of the norm of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>. In fact, because of the square root in <span class="math notranslate nohighlight">\(\|\mathbf{X}\|\)</span>, computing its expectation is difficult. Instead we work with the squared norm</p>
<div class="math notranslate nohighlight">
\[
\|\mathbf{X}\|^2 = X_1^2 + X_2^2 + \cdots + X_d^2,
\]</div>
<p>which has the advantage of being a sum of independent random variables – for which the expectation and variance are much easier to compute. Observe further that the probability of the event of interest <span class="math notranslate nohighlight">\(\{\|\mathbf{X}\| \leq 1/2\}\)</span> can be re-written in terms of <span class="math notranslate nohighlight">\(\|\mathbf{X}\|^2\)</span> as follows</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb{P}
\left[
\|\mathbf{X}\| \leq 1/2
\right]
&amp;= 
\mathbb{P}
\left[
\|\mathbf{X}\|^2 \leq 1/4
\right].
\end{align*}\]</div>
<p>To simplify the notation, we use <span class="math notranslate nohighlight">\(\tilde\mu = \mathbb{E}[X_1^2]\)</span> and <span class="math notranslate nohighlight">\(\tilde\sigma = \sqrt{\mathrm{Var}[X_1^2]}\)</span> for the mean and standard deviation of <span class="math notranslate nohighlight">\(X_1^2\)</span> respectively. Using linearity of expectation and the fact that the <span class="math notranslate nohighlight">\(X_i\)</span>’s are independent, we get</p>
<div class="math notranslate nohighlight">
\[
\mu_{\|\mathbf{X}\|^2}
= \mathbb{E}\left[
\|\mathbf{X}\|^2
\right]
= \sum_{i=1}^d \mathbb{E}[X_i^2]
= d \,\mathbb{E}[X_1^2]
= \tilde\mu \, d,
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\mathrm{Var}\left[
\|\mathbf{X}\|^2
\right]
= \sum_{i=1}^d \mathrm{Var}[X_i^2]
= d \,\mathrm{Var}[X_1^2].
\]</div>
<p>Taking a square root, we get an expression for the standard deviation of our quantity of interest <span class="math notranslate nohighlight">\(\|\mathbf{X}\|^2\)</span> in terms of the standard deviation of <span class="math notranslate nohighlight">\(X_1^2\)</span></p>
<div class="math notranslate nohighlight">
\[
\sigma_{\|\mathbf{X}\|^2}
= \tilde\sigma \, \sqrt{d}.
\]</div>
<p>(Note that we could compute <span class="math notranslate nohighlight">\(\tilde\mu\)</span> and <span class="math notranslate nohighlight">\(\tilde\sigma\)</span> explicitly, but it will not be necessary here.)</p>
<p>We use <em>Chebyshev’s Inequality</em> to show that <span class="math notranslate nohighlight">\(\|\mathbf{X}\|^2\)</span> is highly likely to be close to its mean <span class="math notranslate nohighlight">\(\tilde\mu \, d\)</span>, which is much larger than <span class="math notranslate nohighlight">\(1/4\)</span> when <span class="math notranslate nohighlight">\(d\)</span> is large. And that therefore <span class="math notranslate nohighlight">\(\|\mathbf{X}\|^2\)</span> is highly unlikely to be smaller than <span class="math notranslate nohighlight">\(1/4\)</span>. We give the details next.</p>
<p>By the one-sided version of <em>Chebyshev’s Inequality</em> in terms of the standard deviation, we have</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}\left[
\|\mathbf{X}\|^2 - \mu_{\|\mathbf{X}\|^2} \leq - \alpha
\right]
\leq 
\left(\frac{\sigma_{\|\mathbf{X}\|^2}}{\alpha}\right)^2.
\]</div>
<p>That is, using the formulas above and rearranging slightly,</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}\left[
\|\mathbf{X}\|^2 \leq \tilde\mu \, d - \alpha
\right]
\leq 
\left(\frac{\tilde\sigma \, \sqrt{d}}{\alpha}\right)^2.
\]</div>
<p>How do we relate this to the probability of interest
<span class="math notranslate nohighlight">\(\mathbb{P}\left[\|\mathbf{X}\|^2 \leq 1/4\right]\)</span>? Recall that we are free to choose <span class="math notranslate nohighlight">\(\alpha\)</span> in this inequality. So simply take <span class="math notranslate nohighlight">\(\alpha\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
\tilde\mu \,d - \alpha = \frac{1}{4},
\]</div>
<p>that is, <span class="math notranslate nohighlight">\(\alpha = \tilde\mu \,d - 1/4\)</span>. Observe that, once <span class="math notranslate nohighlight">\(d\)</span> is large enough, it holds that <span class="math notranslate nohighlight">\(\alpha &gt; 0\)</span>.</p>
<p>Finally, replacing this choice of <span class="math notranslate nohighlight">\(\alpha\)</span> in the inequality above gives</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb{P}
\left[
\|\mathbf{X}\| \leq 1/2
\right]
&amp;= \mathbb{P}\left[\|\mathbf{X}\|^2 \leq 1/4\right]\\
&amp;=
\mathbb{P}\left[
\|\mathbf{X}\|^2 \leq \tilde\mu \, d - \alpha
\right]\\
&amp;\leq 
\left(\frac{\tilde\sigma \, \sqrt{d}}{\alpha}\right)^2\\
&amp;\leq 
\left(\frac{\tilde\sigma \, \sqrt{d}}{\tilde\mu \,d - 1/4}\right)^2.
\end{align*}\]</div>
<p>Critically, <span class="math notranslate nohighlight">\(\tilde\mu\)</span> and <span class="math notranslate nohighlight">\(\tilde\sigma\)</span> do not depend on <span class="math notranslate nohighlight">\(d\)</span>. So the right-hand side goes to <span class="math notranslate nohighlight">\(0\)</span> as <span class="math notranslate nohighlight">\(d \to +\infty\)</span>. Indeed, <span class="math notranslate nohighlight">\(d\)</span> is much larger than <span class="math notranslate nohighlight">\(\sqrt{d}\)</span> when <span class="math notranslate nohighlight">\(d\)</span> is large. That proves the claim.<span class="math notranslate nohighlight">\(\square\)</span></p>
<p>We will see later in the course that this high-dimensional phenomenon has implications for data science problems. It is behind what is referred to as the <a class="reference external" href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">Curse of Dimensionality</a><span class="math notranslate nohighlight">\(\idx{curse of dimensionality}\xdi\)</span>.</p>
<p>While <em>Chebyshev’s inequality</em> correctly implies that <span class="math notranslate nohighlight">\(\mathbb{P}[\mathbf{X} \in \mathcal{B}]\)</span> goes to <span class="math notranslate nohighlight">\(0\)</span>, it does not give the correct rate of convergence. In reality, that probability goes to <span class="math notranslate nohighlight">\(0\)</span> at a much faster rate than <span class="math notranslate nohighlight">\(1/d\)</span>. Specifically, <a class="reference external" href="https://en.wikipedia.org/wiki/Volume_of_an_n-ball#High_dimensions">it can be shown</a> that <span class="math notranslate nohighlight">\(\mathbb{P}[\mathbf{X} \in \mathcal{B}]\)</span> goes to <span class="math notranslate nohighlight">\(0\)</span> roughly as <span class="math notranslate nohighlight">\(d^{-d/2}\)</span>. We will not need or derive this fact here.</p>
<p><strong>NUMERICAL CORNER:</strong> We can check the theorem in a simulation. Here we pick <span class="math notranslate nohighlight">\(n\)</span> points uniformly at random in the <span class="math notranslate nohighlight">\(d\)</span>-cube <span class="math notranslate nohighlight">\(\mathcal{C}\)</span>, for a range of dimensions up to <code class="docutils literal notranslate"><span class="pre">dmax</span></code>. We then plot the frequency of landing in the inscribed <span class="math notranslate nohighlight">\(d\)</span>-ball <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> and see that it rapidly converges to <span class="math notranslate nohighlight">\(0\)</span>. Alternatively, we could just plot the formula for the volume of <span class="math notranslate nohighlight">\(\mathcal{B}\)</span>. But knowing how to do simulations is useful in situations where explicit formulas are unavailable or intractable. We plot the result up to dimension <span class="math notranslate nohighlight">\(10\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dmax</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1000</span>

<span class="n">in_ball</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">dmax</span><span class="p">)</span>
<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dmax</span><span class="p">):</span>
    <span class="n">in_ball</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([(</span><span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">d</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)])</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">dmax</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">in_ball</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/99369a6a3e164e0a1629a773735de197a6fbfcdc1232dd47ea6e6ece69ce7263.png" src="../../_images/99369a6a3e164e0a1629a773735de197a6fbfcdc1232dd47ea6e6ece69ce7263.png" />
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><em><strong>Self-assessment quiz</strong></em> <em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>1</strong> The volume of the <span class="math notranslate nohighlight">\(d\)</span>-dimensional cube <span class="math notranslate nohighlight">\(C = [-1/2, 1/2]^d\)</span> is:</p>
<p>a) <span class="math notranslate nohighlight">\(1/d\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(1/2^d\)</span></p>
<p>c) 1</p>
<p>d) <span class="math notranslate nohighlight">\(2^d\)</span></p>
<p><strong>2</strong> In a high-dimensional cube <span class="math notranslate nohighlight">\(C = [-1/2, 1/2]^d\)</span>, as the dimension <span class="math notranslate nohighlight">\(d\)</span> increases, the probability that a randomly chosen point lies within the inscribed sphere <span class="math notranslate nohighlight">\(B = \{x \in \mathbb{R}^d : \|x\| \le 1/2\}\)</span>:</p>
<p>a) Approaches 1</p>
<p>b) Approaches 1/2</p>
<p>c) Approaches 0</p>
<p>d) Remains constant</p>
<p><strong>3</strong> Which of the following best describes the appearance of a high-dimensional cube?</p>
<p>a) A smooth, round ball</p>
<p>b) A spiky ball with most of its volume concentrated in the corners</p>
<p>c) A perfect sphere with uniform volume distribution</p>
<p>d) A flat, pancake-like shape</p>
<p><strong>4</strong> Which inequality is used to prove the theorem about high-dimensional cubes?</p>
<p>a) Cauchy-Schwarz inequality</p>
<p>b) Triangle inequality</p>
<p>c) Markov’s inequality</p>
<p>d) Chebyshev’s inequality</p>
<p><strong>5</strong> In the proof of the theorem about high-dimensional cubes, which property of the squared norm <span class="math notranslate nohighlight">\(\|X\|^2\)</span> is used?</p>
<p>a) It is a sum of dependent random variables.</p>
<p>b) It is a sum of independent random variables.</p>
<p>c) It is a product of independent random variables.</p>
<p>d) It is a product of dependent random variables.</p>
<p>Answer for 1: c. Justification: The side length of the cube is 1, and the volume of a <span class="math notranslate nohighlight">\(d\)</span>-dimensional cube is the side length raised to the power <span class="math notranslate nohighlight">\(d\)</span>.</p>
<p>Answer for 2: c. Justification: This is the statement of the theorem “High-dimensional Cube” in the text.</p>
<p>Answer for 3: b. Justification: The text mentions, “A geometric interpretation is that a high-dimensional cube is a bit like a ‘spiky ball.’”</p>
<p>Answer for 4: d. Justification: The text explicitly states that Chebyshev’s inequality is used in the proof.</p>
<p>Answer for 5: b. Justification: The proof states, “we work with the squared norm</p>
<div class="math notranslate nohighlight">
\[
\|\mathbf{X}\|^2 = X_1^2 + X_2^2 + \cdots + X_d^2,
\]</div>
<p>which has the advantage of being a sum of independent random variables.”</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chap01_intro/04_highdim"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../03_clustering/roch-mmids-intro-clustering.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">1.3. </span>Clustering: an objective, an algorithm and a guarantee</p>
      </div>
    </a>
    <a class="right-next"
       href="../exercises/roch-mmids-intro-exercises.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">1.5. </span>Exercises</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clustering-in-high-dimension">1.4.1. Clustering in high dimension</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#surprising-phenomena-in-high-dimension">1.4.2. Surprising phenomena in high dimension</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Sebastien Roch
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>