{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "addb5274",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell",
     "colab-keep"
    ]
   },
   "source": [
    "***\n",
    "\n",
    "*Course:* [Math 535](https://people.math.wisc.edu/~roch/mmids/) - Mathematical Methods in Data Science (MMiDS)  \n",
    "*Chapter:* 1-Introduction: a first data science problem   \n",
    "*Author:* [Sebastien Roch](https://people.math.wisc.edu/~roch/), Department of Mathematics, University of Wisconsin-Madison  \n",
    "*Updated:* May 18, 2025   \n",
    "*Copyright:* &copy; 2025 Sebastien Roch\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7ab664",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# You will need the files:\n",
    "#     * mmids.py\n",
    "#     * penguins-measurements.csv\n",
    "#     * penguins-species.csv\n",
    "# from https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/tree/main/utils\n",
    "#\n",
    "# IF RUNNING ON GOOGLE COLAB (RECOMMENDED):\n",
    "# \"Upload to session storage\" from the Files tab on the left\n",
    "# Alternative instructions: https://colab.research.google.com/notebooks/io.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c103b80b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch\n",
    "import mmids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd85b9cc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\newcommand{\\bmu}{\\boldsymbol{\\mu}}$ \n",
    "$\\newcommand{\\bSigma}{\\boldsymbol{\\Sigma}}$\n",
    "$\\newcommand{\\bfbeta}{\\boldsymbol{\\beta}}$ \n",
    "$\\newcommand{\\bflambda}{\\boldsymbol{\\lambda}}$\n",
    "$\\newcommand{\\bgamma}{\\boldsymbol{\\gamma}}$ \n",
    "$\\newcommand{\\bsigma}{{\\boldsymbol{\\sigma}}}$\n",
    "$\\newcommand{\\bpi}{\\boldsymbol{\\pi}}$\n",
    "$\\newcommand{\\btheta}{{\\boldsymbol{\\theta}}}$ \n",
    "$\\newcommand{\\bphi}{\\boldsymbol{\\phi}}$ \n",
    "$\\newcommand{\\balpha}{\\boldsymbol{\\alpha}}$\n",
    "$\\newcommand{\\blambda}{\\boldsymbol{\\lambda}}$\n",
    "$\\renewcommand{\\P}{\\mathbb{P}}$\n",
    "$\\newcommand{\\E}{\\mathbb{E}}$\n",
    "$\\newcommand{\\indep}{\\perp\\!\\!\\!\\perp} \\newcommand{\\bx}{\\mathbf{x}}$ \n",
    "$\\newcommand{\\bp}{\\mathbf{p}}$\n",
    "$\\renewcommand{\\bx}{\\mathbf{x}}$\n",
    "$\\newcommand{\\bX}{\\mathbf{X}}$ \n",
    "$\\newcommand{\\by}{\\mathbf{y}}$ \n",
    "$\\newcommand{\\bY}{\\mathbf{Y}}$ \n",
    "$\\newcommand{\\bz}{\\mathbf{z}}$\n",
    "$\\newcommand{\\bZ}{\\mathbf{Z}}$ \n",
    "$\\newcommand{\\bw}{\\mathbf{w}}$ \n",
    "$\\newcommand{\\bW}{\\mathbf{W}}$\n",
    "$\\newcommand{\\bv}{\\mathbf{v}}$ \n",
    "$\\newcommand{\\bV}{\\mathbf{V}}$\n",
    "$\\newcommand{\\bfg}{\\mathbf{g}}$ \n",
    "$\\newcommand{\\bfh}{\\mathbf{h}}$ \n",
    "$\\newcommand{\\horz}{\\rule[.5ex]{2.5ex}{0.5pt}}$\n",
    "$\\renewcommand{\\S}{\\mathcal{S}}$ \n",
    "$\\newcommand{\\X}{\\mathcal{X}}$ \n",
    "$\\newcommand{\\var}{\\mathrm{Var}}$ \n",
    "$\\newcommand{\\pa}{\\mathrm{pa}}$\n",
    "$\\newcommand{\\Z}{\\mathcal{Z}}$ \n",
    "$\\newcommand{\\bh}{\\mathbf{h}}$ \n",
    "$\\newcommand{\\bb}{\\mathbf{b}}$ \n",
    "$\\newcommand{\\bc}{\\mathbf{c}}$ \n",
    "$\\newcommand{\\cE}{\\mathcal{E}}$ \n",
    "$\\newcommand{\\cP}{\\mathcal{P}}$\n",
    "$\\newcommand{\\bbeta}{\\boldsymbol{\\beta}}$\n",
    "$\\newcommand{\\bLambda}{\\boldsymbol{\\Lambda}}$\n",
    "$\\newcommand{\\cov}{\\mathrm{Cov}}$\n",
    "$\\newcommand{\\bfk}{\\mathbf{k}}$\n",
    "$\\newcommand{\\idx}[1]{}$\n",
    "$\\newcommand{\\xdi}{}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db04388",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Motivating example: identifying penguin species"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495cc07d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Imagine that you are an evolutionary biologist studying penguins. You have collected measurements on a large number of individual specimens. Your goal is to identify different [species](https://en.wikipedia.org/wiki/Species) within this collection based on those measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bebcb33",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We use a penguin dataset collected and made available by [Dr. Kristen Gorman](https://www.uaf.edu/cfos/people/faculty/detail/kristen-gorman.php) and the [Palmer Station, Antarctica LTER](https://pallter.marine.rutgers.edu/). We upload the data in the form of a data table (similar to a spreadsheet) called [`DataFrame`](https://pandas.pydata.org/docs/reference/frame.html) in [`pandas`](https://pandas.pydata.org/docs/), where the columns are different measurements (or features) and the rows are different samples. Below, we load the data using [`pandas.read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html?highlight=read_csv#) and show the first $5$ lines of the dataset (using [`DataFrame.head`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html)). This dataset is a simplified version (i.e., with some columns removed) of the full dataset from [Allison Horst](https://allisonhorst.com/)'s [GitHub page](https://github.com/allisonhorst/palmerpenguins/blob/main/README.md). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e32c8c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('penguins-measurements.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19103a56",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Observe that this dataset has missing values (i.e., the entries `NaN` above). A common way to deal with this issue is to remove all rows with missing values. This can be done using [`pandas.DataFrame.dropna`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html). This kind of pre-processing is fundamental in data science, but we will not discuss it much in this book. It is however important to be aware of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09a2fb9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data = data.dropna()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ef3217",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "There are $342$ samples, as can be seen by using [`pandas.DataFrame.shape`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.shape.html) which gives the dimensions of the DataFrame as a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42538c13",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da812c52",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Let us first extract the columns into a NumPy array using [`pandas.DataFrame.to_numpy`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_numpy.html). We will have more to say later on about NumPy, a numerical library for Python that in essence allows to manipulate vectors and matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84634caa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = data.to_numpy()\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0f57bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We visualize two measurements in the data, the bill depth and flipper length. (The original dataset used the more precise term [culmen](https://en.wikipedia.org/wiki/Beak#Culmen) depth.) Below, each point is a sample. This is called a scatter plot$\\idx{scatter plot}\\xdi$. Quoting [Wikipedia](https://en.wikipedia.org/wiki/Scatter_plot):\n",
    "\n",
    "> The data are displayed as a collection of points, each having the value of one variable determining the position on the horizontal axis and the value of the other variable determining the position on the vertical axis.\n",
    "\n",
    "We use [`matplotlib.pyplot`](https://matplotlib.org/stable/api/pyplot_summary.html) for most of our plotting needs in this book, with a few exceptions. Specifically, here we use the function [`matplotlib.pyplot.scatter`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86836eee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X[:,1], X[:,2], s=5, c='k')\n",
    "plt.xlabel('bill_depth_mm'), plt.ylabel('flipper_length_mm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c796e606",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We observe what appears to be two fairly well-defined clusters of samples on the top left and bottom right respectively. What is a [cluster](https://en.wikipedia.org/wiki/Cluster_analysis)? Intuitively, it is a group of samples that are close to each other, but far from every other sample. In this case, it may be an indication that these samples come from a separate species."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768ccfca",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Now let's look at the full dataset. Visualizing the full $4$-dimensional data is not straightforward. One way to do this is to consider all pairwise scatter plots. We use the function [`seaborn.pairplot`](https://seaborn.pydata.org/generated/seaborn.pairplot.html) from the library [Seaborn](https://seaborn.pydata.org/index.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9a868c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.pairplot(data, height=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5d129e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "How many species of penguins do you think there are in this dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2354aa29",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "What would be useful is a method that *automatically* identifies clusters *whatever the dimension of the data*. In this chapter, we will discuss a standard way to do this: $k$-means clustering. We will come back to the penguins dataset later in the chapter. \n",
    "\n",
    "But first we need to review some basic concepts about vectors and distances in order to formulate clustering as an appropriate *optimization* problem, a perspective that will be recurring throughout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c650a138",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**CHAT & LEARN** Ask your favorite AI chatbot for alternative ways to deal with missing values in a dataset. Implement one of these alternatives on the penguins dataset (you can ask the chatbot for the code!). ([Open in Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_intro_notebook.ipynb)) $\\ddagger$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d13cfb1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Background: quick refresher of matrix algebra, differential calculus, and elementary probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4b0b04",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** In NumPy, a vector is defined as a 1d array. We first must import the [NumPy](https://numpy.org) package, which is often abbreviated by `np`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240e3656",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "u = np.array([1., 3., 5. ,7.])\n",
    "print(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ea4766",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We access the entries of `u` as follows, where note that indexing in NumPy starts at $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83f1c8b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(u[0])\n",
    "print(u[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32de6ef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To obtain the norm of a vector, we can use the function [`linalg.norm`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html), which requires the [`numpy.linalg`](https://numpy.org/doc/stable/reference/routines.linalg.html) package (often abbreviated as `LA`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d312242",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "LA.norm(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee74535d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "which we check next \"by hand\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5dc82d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.sqrt(np.sum(u ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbbe75a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "In NumPy, [`**`](https://numpy.org/doc/stable/reference/generated/numpy.power.html) indicates element-wise exponentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d903e4d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**TRY IT!** Compute the inner product of $u = (1,2,3,4)$ and $v = (5, 4, 3, 2)$ without using the function [`np.dot`](https://numpy.org/doc/stable/reference/generated/numpy.dot.html). *Hint*: The product of two real numbers $a$ and $b$ is `a * b`. ([Open in Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_intro_notebook.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41f9896",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.array([1., 2., 3. ,4.])\n",
    "# EDIT THIS LINE: define v\n",
    "# EDIT THIS LINE: compute the inner product between u and v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b799cef",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8913cb17",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We will often work with collections of $n$ vectors $\\mathbf{x}_1, \\ldots, \\mathbf{x}_n$ in $\\mathbb{R}^d$ and it will be convenient to stack them up into a matrix\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{x}_1^T \\\\\n",
    "\\mathbf{x}_2^T \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{x}_n^T \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x_{11} & x_{12} & \\cdots & x_{1d} \\\\\n",
    "x_{21} & x_{22} & \\cdots & x_{2d} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{n1} & x_{n2} & \\cdots & x_{nd} \\\\\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c435c68",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To create a matrix out of two vectors, we use the function [`numpy.stack`](https://numpy.org/doc/stable/reference/generated/numpy.stack.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05d78bc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "u = np.array([1., 3., 5., 7.])\n",
    "v = np.array([2., 4., 6., 8.])\n",
    "X = np.stack((u,v),axis=0)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac73d61",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Quoting the documentation:\n",
    "\n",
    "> The axis parameter specifies the index of the new axis in the dimensions of the result. For example, if axis=0 it will be the first dimension and if axis=-1 it will be the last dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d24df06",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Alternatively, we can define the same matrix as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cbd5b2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Y = np.array([[1., 3., 5., 7.],[2., 4., 6., 8.]])\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbad4e7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We access the entries as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c227f58c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(Y[0,0])\n",
    "print(Y[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb8eac1",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe36f8e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** In NumPy, the Frobenius norm of a matrix can be computed using the function [`numpy.linalg.norm`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972725bb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A = np.array([[1., 0.],[0., 1.],[0., 0.]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79e2cbf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "LA.norm(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f25f4a3",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5e1b63",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** The function $f(x) = x^2$ over $\\mathbb{R}$ has a global minimizer at $x^* = 0$. Indeed, we clearly have $f(x) \\geq 0$ for all $x$ while $f(0) = 0$. To plot the function, we use the Matplotlib package, and specifically its function [`matplotlib.pyplot.plot`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html). We also use the function [`numpy.linspace`](https://numpy.org/doc/stable/reference/generated/numpy.linspace.html) to create an array of evenly spaced numbers where we evaluate $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb67343a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-2,2,100)\n",
    "y = x ** 2\n",
    "\n",
    "plt.plot(x, y, c='k')\n",
    "plt.ylim(-0.25,4.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1c69ba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The function $f(x) = e^x$ over $\\mathbb{R}$ does not have a global minimizer. Indeed, $f(x) > 0$ but no $x$ achieves $0$. And, for any $m > 0$, there is $x$ small enough such that $f(x) < m$. Note that $\\mathbb{R}$ is *not* bounded, therefore the *Extreme Value Theorem* does not apply here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94832050",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-2,2,100)\n",
    "y = np.exp(x)\n",
    "\n",
    "plt.plot(x, y, c='k')\n",
    "plt.ylim(-0.25,4.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19c35a2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The function $f(x) = (x+1)^2 (x-1)^2$ over $\\mathbb{R}$ has two global minimizers at $x^* = -1$ and $x^{**} = 1$. Indeed, $f(x) \\geq 0$ and $f(x) = 0$ if and only $x = x^*$ or $x = x^{**}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bf88a5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-2,2,100)\n",
    "y = ((x+1)**2) * ((x-1)**2)\n",
    "\n",
    "plt.plot(x,y,c='k')\n",
    "plt.ylim(-0.25,4.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e2f857",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "In NumPy, [`*`](https://numpy.org/doc/stable/reference/generated/numpy.multiply.html) is element-wise multiplication. (For a more careful explanation of how it works, see the [broadcasting rules](https://numpy.org/doc/stable/user/basics.broadcasting.html) of NumPy.) $\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d808b9f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We can use simulations to confirm the *Weak Law of Large Numbers*. Recall that a uniform random variable over the interval $[a,b]$ has density\n",
    "\n",
    "$$\n",
    "f_{X}(x)\n",
    "= \\begin{cases}\n",
    "\\frac{1}{b-a} & x \\in [a,b] \\\\\n",
    "0 & \\text{o.w.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "We write $X \\sim \\mathrm{U}[a,b]$. We can obtain a sample from $\\mathrm{U}[0,1]$ by using the function\n",
    "[`numpy.random.Generator.uniform`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.uniform.html). We must first instantiate a random number generator (RNG) with [`numpy.random.default_rng`](https://numpy.org/doc/stable/reference/random/generator.html) in NumPy. We provide a [seed](https://numpy.org/doc/stable/reference/random/bit_generators/index.html#seeding-and-entropy) as an initial state for the RNG. Using the same seed again ensures reproducibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8909f147",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "seed = 535\n",
    "rng = np.random.default_rng(seed)\n",
    "rng.uniform()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc5160d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Now we take $n$ samples from $\\mathrm{U}[0,1]$ and compute their sample mean. We repeat $k$ times and display the empirical distribution of the sample means using an [histogram](https://en.wikipedia.org/wiki/Histogram). We start with $n=10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d42a563",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "n, k = 10, 1000\n",
    "sample_mean = [np.mean(rng.random(n)) for i in range(k)]\n",
    "plt.hist(sample_mean, bins=10, color='lightblue', edgecolor='black')\n",
    "plt.xlim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbb02e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Taking $n$ much larger leads to more concentration around the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830e2119",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "n, k = 100, 1000\n",
    "sample_mean = [np.mean(rng.random(n)) for i in range(k)]\n",
    "plt.hist(sample_mean, bins=10, color='lightblue', edgecolor='black')\n",
    "plt.xlim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9220d6",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**TRY IT!** Recall that the cumulative distribution function (CDF)$\\idx{cumulative distribution function}\\xdi$ of a random variable $X$ is defined as \n",
    "\n",
    "$$\n",
    "F_X(z) = \\mathbb{P}[X \\leq z], \\qquad \\forall z \\in \\mathbb{R}.\n",
    "$$\n",
    "\n",
    "a) Let $\\mathcal{Z}$ be the interval where $F_X(z) \\in (0,1)$ and assume that $F_X$ is strictly increasing on $\\mathcal{Z}$. Let $U \\sim \\mathrm{U}[0,1]$. Show that \n",
    "\n",
    "$$\n",
    "\\mathbb{P}[F_X^{-1}(U) \\leq z] = F_X(z).\n",
    "$$\n",
    "\n",
    "b) Generate a sample from $\\mathrm{U}[a,b]$ for arbitrary $a$, $b$ using `numpy.random.Generator.uniform` and the observation in a). This is called the inverse transform sampling method$\\idx{inverse transform sampling method}\\xdi$. ([Open in Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_intro_notebook.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8085ee96",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "a = -1\n",
    "b = 1\n",
    "X = rng.uniform()\n",
    "# EDIT THIS LINE: transform X to obtain a random variable Y ~ U[a,b]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70f138d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941a4d13",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**Normal distribution** $\\idx{normal or Gaussian distribution}\\xdi$ Recall that a standard Normal variable $X$ has PDF\n",
    "\n",
    "$$\n",
    "f_X(x) \n",
    "= \\frac{1}{\\sqrt{2 \\pi}}\n",
    "\\exp\\left(\n",
    "- x^2/2\n",
    "\\right).\n",
    "$$\n",
    "\n",
    "Its mean is $0$ and its variance is $1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d993b3b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** The following function generates $n$ data points from a spherical $d$-dimensional Gaussians with variance $\\sigma^2$ and mean $\\bmu$.\n",
    "\n",
    "Below, [`rng.normal(0,1,(n,d))`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.normal.html) generates a `n` independent `d`-dimensional spherical Gaussian with mean $\\mathbf{0}$ (as row vectors).\n",
    "\n",
    "Throughout, when defining a function that uses a random number generator (RNG), we initialize the RNG outside the function and pass the RNG to it. It allows us to maintain control over the random number generation process at a higher level and ensures consistent results across multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a386a7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def spherical_gaussian(rng, d, n, mu, sig):\n",
    "    return mu + sig * rng.normal(0,1,(n,d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5b2e92",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We generate $100$ data points in dimension $d=2$. We take $\\sigma^2 = 1$ and $\\bmu = w \\mathbf{e}_1$. Below we use the function [`numpy.hstack`](https://numpy.org/doc/stable/reference/generated/numpy.hstack.html) to create a vector by concatenating two given vectors. We use `[w]` to create a vector with a single entry `w`.  We also use the function [`numpy.zeros`](https://numpy.org/doc/stable/reference/generated/numpy.zeros.html) to create an all-zero vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3439fc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "d, n, w, sig = 2, 100, 3., 1.\n",
    "mu = np.hstack(([w], np.zeros(d-1)))\n",
    "X = spherical_gaussian(rng, d, n, mu, sig)\n",
    "plt.scatter(X[:,0], X[:,1], s=5, c='k')\n",
    "plt.axis([-1, 7, -4, 4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf30393",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cebd98",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "This is straightforward to implement by using [`numpy.random.Generator.choice`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.choice.html) to choose the component of each sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5718c5b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The code is the following. It returns an `d` by `n` array `X`, where each row is a sample from a 2-component spherical Gaussian mixture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904da154",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def gmm2spherical(rng, d, n, phi0, phi1, mu0, sig0, mu1, sig1):\n",
    "    \n",
    "    phi, mu, sig = np.stack((phi0, phi1)), np.stack((mu0, mu1)), np.stack((sig0,sig1))\n",
    "\n",
    "    X = np.zeros((n,d))\n",
    "    component = rng.choice(2, size=n, p=phi)\n",
    "    for i in range(n):\n",
    "        X[i,:] = spherical_gaussian(rng, d, 1, mu[component[i],:], sig[component[i]])\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f919ff1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** Let us try it with following parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf46a5d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "d, n, w, sig0, sig1, phi0, phi1 = 2, 1000, 3., 1.5, 0.5, 0.2, 0.8\n",
    "mu0, mu1 = np.hstack(([w], np.zeros(d-1))), np.hstack(([-w], np.zeros(d-1)))\n",
    "X = gmm2spherical(rng, d, n, phi0, phi1, mu0, sig0, mu1, sig1)\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.scatter(X[:,0], X[:,1], s=5, color='k')\n",
    "plt.axis([-8, 8, -4, 4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63ab542",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "As expected, we observe two clusters. The one on the right (component $0$) is sparser (i.e., it contains fewer data points) since `phi0` is much smaller than `phi1`. It is also larger as its variance is larger. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0612a09",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f6eac8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Clustering: an objective, an algorithm and a guarantee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fe0f10",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** Here's a numerical example. We first define a quadratic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decff069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q(a, b, c, x):\n",
    "    return a * (x ** 2) + b * x + c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cd1125",
   "metadata": {
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We plot it for different values of the coefficients. Note that the $x$ and $y$ axes are on different scales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b784980",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-2, 2, 100)\n",
    "plt.plot(x, q(2, 4, -1, x))\n",
    "plt.plot(x, q(2, -4, 4, x))\n",
    "plt.plot(x, q(-2, 0, 4, x))\n",
    "plt.legend(['$y_1$', '$y_2$', '$y_3$'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01842973",
   "metadata": {
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290839ce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "### Lloyd's algorithm and its analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd1b238",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We are now ready to describe Lloyd's algorithm$\\idx{Lloyd's algorithm}\\xdi$. We start from a random assignment of clusters. (An alternative [initialization strategy](https://en.wikipedia.org/wiki/K-means_clustering#Initialization_methods) is to choose $k$ representatives at random among the data points.) We then alternate between the optimal choices in the lemmas. In lieu of pseudo-code, we write out the algorithm in Python. We will use this approach throughout the book."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4aa554",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The input `X` is assumed to be a collection of $n$ vectors $\\mathbf{x}_1, \\ldots, \\mathbf{x}_n \\in \\mathbb{R}^d$ stacked into a matrix, with one row for each data point. The other input, `k`, is the desired number of clusters. There is an optional input `maxiter` for the maximum number of iterations, which is set to $5$ by default.\n",
    "\n",
    "We first define separate functions for the two main steps. To find the minimum of an array, we use the function [`numpy.argmin`](https://numpy.org/doc/stable/reference/generated/numpy.argmin.html). We also use [`numpy.linalg.norm`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html) to compute the Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3171aad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def opt_reps(X, k, assign):\n",
    "    (n, d) = X.shape\n",
    "    reps = np.zeros((k, d))\n",
    "    for i in range(k):\n",
    "        in_i = [j for j in range(n) if assign[j] == i]             \n",
    "        reps[i,:] = np.sum(X[in_i,:],axis=0) / len(in_i)\n",
    "    return reps\n",
    "\n",
    "def opt_clust(X, k, reps):\n",
    "    (n, d) = X.shape\n",
    "    dist = np.zeros(n)\n",
    "    assign = np.zeros(n, dtype=int)\n",
    "    for j in range(n):\n",
    "        dist_to_i = np.array([LA.norm(X[j,:] - reps[i,:]) for i in range(k)])\n",
    "        assign[j] = np.argmin(dist_to_i)\n",
    "        dist[j] = dist_to_i[assign[j]]\n",
    "    G = np.sum(dist ** 2)\n",
    "    print(G)\n",
    "    return assign"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d58883d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The main function follows. Below, `rng.integers(0,k,n)` is an array of `n` uniformly chosen integers between `0` and `k-1` (inclusive). See [random.Generator.integers](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.integers.html) for details. Recall that throughout, when defining a function that uses a random number generator (RNG), we initialize the RNG outside the function and pass the RNG to it. It allows us to maintain control over the random number generation process at a higher level and ensures consistent results across multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c578b50",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def kmeans(rng, X, k, maxiter=5):\n",
    "    (n, d) = X.shape\n",
    "    assign = rng.integers(0,k,n)\n",
    "    reps = np.zeros((k, d), dtype=int)\n",
    "    for iter in range(maxiter):\n",
    "        reps = opt_reps(X, k, assign) \n",
    "        assign = opt_clust(X, k, reps) \n",
    "    return assign"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeb4423",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We apply our implementation of $k$-means to the example above. We fix `k` to $3$. Here the data matrix `X` is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f627ab1b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "seed = 535\n",
    "rng = np.random.default_rng(seed)\n",
    "X = np.array([[1., 0.],[-2., 0.],[-2.,1.],[1.,-3.],\n",
    "              [-10.,10.],[2.,-2.],[-3.,1.],[3.,-1.]])\n",
    "assign = kmeans(rng, X, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f929eba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We vizualize the output by coloring the points according to their cluster assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9dc65e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1], s=10, c=assign, cmap='brg')\n",
    "plt.axis([-11,4,-4,11])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b8a489",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We can compute the final representatives (optimal for the final assignment) by using the subroutine `opt_reps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cdd33c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(opt_reps(X, 3, assign))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a8613b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Each row is the center of the corresponding cluster. Note these match with the ones we previously computed. Indeed, the clustering is the same (although not necessarily in the same order)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201a20c4",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**TRY IT!** Modify `kmeans` to take a tolerance `tol` as input and stop when the improvement in objective value `G` falls below the tolerance. ([Open in Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_intro_notebook.ipynb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0045d5",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9872b0",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**CHAT & LEARN** AI chatbots can serve as great personal tutors, especially when it comes to coding which they often excel at. In particular, they can provide additional information about the code in this book. Just copy-paste a piece of code and ask \"What is this code doing?\" Don't hesitate to ask follow-up questions. [Here is an example using ChatGPT](https://chat.openai.com/share/66f9e704-c8e8-4489-9ab6-8e57740cab3b). \n",
    "\n",
    "*Warning:* As you probably know, AI chatbots can be wrong so assess what they tell you with a critical mind and/or double-check with other sources (e.g., package documentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa52f82",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "*Prompt engineering:* [Finding the right prompt](https://en.wikipedia.org/wiki/Prompt_engineering), or series of prompts, can make a big difference. It can also be used to produce more interesting answers. Here is an explanation of the $k$-means algorithm in the style of Shakespeare via ChatGPT:\n",
    "\n",
    "> **Scene: The Realm of Numerica, in the Enchanted Forest of Analysis**\n",
    "> \n",
    "> *Enter LORD VECTOR, LADY PARTITION, and an assembly of WHISPERING WINDS*\n",
    "> \n",
    "> LORD VECTOR (a majestic entity): \n",
    "> \n",
    "> Hearken, Whispering Winds, scattered far and wide, \n",
    "> \n",
    "> I am Lord Vector, your anchor and your guide. \n",
    "> \n",
    "> In clusters, you must settle, by my call and by my side, \n",
    "> \n",
    "> To me, you'll draw nigh, in unity, abide. \n",
    "> \n",
    "> LADY PARTITION (a sage enchantress):\n",
    "> \n",
    "> And I, Lady Partition, with vision clear and keen,\n",
    "> \n",
    "> Determine where amongst the clusters each of you shall lean.\n",
    "> \n",
    "> With Lord Vector, a ballet we compose, with steps so fine,\n",
    "> \n",
    "> To sort, to gather, to align, under a sign divine.\n",
    "> \n",
    "> WHISPERING WIND 1:\n",
    "> \n",
    "> But how, noble Vector, wilt thou find thy rightful stead?\n",
    "> \n",
    "> In this expanse diverse, how set thy bed?\n",
    "> \n",
    "> LORD VECTOR:\n",
    "> \n",
    "> Through cycles and through wisdom, I seek my sacred ground,\n",
    "> \n",
    "> Where the sum of all distances to me is least found.\n",
    "> \n",
    "> From your essence, I emerge, an average pure and sound,\n",
    "> \n",
    "> A beacon most true, by which I am crowned.\n",
    "> \n",
    "> WHISPERING WIND 2:\n",
    "> \n",
    "> And Lady Partition, how do we seek our kin,\n",
    "> \n",
    "> In this multitude, how does our journey begin?\n",
    "> \n",
    "> LADY PARTITION:\n",
    "> \n",
    "> By measure and affinity, to your closest Lord Vector, you'll gravitate,\n",
    "> \n",
    "> To him, your allegiance, you'll consecrate.\n",
    "> \n",
    "> In groups, you'll assemble, by likeness you're drawn,\n",
    "> \n",
    "> A collective symphony, from dusk till dawn.\n",
    "> \n",
    "> \n",
    "> *LORD VECTOR and LADY PARTITION assume their roles, guiding the WHISPERING WINDS to move and regroup.*\n",
    "> \n",
    "> \n",
    "> LORD VECTOR:\n",
    "> \n",
    "> Observe now how I traverse, as your allegiances refine,\n",
    "> \n",
    "> A center more apt, in this spell of mine.\n",
    "> \n",
    "> LADY PARTITION:\n",
    "> \n",
    "> And as you drift, dear winds, to your Lords true,\n",
    "> \n",
    "> A closer union we construe, a balance anew.\n",
    "> \n",
    "> \n",
    "> *The WHISPERING WINDS shuffle and align, moving from one cluster to another, as LORD VECTOR adjusts his position with grace.*\n",
    "> \n",
    "> \n",
    "> WHISPERING WIND 3:\n",
    "> \n",
    "> At length, I perceive, where I am to dwell,\n",
    "> \n",
    "> With peers akin, in cluster well.\n",
    "> \n",
    "> \n",
    "> LORD VECTOR:\n",
    "> \n",
    "> So we dance and weave, 'til stillness we achieve,\n",
    "> \n",
    "> When each wind has settled, and we, in relief.\n",
    "> \n",
    "> \n",
    "> LADY PARTITION:\n",
    "> \n",
    "> Thus, optimal clusters come forth, vivid and serene,\n",
    "> \n",
    "> With centers well placed, a sight unseen.\n",
    "> \n",
    "> \n",
    "> *The WHISPERING WINDS now rest peacefully in clusters around their LORD VECTORS, the Enchanted Forest of Analysis a testament to their orderly ballet.*\n",
    "> \n",
    "> \n",
    "> LORD VECTOR & LADY PARTITION (together):\n",
    "> \n",
    "> Our task here concludes, with harmony and grace,\n",
    "> \n",
    "> The K-means enchantment, a solution embraced.\n",
    "> \n",
    "> \n",
    "> *Exeunt omnes, as the WHISPERING WINDS repose in unity around their LORD VECTORS, the realm of Numerica reflecting their harmonious dance.*\n",
    "> \n",
    "> \n",
    "> **End of Scene**\n",
    "\n",
    "$\\ddagger$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41af1964",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We will test our implementation of $k$-means on the penguins dataset introduced earlier in the chapter. We first extract the columns and combine them into a data matrix `X`. As we did previously, we also remove the rows with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d97015",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('penguins-measurements.csv')\n",
    "data = data.dropna()\n",
    "X = data[['bill_length_mm', 'bill_depth_mm', \n",
    "        'flipper_length_mm', 'body_mass_g']].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccee712a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We  visualize a two-dimensional slice of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8b5235",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:,1], X[:,3], s=5, c='k')\n",
    "plt.xlabel('bill_depth_mm'), plt.ylabel('body_mass_g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6679b4c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Observe that the features have quite different scales (tens versus thousands in the plot above). In such a case, it is common to standardize the data so that each feature has roughly the same scale. For each column of `X`, we subtract its empirical mean and divide by its empirical standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e85766",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mean = np.mean(X, axis=0)\n",
    "std = np.std(X, axis=0)\n",
    "X = (X - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f35d45",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Now we run Lloyd's algorithm with $k=2$ clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaa661e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "assign = kmeans(rng, X, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60119b32",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We vizualize the output as we did before, but this time coloring the data points by their cluster assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376747d6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:,1], X[:,3], s=5, c=assign, cmap='brg')\n",
    "plt.xlabel('bill_depth (standardized)'), plt.ylabel('body_mass (standardized)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c490cfc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "This clustering looks quite good. Nevertheless recall that:\n",
    "\n",
    "1. in this plot we are looking at only two of the four variables while $k$-means uses all of them, \n",
    "\n",
    "2. we are not guaranteed to find the best solution, \n",
    "\n",
    "3. our objective function is somewhat arbitrary, and \n",
    "\n",
    "4. it is not clear what the right choice of $k$ is. \n",
    "\n",
    "In fact, the original dataset contained the correct answer, as provided by biologists. Despite what the figure above may lead us to believe, there are in reality three separate species. So let us try with $k=3$ clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2b97bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "assign = kmeans(rng, X, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f1b1dc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The output does not seem quite right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5a40d3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:,1], X[:,3], s=5, c=assign, cmap='brg')\n",
    "plt.xlabel('bill_depth (standardized)'), plt.ylabel('body_mass (standardized)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359b4a88",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "But, remembering the warnings mentioned previously, let us look at a different two-dimensional slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b20303",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,3], s=5, c=assign, cmap='brg')\n",
    "plt.xlabel('bill_length (standardized)'), plt.ylabel('body_mass (standardized)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9741239d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Let us load up the truth and compare. We only keep those samples that were not removed because of missing values (see [`pandas.DataFrame.iloc`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e8c04d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data_truth = pd.read_csv('penguins-species.csv') \n",
    "data_truth = data_truth.iloc[data.index]\n",
    "data_truth.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cc0687",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The species are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e24f51",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "species = data_truth['species']\n",
    "print(species.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2cb652",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To plot the outcome, we color the species blue-green-red using a [dictionary](https://docs.python.org/3/tutorial/datastructures.html#dictionaries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46706cca",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "species2color_dict = {'Adelie': 'blue', 'Chinstrap': 'lime', 'Gentoo': 'red'}\n",
    "truth = [species2color_dict[a] for a in species]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8f93f4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Finally, we can compare the output to the truth. The match is quite good -- but certainly not perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee94dba3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(6.5, 3))\n",
    "ax1.scatter(X[:,0], X[:,3], s=5, c=truth)\n",
    "ax1.set_title('truth')\n",
    "ax2.scatter(X[:,0], X[:,3], s=5, c=assign, cmap='brg')\n",
    "ax2.set_title('kmeans')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb10f78a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Determining the appropriate number of clusters is not a straighforward problem. To quote [Wikipedia](https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set):\n",
    "\n",
    "> The correct choice of $k$ is often ambiguous, with interpretations depending on the shape and scale of the distribution of points in a data set and the desired clustering resolution of the user. In addition, increasing $k$ without penalty will always reduce the amount of error in the resulting clustering, to the extreme case of zero error if each data point is considered its own cluster (i.e., when $k$ equals the number of data points, $n$). Intuitively then, the optimal choice of $k$ will strike a balance between maximum compression of the data using a single cluster, and maximum accuracy by assigning each data point to its own cluster. If an appropriate value of $k$ is not apparent from prior knowledge of the properties of the data set, it must be chosen somehow. There are several categories of methods for making this decision.\n",
    "\n",
    "\n",
    "In practice, [several heuristics](https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set) are in use. Other approaches to clustering, e.g. [DBSCAN](https://en.wikipedia.org/wiki/DBSCAN) and [hierarchical clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering), do not require a number of clusters as input. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070eb6f3",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**TRY IT!** Run the analysis again, but this time *without the standardization step*. What do you observe? Is one feature more influential on the final output than the others? Why do you think that is? ([Open in Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_intro_notebook.ipynb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37a94db",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c49b10",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Some observations about high-dimensional data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb59e19",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "### Clustering in high dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82bc1f0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "In this section, we test our implementation of $k$-means on a simple simulated dataset in high dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13e4238",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The following function generates $n$ data points from a mixture of two equally likely, spherical $d$-dimensional Gaussians with variance $1$, one with mean $-w\\mathbf{e}_1$ and one with mean $w \\mathbf{e}_1$. We use `gmm2spherical` from a previous section. It is found in `mmids.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782e9513",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def two_mixed_clusters(rng, d, n, w):\n",
    "    mu0 = np.hstack(([w], np.zeros(d-1)))\n",
    "    mu1 = np.hstack(([-w], np.zeros(d-1)))\n",
    "    return mmids.gmm2spherical(rng, d, n, 0.5, 0.5, mu0, 1, mu1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd01b7c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We start with $d=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f8101e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "seed = 535\n",
    "rng = np.random.default_rng(seed)\n",
    "d, n, w = 2, 100, 3.\n",
    "X = two_mixed_clusters(rng, d, n, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4bc1e6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Let's run $k$-means on this dataset using $k=2$. We use `kmeans()` from the `mmids.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bd154d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "assign = mmids.kmeans(rng, X, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4073bda",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Our default of $10$ iterations seem to have been enough for the algorithm to converge. We can visualize the result by [coloring](https://matplotlib.org/stable/api/_as_gen/matplotlib.lines.Line2D.html) the points according to the assignment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41a02df",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,3))\n",
    "plt.scatter(X[:,0], X[:,1], s=10, c=assign, cmap='brg')\n",
    "plt.axis([-6,6,-3,3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8cf91a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Let's see what happens in higher dimension. We repeat our experiment with $d=1000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbca413",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "outputs": [],
   "source": [
    "d, n, w = 1000, 100, 3.\n",
    "X = two_mixed_clusters(rng, d, n, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d07c36",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Again, we observe two clearly delineated clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8470a05a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,3))\n",
    "plt.scatter(X[:,0], X[:,1], s=10, c='k')\n",
    "plt.axis([-6,6,-3,3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3d3fcf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "This dataset is in $1000$ dimensions, but we've plotted the data in only the first two dimensions. If we plot in any two dimensions not including the first one instead, we see only one cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878c9286",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,3))\n",
    "plt.scatter(X[:,1], X[:,2], s=10, c='k')\n",
    "plt.axis([-6,6,-3,3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f1943a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Let's see how $k$-means fares on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92245e0d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "assign = mmids.kmeans(rng, X, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a04ae94",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Our attempt at clustering does not appear to have been successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e25557a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,3))\n",
    "plt.scatter(X[:,0], X[:,1], s=10, c=assign, cmap='brg')\n",
    "plt.axis([-6,6,-3,3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bf9a54",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06842047",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "What happened? While the clusters are easy to tease apart *if we know to look at the first coordinate only*, in the full space the within-cluster and between-cluster distances become harder to distinguish: the noise overwhelms the signal. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c20b7ad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "As the dimension increases, the distributions of intra-cluster and inter-cluster distances overlap significantly and become more or less indistinguishable. That provides some insights into why clustering may fail here. Note that we used the same offset for all simulations. On the other hand, if the separation between the clusters is sufficiently large, one would expect clustering to work even in high dimension. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a7ebe3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**TRY IT!** What precedes (and what follows in the next subsection) is not a formal proof that $k$-means clustering will be unsuccessful here. The behavior of the algorithm is quite complex and depends, in particular, on the initialization and the density of points. Here, increasing the number of data points eventually leads to a much better performance. Explore this behavior on your own by modifying the code. (For some theoretical justifications (beyond this course), see [here](https://arxiv.org/pdf/0912.0086.pdf) and [here](http://www.stat.yale.edu/~pollard/Papers/Pollard81AS.pdf).)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb85a75",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We can check the theorem in a simulation. Here we pick $n$ points uniformly at random in the $d$-cube $\\mathcal{C}$, for a range of dimensions up to `dmax`. We then plot the frequency of landing in the inscribed $d$-ball $\\mathcal{B}$ and see that it rapidly converges to $0$. Alternatively, we could just plot the formula for the volume of $\\mathcal{B}$. But knowing how to do simulations is useful in situations where explicit formulas are unavailable or intractable. We plot the result up to dimension $10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b46e26e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dmax, n = 10, 1000\n",
    "\n",
    "in_ball = np.zeros(dmax)\n",
    "for d in range(dmax):\n",
    "    in_ball[d] = np.mean([(LA.norm(rng.random(d+1) - 1/2) < 1/2) for _ in range(n)])\n",
    "    \n",
    "plt.plot(np.arange(1,dmax+1), in_ball, c='k') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22837566",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
