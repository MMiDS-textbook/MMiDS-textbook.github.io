
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>1.2. Background: quick refresher of matrix algebra, differential calculus, and elementary probability &#8212; MMiDS Textbook</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=87e54e7c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-P5E8DW088F"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-P5E8DW088F');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-P5E8DW088F');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chap01_intro/02_review/roch-mmids-intro-review';</script>
    <link rel="canonical" href="https://mmids-textbook.github.io/chap01_intro/02_review/roch-mmids-intro-review.html" />
    <link rel="icon" href="https://mmids-textbook.github.io/favicon-32x32.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="1.3. Clustering: an objective, an algorithm and a guarantee" href="../03_clustering/roch-mmids-intro-clustering.html" />
    <link rel="prev" title="1.1. Motivating example: identifying penguin species" href="../01_motiv/roch-mmids-intro-motiv.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/mmids-cover-small.jpg" class="logo__image only-light" alt="MMiDS Textbook - Home"/>
    <script>document.write(`<img src="../../_static/mmids-cover-small.jpg" class="logo__image only-dark" alt="MMiDS Textbook - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../index.html">
                    <b>MATHEMATICAL METHODS in DATA SCIENCE (with Python)</b>
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../00_intro/roch-mmids-intro-intro.html">1. Introduction: a first data science problem</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../01_motiv/roch-mmids-intro-motiv.html">1.1. Motivating example: identifying penguin species</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">1.2. Background: quick refresher of matrix algebra, differential calculus, and elementary probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03_clustering/roch-mmids-intro-clustering.html">1.3. Clustering: an objective, an algorithm and a guarantee</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04_highdim/roch-mmids-intro-highdim.html">1.4. Some observations about high-dimensional data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../exercises/roch-mmids-intro-exercises.html">1.5. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../supp/roch-mmids-intro-supp.html">1.6. Online supplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap02_ls/00_intro/roch-mmids-ls-intro.html">2. Least squares: geometric, algebraic, and numerical aspects</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/01_motiv/roch-mmids-ls-motiv.html">2.1. Motivating example: predicting sales</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/02_spaces/roch-mmids-ls-spaces.html">2.2. Background: review of vector spaces and matrix inverses</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/03_orthog/roch-mmids-ls-orthog.html">2.3. Geometry of least squares: the orthogonal projection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/04_qr/roch-mmids-ls-qr.html">2.4. QR decomposition and Householder transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/05_regression/roch-mmids-ls-regression.html">2.5. Application: regression analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/exercises/roch-mmids-ls-exercises.html">2.6. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/supp/roch-mmids-ls-supp.html">2.7. Online supplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap03_opt/00_intro/roch-mmids-opt-intro.html">3. Optimization theory and algorithms</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap03_opt/01_motiv/roch-mmids-opt-motiv.html">3.1. Motivating example:  analyzing customer satisfaction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap03_opt/02_several/roch-mmids-opt-several.html">3.2. Background: review of differentiable functions of several variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap03_opt/03_optimality/roch-mmids-opt-optimality.html">3.3. Optimality conditions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap03_opt/04_convexity/roch-mmids-opt-convexity.html">3.4. Convexity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap03_opt/05_gd/roch-mmids-opt-gd.html">3.5. Gradient descent and its convergence analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap03_opt/06_logistic/roch-mmids-opt-logistic.html">3.6. Application: logistic regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap03_opt/exercises/roch-mmids-opt-exercises.html">3.7. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap03_opt/supp/roch-mmids-opt-supp.html">3.8. Online supplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap04_svd/00_intro/roch-mmids-svd-intro.html">4. Singular value decomposition</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_svd/01_motiv/roch-mmids-svd-motiv.html">4.1. Motivating example: visualizing viral evolution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_svd/02_spectral/roch-mmids-svd-spectral.html">4.2. Background: review of matrix rank  and spectral decomposition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_svd/03_svd/roch-mmids-svd-svd.html">4.3. Approximating subspaces and the SVD</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_svd/04_power/roch-mmids-svd-power.html">4.4. Power iteration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_svd/05_pca/roch-mmids-svd-pca.html">4.5. Application: principal components analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_svd/06_further/roch-mmids-svd-further.html">4.6. Further applications of the SVD: low-rank approximations and ridge regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_svd/exercises/roch-mmids-svd-exercises.html">4.7. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap04_svd/supp/roch-mmids-svd-supp.html">4.8. Online supplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap05_specgraph/00_intro/roch-mmids-specgraph-intro.html">5. Spectral graph theory</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap05_specgraph/01_motiv/roch-mmids-specgraph-motiv.html">5.1. Motivating example: uncovering social groups</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap05_specgraph/02_graph/roch-mmids-specgraph-graph.html">5.2. Background: basic concepts in graph theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap05_specgraph/03_extremal/roch-mmids-specgraph-extremal.html">5.3. Variational characterization of eigenvalues</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap05_specgraph/04_laplacian/roch-mmids-specgraph-laplacian.html">5.4. Spectral properties of the Laplacian matrix</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap05_specgraph/05_partitioning/roch-mmids-specgraph-partitioning.html">5.5. Application: graph partitioning via spectral clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap05_specgraph/06_sbm/roch-mmids-specgraph-sbm.html">5.6. Erdős-Rényi random graph and stochastic blockmodel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap05_specgraph/exercises/roch-mmids-specgraph-exercises.html">5.7. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap05_specgraph/supp/roch-mmids-specgraph-supp.html">5.8. Online supplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap06_prob/00_intro/roch-mmids-prob-intro.html">6. Probabilistic models: from simple to complex</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap06_prob/01_motiv/roch-mmids-prob-motiv.html">6.1. Motivating example: tracking location</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap06_prob/02_parametric/roch-mmids-prob-parametric.html">6.2. Background: introduction to parametric families and maximum likelihood estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap06_prob/03_joint/roch-mmids-prob-joint.html">6.3. Modeling more complex dependencies 1: using conditional independence</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap06_prob/04_em/roch-mmids-prob-em.html">6.4. Modeling more complex dependencies 2: marginalizing out an unobserved variable</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap06_prob/05_kalman/roch-mmids-prob-kalman.html">6.5. Application: linear-Gaussian models and Kalman filtering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap06_prob/exercises/roch-mmids-prob-exercises.html">6.6. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap06_prob/supp/roch-mmids-prob-supp.html">6.7. Online supplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap07_rwmc/00_intro/roch-mmids-rwmc-intro.html">7. Random walks on graphs and Markov chains</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap07_rwmc/01_motiv/roch-mmids-rwmc-motiv.html">7.1. Motivating example: discovering mathematical topics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap07_rwmc/02_mcdefs/roch-mmids-rwmc-mcdefs.html">7.2. Background: elements of finite Markov chains</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap07_rwmc/03_stat/roch-mmids-rwmc-stat.html">7.3. Limit behavior 1: stationary distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap07_rwmc/04_mclimit/roch-mmids-rwmc-mclimit.html">7.4. Limit behavior 2: convergence to equilibrium</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap07_rwmc/05_pagerank/roch-mmids-rwmc-pagerank.html">7.5. Application: random walks on graphs and PageRank</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap07_rwmc/06_gibbs/roch-mmids-rwmc-gibbs.html">7.6. Further applications: Gibbs sampling and generating images</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap07_rwmc/exercises/roch-mmids-rwmc-exercises.html">7.7. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap07_rwmc/supp/roch-mmids-rwmc-supp.html">7.8. Online supplementary material</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap08_nn/00_intro/roch-mmids-nn-intro.html">8. Neural networks, backpropagation and stochastic gradient descent</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/01_motiv/roch-mmids-nn-motiv.html">8.1. Motivating example:  classifying natural images</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/02_chain/roch-mmids-nn-chain.html">8.2. Background: Jacobian, chain rule, and a brief introduction to automatic differentiation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/03_backprop/roch-mmids-nn-backprop.html">8.3. Building blocks of AI 1: backpropagation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/04_sgd/roch-mmids-nn-sgd.html">8.4. Building blocks of AI 2: stochastic gradient descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/05_nn/roch-mmids-nn-nn.html">8.5. Building blocks of AI 3: neural networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/exercises/roch-mmids-nn-exercises.html">8.6. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap08_nn/supp/roch-mmids-nn-supp.html">8.7. Online supplementary material</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/issues/new?title=Issue%20on%20page%20%2Fchap01_intro/02_review/roch-mmids-intro-review.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/chap01_intro/02_review/roch-mmids-intro-review.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Background: quick refresher of matrix algebra, differential calculus, and elementary probability</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vectors-and-matrices">1.2.1. Vectors and matrices</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#differential-calculus">1.2.2. Differential calculus</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability">1.2.3. Probability</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <p><span class="math notranslate nohighlight">\(\newcommand{\bmu}{\boldsymbol{\mu}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bSigma}{\boldsymbol{\Sigma}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bfbeta}{\boldsymbol{\beta}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bflambda}{\boldsymbol{\lambda}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bgamma}{\boldsymbol{\gamma}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bsigma}{{\boldsymbol{\sigma}}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bpi}{\boldsymbol{\pi}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\btheta}{{\boldsymbol{\theta}}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bphi}{\boldsymbol{\phi}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\balpha}{\boldsymbol{\alpha}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\blambda}{\boldsymbol{\lambda}}\)</span>
<span class="math notranslate nohighlight">\(\renewcommand{\P}{\mathbb{P}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\E}{\mathbb{E}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\indep}{\perp\!\!\!\perp} \newcommand{\bx}{\mathbf{x}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bp}{\mathbf{p}}\)</span>
<span class="math notranslate nohighlight">\(\renewcommand{\bx}{\mathbf{x}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bX}{\mathbf{X}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\by}{\mathbf{y}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bY}{\mathbf{Y}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bz}{\mathbf{z}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bZ}{\mathbf{Z}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bw}{\mathbf{w}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bW}{\mathbf{W}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bv}{\mathbf{v}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bV}{\mathbf{V}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bfg}{\mathbf{g}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bfh}{\mathbf{h}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\horz}{\rule[.5ex]{2.5ex}{0.5pt}}\)</span>
<span class="math notranslate nohighlight">\(\renewcommand{\S}{\mathcal{S}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\X}{\mathcal{X}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\var}{\mathrm{Var}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\pa}{\mathrm{pa}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\Z}{\mathcal{Z}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bh}{\mathbf{h}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bb}{\mathbf{b}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bc}{\mathbf{c}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\cE}{\mathcal{E}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\cP}{\mathcal{P}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bbeta}{\boldsymbol{\beta}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bLambda}{\boldsymbol{\Lambda}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\cov}{\mathrm{Cov}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\bfk}{\mathbf{k}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\idx}[1]{}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\xdi}{}\)</span></p>
<section id="background-quick-refresher-of-matrix-algebra-differential-calculus-and-elementary-probability">
<h1><span class="section-number">1.2. </span>Background: quick refresher of matrix algebra, differential calculus, and elementary probability<a class="headerlink" href="#background-quick-refresher-of-matrix-algebra-differential-calculus-and-elementary-probability" title="Link to this heading">#</a></h1>
<p>We first review a few basic mathematical concepts. In this chapter, we focus on vector and matrix algebra, some basic calculus and optimization, as well as elementary probability. Along the way, we also introduce Python, especially the library Numpy which will be used throughout.</p>
<section id="vectors-and-matrices">
<h2><span class="section-number">1.2.1. </span>Vectors and matrices<a class="headerlink" href="#vectors-and-matrices" title="Link to this heading">#</a></h2>
<p>Throughout, <span class="math notranslate nohighlight">\(\mathbb{R}\)</span> and <span class="math notranslate nohighlight">\(\mathbb{R}_+\)</span> denote respectively the real numbers and positive real numbers.</p>
<p><strong>Vectors and norms</strong> For a vector</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{x} 
= 
\begin{bmatrix}
x_1 \\
x_2 \\
\vdots \\
x_d
\end{bmatrix}
\in \mathbb{R}^d
\end{split}\]</div>
<p>the Euclidean norm<span class="math notranslate nohighlight">\(\idx{Euclidean norm}\xdi\)</span> of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[
\|\mathbf{x}\|_2 
= 
\sqrt{
\sum_{i=1}^d x_i^2
} 
= 
\sqrt{\langle \mathbf{x}, \mathbf{x}\rangle}
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
\langle \mathbf{u}, \mathbf{v} \rangle = \sum_{i=1}^d u_i v_i
\]</div>
<p>is the <a class="reference external" href="https://en.wikipedia.org/wiki/Inner_product_space">inner product</a><span class="math notranslate nohighlight">\(\idx{inner product}\xdi\)</span> of <span class="math notranslate nohighlight">\(\mathbf{u}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{v}\)</span>.
This is also known as the <span class="math notranslate nohighlight">\(\ell^2\)</span>-norm. Throughout we use the notation <span class="math notranslate nohighlight">\(\|\mathbf{x}\| = \|\mathbf{x}\|_2\)</span> to indicate the <span class="math notranslate nohighlight">\(2\)</span>-norm of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> unless specified otherwise. We use <span class="math notranslate nohighlight">\(x_i\)</span> and <span class="math notranslate nohighlight">\((\mathbf{x})_i\)</span> to denote the entries of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. We also write <span class="math notranslate nohighlight">\(\mathbf{x} = (x_1, \ldots, x_d) = (x_{i})_{i\in [d]}\)</span>, where <span class="math notranslate nohighlight">\([d] := \{1,2,\ldots,d\}\)</span>.</p>
<p>The inner product has the following useful properties (check them!). For one, it is symmetric in the sense that</p>
<div class="math notranslate nohighlight">
\[
\langle \mathbf{x}, \mathbf{y} \rangle 
= \langle \mathbf{y}, \mathbf{x} \rangle \qquad \forall \mathbf{x}, \mathbf{y} \in \mathbb{R}^d.
\]</div>
<p>Second, it is linear in each input: for any <span class="math notranslate nohighlight">\(\mathbf{x}_1, \mathbf{x}_2, \mathbf{x}_3 \in \mathbb{R}^d\)</span> and <span class="math notranslate nohighlight">\(\beta \in \mathbb{R}\)</span>, it holds that</p>
<div class="math notranslate nohighlight">
\[
\langle \beta \,\mathbf{x}_1 + \mathbf{x}_2, \mathbf{x}_3 \rangle = \beta \,\langle \mathbf{x}_1,\mathbf{x}_3\rangle + \langle \mathbf{x}_2,\mathbf{x}_3\rangle.
\]</div>
<p>Repeated application of the latter property implies for instance that: for any <span class="math notranslate nohighlight">\(\mathbf{x}_1, \ldots, \mathbf{x}_m, \mathbf{y}_1, \ldots, \mathbf{y}_\ell \in \mathbb{R}^d\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\left\langle \sum_{i=1}^m \mathbf{x}_i, \sum_{j=1}^\ell \mathbf{y}_j \right\rangle
= \sum_{i=1}^m \sum_{j=1}^\ell \langle \mathbf{x}_i,\mathbf{y}_j\rangle.
\]</div>
<p>The triangle inequality for the <span class="math notranslate nohighlight">\(\ell^2\)</span>-norm <a class="reference external" href="https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality#Analysis">follows</a> from the <a class="reference external" href="https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality">Cauchy-Schwarz inequality</a>, which is useful in proving many facts.</p>
<p><strong>THEOREM</strong> <strong>(Cauchy-Schwarz)</strong> <span class="math notranslate nohighlight">\(\idx{Cauchy-Schwarz inequality}\xdi\)</span> For all <span class="math notranslate nohighlight">\(\mathbf{u}, \mathbf{v} \in \mathbb{R}^d\)</span></p>
<div class="math notranslate nohighlight">
\[
|\langle \mathbf{u}, \mathbf{v} \rangle| 
\leq \|\mathbf{u}\| \|\mathbf{v}\|.
\]</div>
<p><span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p>Given a collection of vectors <span class="math notranslate nohighlight">\(\mathbf{u}_1,\ldots,\mathbf{u}_k \in \mathbb{R}^d\)</span> and real numbers <span class="math notranslate nohighlight">\(\alpha_1,\ldots,\alpha_k \in \mathbb{R}\)</span>, the linear combination of <span class="math notranslate nohighlight">\(\mathbf{u}_\ell\)</span>’s with coefficients <span class="math notranslate nohighlight">\(\alpha_\ell\)</span>’s is the vector</p>
<div class="math notranslate nohighlight">
\[
\mathbf{z} 
= \sum_{\ell=1}^k \alpha_\ell \mathbf{u}_\ell,
\]</div>
<p>whose entries are</p>
<div class="math notranslate nohighlight">
\[
z_i
= \sum_{\ell=1}^k \alpha_\ell (\mathbf{u}_\ell)_i, \quad i=1,\ldots,d.
\]</div>
<p>We also use <span class="math notranslate nohighlight">\(u_{\ell, i} = (\mathbf{u}_\ell)_i\)</span> to denote the entries of <span class="math notranslate nohighlight">\(\mathbf{u}_\ell\)</span>.</p>
<p>It will be convenient to introduce special notation for common vectors. The dimension of these vectors will often be clear from the context.</p>
<ul class="simple">
<li><p>The all-<span class="math notranslate nohighlight">\(0\)</span> vector in <span class="math notranslate nohighlight">\(d\)</span> dimensions is denoted by <span class="math notranslate nohighlight">\(\mathbf{0}_d = \mathbf{0}\)</span>.</p></li>
<li><p>The all-<span class="math notranslate nohighlight">\(1\)</span> vector in <span class="math notranslate nohighlight">\(d\)</span> dimensions is denoted by <span class="math notranslate nohighlight">\(\mathbf{1}_d = \mathbf{1}\)</span>.</p></li>
<li><p>The standard or canonical basis is denoted by <span class="math notranslate nohighlight">\(\mathbf{e}_i\)</span>, <span class="math notranslate nohighlight">\(i=1,\ldots,d\)</span>, where</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
(\mathbf{e}_i)_j 
= \begin{cases}
1, &amp; \text{if $j = i$,}\\
0, &amp; \text{o.w.}
\end{cases}
\end{split}\]</div>
<p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Euclidean_distance">Euclidean distance</a><span class="math notranslate nohighlight">\(\idx{Euclidean distance}\xdi\)</span> between two vectors <span class="math notranslate nohighlight">\(\mathbf{u}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> in <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span> is the <span class="math notranslate nohighlight">\(2\)</span>-norm of their difference</p>
<div class="math notranslate nohighlight">
\[
d(\mathbf{u},\mathbf{v})
= \|\mathbf{u} - \mathbf{v}\|_2.
\]</div>
<p>More generally, for <span class="math notranslate nohighlight">\(p \geq 1\)</span>, the <a class="reference external" href="https://en.wikipedia.org/wiki/Lp_space#The_p-norm_in_finite_dimensions"><span class="math notranslate nohighlight">\(\ell^p\)</span>-norm</a><span class="math notranslate nohighlight">\(\idx{lp-norm}\xdi\)</span> of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[
\|\mathbf{x}\|_p 
= 
\left(
\sum_{i=1}^d |x_i|^p
\right)^{1/p}.
\]</div>
<p>Finally the <span class="math notranslate nohighlight">\(\ell^\infty\)</span>-norm is defined as</p>
<div class="math notranslate nohighlight">
\[
\|\mathbf{x}\|_\infty = \max_{i=1,\ldots,d}|x_i|.
\]</div>
<p>There exist other norms. Formally:</p>
<p><strong>DEFINITION</strong> <strong>(Norm)</strong> <span class="math notranslate nohighlight">\(\idx{norm}\xdi\)</span> A norm is a function <span class="math notranslate nohighlight">\(\ell\)</span> from <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span> to <span class="math notranslate nohighlight">\(\mathbb{R}_+\)</span> that satisfies for all <span class="math notranslate nohighlight">\(a \in \mathbb{R}\)</span>,
<span class="math notranslate nohighlight">\(\mathbf{u}, \mathbf{v} \in \mathbb{R}^d\)</span></p>
<ul class="simple">
<li><p><em>(Absolute homogeneity):</em> <span class="math notranslate nohighlight">\(\idx{absolute homogeneity}\xdi\)</span> <span class="math notranslate nohighlight">\(\ell(a \mathbf{u}) = |a| \ell(\mathbf{u})\)</span></p></li>
<li><p><em>(Triangle inequality):</em> <span class="math notranslate nohighlight">\(\idx{triangle inequality}\xdi\)</span> <span class="math notranslate nohighlight">\(\ell(\mathbf{u}+\mathbf{v}) \leq \ell(\mathbf{u}) + \ell(\mathbf{v})\)</span></p></li>
<li><p><em>(Point-separating):</em> <span class="math notranslate nohighlight">\(\idx{point-separating property}\xdi\)</span> <span class="math notranslate nohighlight">\(\ell(\mathbf{u}) = 0\)</span> implies <span class="math notranslate nohighlight">\(\mathbf{u} =0\)</span>.</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\natural\)</span></p>
<p><strong>NUMERICAL CORNER:</strong> In Numpy, a vector is defined as a 1d array. We first must import the <a class="reference external" href="https://numpy.org">Numpy</a> package, which is often abbreviated by <code class="docutils literal notranslate"><span class="pre">np</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">5.</span> <span class="p">,</span><span class="mf">7.</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1. 3. 5. 7.]
</pre></div>
</div>
</div>
</div>
<p>We access the entries of <code class="docutils literal notranslate"><span class="pre">u</span></code> as follows, where note that indexing in Numpy starts at <span class="math notranslate nohighlight">\(0\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">u</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">u</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.0
3.0
</pre></div>
</div>
</div>
</div>
<p>To obtain the norm of a vector, we can use the function <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html"><code class="docutils literal notranslate"><span class="pre">linalg.norm</span></code></a>, which requires the <a class="reference external" href="https://numpy.org/doc/stable/reference/routines.linalg.html"><code class="docutils literal notranslate"><span class="pre">numpy.linalg</span></code></a> package (often abbreviated as <code class="docutils literal notranslate"><span class="pre">LA</span></code>):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">linalg</span> <span class="k">as</span> <span class="n">LA</span>
<span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>9.16515138991168
</pre></div>
</div>
</div>
</div>
<p>which we check next “by hand”</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">u</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>9.16515138991168
</pre></div>
</div>
</div>
</div>
<p>In Numpy, <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.power.html"><code class="docutils literal notranslate"><span class="pre">**</span></code></a> indicates element-wise exponentiation.</p>
<p><strong>TRY IT!</strong> Compute the inner product of <span class="math notranslate nohighlight">\(u = (1,2,3,4)\)</span> and <span class="math notranslate nohighlight">\(v = (5, 4, 3, 2)\)</span> without using the function <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.dot.html"><code class="docutils literal notranslate"><span class="pre">np.dot</span></code></a>. <em>Hint</em>: The product of two real numbers <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> is <code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">*</span> <span class="pre">b</span></code>. (<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_intro_notebook.ipynb">Open in Colab</a>)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span> <span class="p">,</span><span class="mf">4.</span><span class="p">])</span>
<span class="c1"># EDIT THIS LINE: define v</span>
<span class="c1"># EDIT THIS LINE: compute the inner product between u and v</span>
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>Matrices</strong> For an <span class="math notranslate nohighlight">\(n \times m\)</span> matrix <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times m}\)</span> with real entries, we denote by <span class="math notranslate nohighlight">\(A_{i,j}\)</span> or <span class="math notranslate nohighlight">\(A_{ij}\)</span> its entry in row <span class="math notranslate nohighlight">\(i\)</span> and column <span class="math notranslate nohighlight">\(j\)</span> (unless specified otherwise). We also refer to a matrix as the collection of all of its entries as follows</p>
<div class="math notranslate nohighlight">
\[
A = (A_{ij})_{i\in [n],j \in [m]}.
\]</div>
<p>We occasionally simplify the notation to <span class="math notranslate nohighlight">\(A = (A_{ij})_{i,j}\)</span> when the range of the indices is clear from context. We use the notation</p>
<div class="math notranslate nohighlight">
\[
A_{i,\cdot} = (A_{i1} \cdots A_{im}),
\]</div>
<p>to indicate the <span class="math notranslate nohighlight">\(i\)</span>-th row of <span class="math notranslate nohighlight">\(A\)</span> – as a row vector, i.e., a matrix with a single row – and similarly</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A_{\cdot,j} = \begin{pmatrix}
A_{1j}\\
\vdots\\
A_{nj}
\end{pmatrix},
\end{split}\]</div>
<p>for the <span class="math notranslate nohighlight">\(j\)</span>-th column of <span class="math notranslate nohighlight">\(A\)</span> – as a column vector, i.e., a matrix with a single column.</p>
<p><strong>EXAMPLE:</strong> Suppose</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A 
= \begin{bmatrix}
2 &amp; 5\\
3 &amp; 6\\
1 &amp; 1
\end{bmatrix}.
\end{split}\]</div>
<p>Then the second row and column are respectively</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A_{2,\cdot} = \begin{bmatrix}
3 &amp; 6
\end{bmatrix}
\quad\text{and}\quad
A_{\cdot,2} 
= \begin{bmatrix}
5\\
6\\
1
\end{bmatrix}.
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p>Matrices can be multiplied by scalars: let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n\times m} = (A_{ij})_{i\in [n],j \in [m]}\)</span> and <span class="math notranslate nohighlight">\(\gamma \in \mathbb{R}\)</span>, then <span class="math notranslate nohighlight">\(\gamma A = (\gamma A_{ij})_{i\in [n],j \in [m]}\)</span> is the matrix whose entries are multiplied by <span class="math notranslate nohighlight">\(\gamma\)</span>. Matrices can also be added to each other – provided they have the same size: let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n\times m} = (A_{ij})_{i\in [n],j \in [m]}\)</span> and <span class="math notranslate nohighlight">\(B \in \mathbb{R}^{n\times m} = (B_{ij})_{i\in [n],j \in [m]}\)</span>, then <span class="math notranslate nohighlight">\(C = A + B\)</span> is the matrix <span class="math notranslate nohighlight">\(C = (C_{ij})_{i\in [n],j \in [m]}\)</span> where <span class="math notranslate nohighlight">\(C_{ij} = A_{ij} + B_{ij}\)</span> for all <span class="math notranslate nohighlight">\(i,j\)</span>.</p>
<p>Recall that the <a class="reference external" href="https://en.wikipedia.org/wiki/Transpose">transpose</a><span class="math notranslate nohighlight">\(\idx{transpose}\xdi\)</span> <span class="math notranslate nohighlight">\(A^T\)</span> of a matrix <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n\times m}\)</span> is defined as the matrix in <span class="math notranslate nohighlight">\(\mathbb{R}^{m\times n}\)</span> that switches the row and column indices of <span class="math notranslate nohighlight">\(A\)</span>, that is, its entries are</p>
<div class="math notranslate nohighlight">
\[
[A^T]_{ij} = A_{ji},\quad i=1,\ldots,m, j=1,\ldots,n.
\]</div>
<p><strong>Figure:</strong> Illustration of transposition (<a class="reference external" href="https://commons.wikimedia.org/wiki/File:Matrix_transpose.gif">Source</a>)</p>
<p><img alt="transpose" src="https://upload.wikimedia.org/wikipedia/commons/e/e4/Matrix_transpose.gif" /></p>
<p><span class="math notranslate nohighlight">\(\bowtie\)</span></p>
<p><strong>EXAMPLE:</strong> Suppose again</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A 
= \begin{bmatrix}
2 &amp; 5\\
3 &amp; 6\\
1 &amp; 1
\end{bmatrix}.
\end{split}\]</div>
<p>Then its transpose is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A^T
= \begin{bmatrix}
2 &amp; 3 &amp; 1\\
5 &amp; 6 &amp; 1
\end{bmatrix}.
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p>We list some useful properties of the transpose (check them!). For any <span class="math notranslate nohighlight">\(\gamma \in \mathbb{R}\)</span> and <span class="math notranslate nohighlight">\(A, B \in \mathbb{R}^{n \times m}\)</span>:</p>
<p>a) <span class="math notranslate nohighlight">\((A^T)^T = A\)</span></p>
<p>b) <span class="math notranslate nohighlight">\((\gamma A + B)^T = \gamma A^T + B^T\)</span></p>
<p><strong>DEFINITION</strong> <strong>(Symmetric Matrix)</strong> <span class="math notranslate nohighlight">\(\idx{symmetric matrix}\xdi\)</span> A square matrix <span class="math notranslate nohighlight">\(B \in \mathbb{R}^{n \times n}\)</span> is symmetric if <span class="math notranslate nohighlight">\(B^T = B\)</span>. <span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>The transpose in particular can be used to turn a column vector into a row vector and vice versa. That is, if <span class="math notranslate nohighlight">\(\mathbf{b} = (b_1, b_2, \ldots,b_n) \in \mathbb{R}^n\)</span> is a column vector, then <span class="math notranslate nohighlight">\(\mathbf{b}^T = \begin{pmatrix}
b_1 &amp;
b_2 &amp;
\cdots &amp;
b_n
\end{pmatrix}\)</span> is the corresponding row vector. Note the absence of commas in the latter. For instance,</p>
<div class="math notranslate nohighlight">
\[
\mathbf{b}^T \mathbf{b} = b_1^2 + \cdots + b_n^2 = \sum_{i=1}^n b_i^2
= \|\mathbf{b}\|^2
\]</div>
<p>is the squared Euclidean norm of <span class="math notranslate nohighlight">\(\mathbf{b}\)</span>.</p>
<p><strong>NUMERICAL CORNER:</strong> We will often work with collections of <span class="math notranslate nohighlight">\(n\)</span> vectors <span class="math notranslate nohighlight">\(\mathbf{x}_1, \ldots, \mathbf{x}_n\)</span> in <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span> and it will be convenient to stack them up into a matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
X =
\begin{bmatrix}
\mathbf{x}_1^T \\
\mathbf{x}_2^T \\
\vdots \\
\mathbf{x}_n^T \\
\end{bmatrix}
=
\begin{bmatrix}
x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1d} \\
x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2d} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{nd} \\
\end{bmatrix}.
\end{split}\]</div>
<p>To create a matrix out of two vectors, we use the function <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.stack.html"><code class="docutils literal notranslate"><span class="pre">numpy.stack</span></code></a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">7.</span><span class="p">])</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">])</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">u</span><span class="p">,</span><span class="n">v</span><span class="p">),</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[1. 3. 5. 7.]
 [2. 4. 6. 8.]]
</pre></div>
</div>
</div>
</div>
<p>Quoting the documentation:</p>
<blockquote>
<div><p>The axis parameter specifies the index of the new axis in the dimensions of the result. For example, if axis=0 it will be the first dimension and if axis=-1 it will be the last dimension.</p>
</div></blockquote>
<p>Alternatively, we can define the same matrix as follows.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">7.</span><span class="p">],[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[1. 3. 5. 7.]
 [2. 4. 6. 8.]]
</pre></div>
</div>
</div>
</div>
<p>We access the entries as follows.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.0
3.0
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p>As for vectors, it will be convenient to introduce a special notation for common matrices. The dimensions are sometimes omitted when clear from context.</p>
<ul class="simple">
<li><p>The all-<span class="math notranslate nohighlight">\(0\)</span> matrix of dimension <span class="math notranslate nohighlight">\(m \times n\)</span> is denoted by <span class="math notranslate nohighlight">\(\mathbf{0}_{m \times n} = \mathbf{0}\)</span>.</p></li>
<li><p>The all-<span class="math notranslate nohighlight">\(1\)</span> matrix of dimension <span class="math notranslate nohighlight">\(m \times n\)</span> is denoted by <span class="math notranslate nohighlight">\(J_{m \times n} = J\)</span>.</p></li>
<li><p>A square diagonal matrix<span class="math notranslate nohighlight">\(\idx{diagonal matrix}\xdi\)</span> <span class="math notranslate nohighlight">\(A = (A_{ij}) \in \mathbb{R}^{n \times n}\)</span> is a matrix that satisfies <span class="math notranslate nohighlight">\(A_{ij} = 0\)</span> for all <span class="math notranslate nohighlight">\(i \neq j\)</span>. We denote by <span class="math notranslate nohighlight">\(\mathrm{diag}(\lambda_1,\ldots,\lambda_n)\)</span> the diagonal matrix with diagonal entries <span class="math notranslate nohighlight">\(\lambda_1,\ldots,\lambda_n\)</span>.</p></li>
<li><p>The identity matrix<span class="math notranslate nohighlight">\(\idx{identity matrix}\xdi\)</span> of dimension <span class="math notranslate nohighlight">\(n \times n\)</span> is denoted by <span class="math notranslate nohighlight">\(I_{n \times n} = I\)</span>. Specifically, this the matrix whose <span class="math notranslate nohighlight">\(i\)</span>-th column is the standard basis vector <span class="math notranslate nohighlight">\(\mathbf{e}_i\)</span>, <span class="math notranslate nohighlight">\(i=1,\ldots,n\)</span>. Put differently, it is the square diagonal matrix with ones on the diagonal.</p></li>
</ul>
<p><strong>Matrix-vector product</strong> Recall that, for a matrix <span class="math notranslate nohighlight">\(A = (A_{ij})_{i\in [n],j \in [m]} \in \mathbb{R}^{n \times m}\)</span> and a column vector <span class="math notranslate nohighlight">\(\mathbf{b} = (b_{i})_{i\in [m]} \in \mathbb{R}^{m}\)</span>, the matrix-vector product <span class="math notranslate nohighlight">\(\mathbf{c} = A \mathbf{b}\)</span> is the vector with entries</p>
<div class="math notranslate nohighlight">
\[
c_i = (A\mathbf{b})_i = \sum_{j=1}^m A_{ij} b_j.
\]</div>
<p>In vector form,</p>
<div class="math notranslate nohighlight">
\[
A \mathbf{b} = \sum_{j=1}^m A_{\cdot,j} b_j,
\]</div>
<p>that is, <span class="math notranslate nohighlight">\(A \mathbf{b}\)</span> is a linear combination of the columns of <span class="math notranslate nohighlight">\(A\)</span> where the coefficients are the entries of <span class="math notranslate nohighlight">\(\mathbf{b}\)</span>. Matrix-vector products are linear in the following sense (check it!): for any <span class="math notranslate nohighlight">\(\gamma \in \mathbb{R}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b}_1, \mathbf{b}_2 \in \mathbb{R}^m\)</span></p>
<div class="math notranslate nohighlight">
\[
A(\gamma \mathbf{b}_1 + \mathbf{b}_2)
= \gamma A\mathbf{b}_1 + A \mathbf{b}_2.
\]</div>
<p><strong>EXAMPLE:</strong> <strong>(continued)</strong> Consider the column vector <span class="math notranslate nohighlight">\(\mathbf{b} = (1, 0)\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A \mathbf{b} 
= \begin{bmatrix}
2(1) + 5(0)\\
3(1) + 6(0)\\
1(1) + 1(0)
\end{bmatrix}
= \begin{bmatrix}
2\\
3\\
1
\end{bmatrix},
\end{split}\]</div>
<p>which can also be written in vector form as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
(1) \begin{bmatrix}
2\\
3\\
1
\end{bmatrix}
+
(0) \begin{bmatrix}
5\\
6\\
1
\end{bmatrix}
= \begin{bmatrix}
2\\
3\\
1
\end{bmatrix}.
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>Matrix-matrix product</strong> Recall that, for matrices <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times k}\)</span> and <span class="math notranslate nohighlight">\(B \in \mathbb{R}^{k \times m}\)</span>, their matrix product is defined as the matrix <span class="math notranslate nohighlight">\(C = AB \in \mathbb{R}^{n \times m}\)</span> whose entries are</p>
<div class="math notranslate nohighlight">
\[
C_{i\ell} = (AB)_{i\ell} = \sum_{j=1}^k A_{ij} B_{j\ell}.
\]</div>
<p>The number of columns of <span class="math notranslate nohighlight">\(A\)</span> and the number of rows of <span class="math notranslate nohighlight">\(B\)</span> must match. There are many different ways to view this formula that are helpful in interpreting matrix-matrix products in different contexts.</p>
<p>First, we observe that the entry <span class="math notranslate nohighlight">\(C_{i\ell}\)</span> is an inner product of the <span class="math notranslate nohighlight">\(i\)</span>-th row of <span class="math notranslate nohighlight">\(A\)</span> and of the <span class="math notranslate nohighlight">\(\ell\)</span>-th column of <span class="math notranslate nohighlight">\(B\)</span>. That is,</p>
<div class="math notranslate nohighlight">
\[
C_{i\ell} = A_{i,\cdot} B_{\cdot,\ell}.
\]</div>
<p>In matrix form,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
AB = \begin{bmatrix}
A_{1,\cdot} B_{\cdot,1} &amp; A_{1,\cdot} B_{\cdot,2} &amp; \cdots &amp; A_{1,\cdot} B_{\cdot,m} \\
A_{2,\cdot} B_{\cdot,1} &amp; A_{2,\cdot} B_{\cdot,2} &amp; \cdots &amp; A_{2,\cdot} B_{\cdot,m} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
A_{n,\cdot} B_{\cdot,1} &amp; A_{n,\cdot} B_{\cdot,2} &amp; \cdots &amp; A_{n,\cdot} B_{\cdot,m}
\end{bmatrix}.
\end{split}\]</div>
<p>Alternatively,</p>
<div class="math notranslate nohighlight">
\[
AB
= 
\begin{bmatrix}
A (B_{\cdot,1}) &amp; A (B_{\cdot,2}) &amp; \cdots &amp; A (B_{\cdot,m})
\end{bmatrix},
\]</div>
<p>where we specify a matrix by the collection of its columns. Put differently, by the matrix-vector product formula, the <span class="math notranslate nohighlight">\(j\)</span>-th column of the product <span class="math notranslate nohighlight">\(AB\)</span> is a linear combination of the columns of <span class="math notranslate nohighlight">\(A\)</span> where the coefficients are the entries in column <span class="math notranslate nohighlight">\(j\)</span> of <span class="math notranslate nohighlight">\(B\)</span></p>
<div class="math notranslate nohighlight">
\[
(AB)_{\cdot,j} 
= A B_{\cdot,j}
= \sum_{\ell=1}^k A_{\cdot,\ell} B_{\ell j}.
\]</div>
<p>Similarly, the <span class="math notranslate nohighlight">\(i\)</span>-th row of the product <span class="math notranslate nohighlight">\(AB\)</span> is a linear combination of the rows of <span class="math notranslate nohighlight">\(B\)</span> where the coefficients are the entries in row <span class="math notranslate nohighlight">\(i\)</span> of <span class="math notranslate nohighlight">\(A\)</span></p>
<div class="math notranslate nohighlight">
\[
(AB)_{i,\cdot} = \sum_{\ell=1}^k A_{i\ell} B_{\ell,\cdot}.
\]</div>
<p><strong>EXAMPLE:</strong> Recall that if we think of a vector <span class="math notranslate nohighlight">\(\mathbf{b} \in \mathbb{R}^n\)</span> as a column vector, then its transpose <span class="math notranslate nohighlight">\(\mathbf{b}^T\)</span> is a row vector. We previously showed that <span class="math notranslate nohighlight">\(\mathbf{b}^T \mathbf{b} = \sum_{i=1}^n b_i^2\)</span> is a scalar, i.e., a real number. This time, we compute <span class="math notranslate nohighlight">\(\mathbf{b} \mathbf{b}^T\)</span>. Let us first make sure that the dimensions fit. Seeing these vectors as matrices, we have <span class="math notranslate nohighlight">\(\mathbf{b} \in \mathbb{R}^{n\times 1}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b}^T \in \mathbb{R}^{1\times n}\)</span>. So indeed we can multiply them together since the number of columns of the first matrix matches the number of rows of the second one. What are the dimensions of the final product? Taking the number of rows of the first matrix and the number of columns of the second one, we see that it is <span class="math notranslate nohighlight">\(n \times n\)</span>.</p>
<p>Finally we get</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{b} \mathbf{b}^T
= \begin{bmatrix}
b_1\\
b_2\\
\vdots\\
b_n
\end{bmatrix}
\begin{bmatrix}
b_1 &amp;
b_2 &amp;
\cdots &amp;
b_n
\end{bmatrix}
= 
\begin{bmatrix}
b_{1} b_{1} &amp; b_{1} b_{2} &amp; \cdots &amp; b_{1} b_{n} \\
b_{2} b_{1} &amp; b_{2} b_{2} &amp; \cdots &amp; b_{2} b_{n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
b_{n} b_{1} &amp; b_{n} b_{2} &amp; \cdots &amp; b_{n} b_{n}
\end{bmatrix}.
\end{split}\]</div>
<p>That is, <span class="math notranslate nohighlight">\((\mathbf{b} \mathbf{b}^T)_{i,j} = b_i b_j\)</span>. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p>We list some useful properties of the matrix-matrix product (check them!). For any <span class="math notranslate nohighlight">\(\gamma \in \mathbb{R}\)</span>, <span class="math notranslate nohighlight">\(A, B \in \mathbb{R}^{n \times m}\)</span> and <span class="math notranslate nohighlight">\(C \in \mathbb{R}^{m \times \ell}\)</span>:</p>
<p>a) <span class="math notranslate nohighlight">\((\gamma A)B = A (\gamma B) = \gamma A B\)</span></p>
<p>b) <span class="math notranslate nohighlight">\((A + B)C = AC + BC\)</span></p>
<p>c) <span class="math notranslate nohighlight">\((BC)^T = C^T B^T\)</span></p>
<p><strong>KNOWLEDGE CHECK:</strong> Let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times m}\)</span>, <span class="math notranslate nohighlight">\(B \in \mathbb{R}^{m \times n}\)</span>, <span class="math notranslate nohighlight">\(C \in \mathbb{R}^{n \times \ell}\)</span>, and <span class="math notranslate nohighlight">\(D \in \mathbb{R}^{\ell \times n}\)</span>. Determine the dimensions of the <em>transpose</em> of the matrix:</p>
<div class="math notranslate nohighlight">
\[
(A^T + B) C D
\]</div>
<p>a) <span class="math notranslate nohighlight">\(m \times m\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(n \times n\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(m \times n\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(n \times m\)</span></p>
<p>e) The matrix is not well-defined.</p>
<p><span class="math notranslate nohighlight">\(\checkmark\)</span></p>
<p><strong>Block matrices</strong> It will be convenient to introduce block matrices. First, for a vector <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^n\)</span>, we write <span class="math notranslate nohighlight">\(\mathbf{x} = (\mathbf{x}_1, \mathbf{x}_2)\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{x}_1 \in \mathbb{R}^{n_1}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{x}_2 \in \mathbb{R}^{n_2}\)</span> with <span class="math notranslate nohighlight">\(n_1 + n_2 = n\)</span>, to indicate that <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is partitioned into two blocks: <span class="math notranslate nohighlight">\(\mathbf{x}_1\)</span> corresponds to the first <span class="math notranslate nohighlight">\(n_1\)</span> coordinates of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> while <span class="math notranslate nohighlight">\(\mathbf{x}_2\)</span> corresponds to the following <span class="math notranslate nohighlight">\(n_2\)</span> coordinates.</p>
<p>More generally, a block matrix<span class="math notranslate nohighlight">\(\idx{block matrix}\xdi\)</span> is a partitioning of the rows and columns of a matrix of the form</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A 
=
\begin{pmatrix}
A_{11} &amp; A_{12}\\
A_{21} &amp; A_{22}
\end{pmatrix}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times m}\)</span>, <span class="math notranslate nohighlight">\(A_{ij} \in \mathbb{R}^{n_i \times m_j}\)</span> for <span class="math notranslate nohighlight">\(i,j = 1, 2\)</span> with the conditions <span class="math notranslate nohighlight">\(n_1 + n_2 = n\)</span> and <span class="math notranslate nohighlight">\(m_1 + m_2 = m\)</span>. One can also consider larger numbers of blocks.</p>
<p>Block matrices have a convenient algebra that mimics the usual matrix algebra. Specifically, if <span class="math notranslate nohighlight">\(B_{ij} \in \mathbb{R}^{m_i \times p_j}\)</span> for <span class="math notranslate nohighlight">\(i,j = 1, 2\)</span>, then it holds that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{pmatrix}
A_{11} &amp; A_{12}\\
A_{21} &amp; A_{22}
\end{pmatrix}
\begin{pmatrix}
B_{11} &amp; B_{12}\\
B_{21} &amp; B_{22}
\end{pmatrix}
=
\begin{pmatrix}
A_{11} B_{11} + A_{12} B_{21} &amp; A_{11} B_{12} + A_{12} B_{22}\\
A_{21} B_{11} + A_{22} B_{21} &amp; A_{21} B_{12} + A_{22} B_{22}
\end{pmatrix}.
\end{split}\]</div>
<p>Observe that the block sizes of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> must match for this formula to make sense. You can convince yourself of this identity by trying it on a simple example.</p>
<p><em>Warning:</em> While the formula is similar to the usual matrix product, the order of multiplication matters because the blocks are matrices and they do not in general commute!</p>
<p><strong>Matrix norms</strong> We will also need notions of matrix norm<span class="math notranslate nohighlight">\(\idx{matrix norm}\xdi\)</span>. A natural way to define a norm for matrices is to notice that an <span class="math notranslate nohighlight">\(n \times m\)</span> matrix <span class="math notranslate nohighlight">\(A\)</span> can be thought of as an <span class="math notranslate nohighlight">\(nm\)</span> vector, with one element for each entry of <span class="math notranslate nohighlight">\(A\)</span>. Indeed, addition and scalar multiplication work exactly in the same way. Hence, we can define the <span class="math notranslate nohighlight">\(2\)</span>-norm of a matrix in terms of the sum of its squared entries. (We will encounter other matrix norms later in the course.)</p>
<p><strong>DEFINITION</strong> <strong>(Frobenius Norm)</strong> <span class="math notranslate nohighlight">\(\idx{Frobenius norm}\xdi\)</span> The Frobenius norm of an <span class="math notranslate nohighlight">\(n \times m\)</span> matrix <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times m}\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[
\|A\|_F
= \sqrt{\sum_{i=1}^n \sum_{j=1}^m A_{ij}^2}.
\]</div>
<p><span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>Using the row notation, we see that the square of the Frobenius norm can be written as the sum of the squared Euclidean norms of the rows <span class="math notranslate nohighlight">\(\|A\|_F^2 = \sum_{i=1}^n \|A_{i,\cdot}\|^2\)</span>. Similarly in terms of the columns <span class="math notranslate nohighlight">\(A_{\cdot,j}\)</span>, <span class="math notranslate nohighlight">\(j=1,\ldots,m\)</span>, of <span class="math notranslate nohighlight">\(A\)</span> we have
<span class="math notranslate nohighlight">\(\|A\|_F^2 = \sum_{j=1}^m \|A_{\cdot,j}\|^2\)</span>.</p>
<p>For two matrices <span class="math notranslate nohighlight">\(A, B \in \mathbb{R}^{n \times m}\)</span>, the Frobenius norm of their difference <span class="math notranslate nohighlight">\(\|A - B\|_F\)</span> can be interpreted as a distance between <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>, that is, a measure of how dissimilar they are.</p>
<p>It can be shown (try it!) using the <em>Cauchy-Schwarz inequality</em> that for any <span class="math notranslate nohighlight">\(A, B\)</span> for which <span class="math notranslate nohighlight">\(AB\)</span> is well-defined it holds that</p>
<div class="math notranslate nohighlight">
\[
\|A B \|_F \leq \|A\|_F \|B\|_F.
\]</div>
<p>This applies in particular when <span class="math notranslate nohighlight">\(B\)</span> is a column vector, in which case <span class="math notranslate nohighlight">\(\|B\|_F\)</span> is its Euclidean norm.</p>
<p><strong>NUMERICAL CORNER:</strong> In Numpy, the Frobenius norm of a matrix can be computed using the function <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html"><code class="docutils literal notranslate"><span class="pre">numpy.linalg.norm</span></code></a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[1. 0.]
 [0. 1.]
 [0. 0.]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.4142135623730951
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>Quadratic forms</strong> Let <span class="math notranslate nohighlight">\(B \in \mathbb{R}^{n \times n}\)</span> be a square matrix. The associated quadratic form<span class="math notranslate nohighlight">\(\idx{quadratic form}\xdi\)</span></p>
<div class="math notranslate nohighlight">
\[
\langle \mathbf{z}, B \mathbf{z} \rangle 
= \mathbf{z}^T B \mathbf{z}
= \sum_{i=1}^n z_i \sum_{j=1}^n B_{i,j} z_j
= \sum_{i=1}^n \sum_{j=1}^n z_i B_{i,j} z_j
\]</div>
<p>defined for any <span class="math notranslate nohighlight">\(\mathbf{z} = (z_1,\ldots,z_n)\)</span>, will make many appearances throughout.</p>
<p>A <a class="reference external" href="https://en.wikipedia.org/wiki/Homogeneous_polynomial">form</a> is a homogeneous polynomial <span class="math notranslate nohighlight">\(f(\mathbf{z})\)</span>, viewed as a function of <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>. By <a class="reference external" href="https://en.wikipedia.org/wiki/Homogeneous_function">homogeneous</a>, we mean that for any <span class="math notranslate nohighlight">\(\mathbf{z} \in \mathbb{R}^n\)</span> and any scalar <span class="math notranslate nohighlight">\(\alpha \in \mathbb{R}\)</span></p>
<div class="math notranslate nohighlight">
\[
f(\alpha \mathbf{z})
= \alpha^k f(\mathbf{z}),
\]</div>
<p>for some integer <span class="math notranslate nohighlight">\(k\)</span> that is called the degree of homogeneity. (Note that this is different from the absolute homogeneity of norms.) When <span class="math notranslate nohighlight">\(k=2\)</span>, we refer to it as a quadratic form. Let us check that <span class="math notranslate nohighlight">\(\langle \mathbf{z}, B \mathbf{z} \rangle\)</span> indeed satisfies these properties. The alternative expression <span class="math notranslate nohighlight">\(\sum_{i=1}^n \sum_{j=1}^n z_i B_{i,j} z_j\)</span> makes it clear that it is a polynomial in the variables <span class="math notranslate nohighlight">\(z_1,\ldots,z_n\)</span>. Moreover, for any <span class="math notranslate nohighlight">\(\alpha \in \mathbb{R}\)</span>, by using linearity multiple times</p>
<div class="math notranslate nohighlight">
\[
\langle \alpha \mathbf{z}, B (\alpha \mathbf{z}) \rangle
= \langle \alpha \mathbf{z}, \alpha B \mathbf{z} \rangle
=  \alpha \langle \mathbf{z}, \alpha B \mathbf{z} \rangle
=  \alpha^2 \langle \mathbf{z}, B \mathbf{z} \rangle.
\]</div>
<p>In particular, the following property of matrices will play an important role. It is defined in terms of the associated quadratic form.</p>
<p><strong>DEFINITION</strong> <strong>(Positive Semidefinite Matrix)</strong> <span class="math notranslate nohighlight">\(\idx{positive semidefinite matrix}\xdi\)</span> A symmetric matrix <span class="math notranslate nohighlight">\(B \in \mathbb{R}^{n \times n}\)</span> is positive semidefinite if</p>
<div class="math notranslate nohighlight">
\[
\langle \mathbf{z}, B \mathbf{z} \rangle \geq 0, \quad \forall \mathbf{z} \neq \mathbf{0}. 
\]</div>
<p>We also write <span class="math notranslate nohighlight">\(B \succeq 0\)</span> in that case. If the inequality above is strict, we say that <span class="math notranslate nohighlight">\(B\)</span> is positive definite, in which case we write <span class="math notranslate nohighlight">\(B \succ 0\)</span>. <span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>We will see an important example later in this section.</p>
</section>
<section id="differential-calculus">
<h2><span class="section-number">1.2.2. </span>Differential calculus<a class="headerlink" href="#differential-calculus" title="Link to this heading">#</a></h2>
<p>Next, we review some basic concepts from differential calculus. We focus here on definitions and results relevant to optimization theory, which plays a central role in data science.</p>
<p><strong>Limits and continuity</strong> The open <span class="math notranslate nohighlight">\(r\)</span>-ball around <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^d\)</span> is the set of points within Euclidean distance <span class="math notranslate nohighlight">\(r\)</span> of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, that is,</p>
<div class="math notranslate nohighlight">
\[
B_r(\mathbf{x}) = \{\mathbf{y}  \in \mathbb{R}^d \,:\, \|\mathbf{y} - \mathbf{x}\| &lt; r\}.
\]</div>
<p>A point <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^d\)</span> is an interior point of a set <span class="math notranslate nohighlight">\(A \subseteq \mathbb{R}^d\)</span> if there exists an <span class="math notranslate nohighlight">\(r &gt; 0\)</span> such that <span class="math notranslate nohighlight">\(B_r(\mathbf{x}) \subseteq A\)</span>.
A set <span class="math notranslate nohighlight">\(A\)</span> is <a class="reference external" href="https://en.wikipedia.org/wiki/Open_set">open</a><span class="math notranslate nohighlight">\(\idx{open set}\xdi\)</span> if it consists entirely of interior points. A point <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^d\)</span> is a limit point of a set <span class="math notranslate nohighlight">\(A \subseteq \mathbb{R}^d\)</span> if every open ball around <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> contains an element <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> of <span class="math notranslate nohighlight">\(A\)</span> such that <span class="math notranslate nohighlight">\(\mathbf{a} \neq \mathbf{x}\)</span>. A set <span class="math notranslate nohighlight">\(A\)</span> is <a class="reference external" href="https://en.wikipedia.org/wiki/Closed_set">closed</a><span class="math notranslate nohighlight">\(\idx{closed set}\xdi\)</span> if every limit point of <span class="math notranslate nohighlight">\(A\)</span> belongs to <span class="math notranslate nohighlight">\(A\)</span>. Or, put differently, a set is <a class="reference external" href="https://en.wikipedia.org/wiki/Closed_set">closed</a> if its complement is open. A set <span class="math notranslate nohighlight">\(A \subseteq \mathbb{R}^d\)</span> is bounded if there exists an <span class="math notranslate nohighlight">\(r &gt; 0\)</span> such that <span class="math notranslate nohighlight">\(A \subseteq B_r(\mathbf{0})\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{0} = (0,\ldots,0)^T\)</span>.</p>
<p><strong>Figure:</strong> Illustration of an open ball of radius <span class="math notranslate nohighlight">\(\varepsilon\)</span> around point <span class="math notranslate nohighlight">\(x\)</span>. (<a class="reference external" href="https://commons.wikimedia.org/wiki/File:Open_set_-_example.png">Source</a>)</p>
<p><img alt="open ball" src="https://upload.wikimedia.org/wikipedia/commons/thumb/d/de/Open_set_-_example.png/640px-Open_set_-_example.png" /></p>
<p><span class="math notranslate nohighlight">\(\bowtie\)</span></p>
<p><strong>DEFINITION</strong> <strong>(Limits of a Function)</strong> Let <span class="math notranslate nohighlight">\(f: D \to \mathbb{R}\)</span> be a real-valued function on <span class="math notranslate nohighlight">\(D \subseteq \mathbb{R}^d\)</span>. Then <span class="math notranslate nohighlight">\(f\)</span> is said to have a limit <span class="math notranslate nohighlight">\(L \in \mathbb{R}\)</span> as <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> approaches <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> if: for any <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span>, there exists a <span class="math notranslate nohighlight">\(\delta &gt; 0\)</span> such that <span class="math notranslate nohighlight">\(|f(\mathbf{x}) - L| &lt; \varepsilon\)</span> for all <span class="math notranslate nohighlight">\(\mathbf{x} \in D \cap B_\delta(\mathbf{a})\setminus \{\mathbf{a}\}\)</span>. This is written as</p>
<div class="math notranslate nohighlight">
\[
\lim_{\mathbf{x} \to \mathbf{a}} f(\mathbf{x}) = L.
\]</div>
<p><span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>Note that we explicitly exclude <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> itself from having to satisfy the condition <span class="math notranslate nohighlight">\(|f(\mathbf{x}) - L| &lt; \varepsilon\)</span>. In particular, we may have <span class="math notranslate nohighlight">\(f(\mathbf{a}) \neq L\)</span>. We also do not restrict <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> to be in <span class="math notranslate nohighlight">\(D\)</span>.</p>
<p><strong>DEFINITION</strong> <strong>(Continuous Function)</strong> <span class="math notranslate nohighlight">\(\idx{continuous function}\xdi\)</span> Let <span class="math notranslate nohighlight">\(f: D \to \mathbb{R}\)</span> be a real-valued function on <span class="math notranslate nohighlight">\(D \subseteq \mathbb{R}^d\)</span>. Then <span class="math notranslate nohighlight">\(f\)</span> is said to be continuous at <span class="math notranslate nohighlight">\(\mathbf{a} \in D\)</span> if</p>
<div class="math notranslate nohighlight">
\[
\lim_{\mathbf{x} \to \mathbf{a}} f(\mathbf{x}) = f(\mathbf{a}).
\]</div>
<p><span class="math notranslate nohighlight">\(\natural\)</span></p>
<p><strong>Figure:</strong> A continuous function. E.g., in a small neighborhood around <span class="math notranslate nohighlight">\(2\)</span>, the function varies only slightly. (<a class="reference external" href="https://commons.wikimedia.org/wiki/File:Example_of_continuous_function.svg">Source</a>)</p>
<p><img alt="A continuous function" src="https://upload.wikimedia.org/wikipedia/commons/thumb/7/7f/Example_of_continuous_function.svg/463px-Example_of_continuous_function.svg.png" /></p>
<p><span class="math notranslate nohighlight">\(\bowtie\)</span></p>
<p>We will not prove the following fundamental analysis result, which will be used repeatedly in this course. (See e.g. <a class="reference external" href="https://en.wikipedia.org/wiki/Extreme_value_theorem">Wikipedia</a> for a sketch of the proof.) Suppose <span class="math notranslate nohighlight">\(f : D \to \mathbb{R}\)</span> is defined on a set <span class="math notranslate nohighlight">\(D \subseteq \mathbb{R}^d\)</span>. We say that <span class="math notranslate nohighlight">\(f\)</span> attains a maximum value <span class="math notranslate nohighlight">\(M\)</span> at <span class="math notranslate nohighlight">\(\mathbf{z}^*\)</span> if <span class="math notranslate nohighlight">\(f(\mathbf{z}^*) = M\)</span> and <span class="math notranslate nohighlight">\(M \geq f(\mathbf{x})\)</span> for all <span class="math notranslate nohighlight">\(\mathbf{x} \in D\)</span>. Similarly, we say <span class="math notranslate nohighlight">\(f\)</span> attains a minimum value <span class="math notranslate nohighlight">\(m\)</span> at <span class="math notranslate nohighlight">\(\mathbf{z}_*\)</span> if <span class="math notranslate nohighlight">\(f(\mathbf{z}_*) = m\)</span> and <span class="math notranslate nohighlight">\(m \geq f(\mathbf{x})\)</span> for all <span class="math notranslate nohighlight">\(\mathbf{x} \in D\)</span>.</p>
<p><strong>THEOREM</strong> <strong>(Extreme Value)</strong> <span class="math notranslate nohighlight">\(\idx{extreme value theorem}\xdi\)</span> Let <span class="math notranslate nohighlight">\(f : D \to \mathbb{R}\)</span> be a real-valued, continuous function on a nonempty, closed, bounded set <span class="math notranslate nohighlight">\(D\subseteq \mathbb{R}^d\)</span>. Then <span class="math notranslate nohighlight">\(f\)</span> attains a maximum and a minimum on <span class="math notranslate nohighlight">\(D\)</span>. <span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p><strong>Derivatives</strong> We move on to derivatives. Recall that the derivative of a function of a real variable is the rate of change of the function with respect to the change in the variable. It gives the slope of the tangent line at a point. Formally:</p>
<p><strong>DEFINITION</strong> <strong>(Derivative)</strong> <span class="math notranslate nohighlight">\(\idx{derivative}\xdi\)</span> Let <span class="math notranslate nohighlight">\(f : D \to \mathbb{R}\)</span> where <span class="math notranslate nohighlight">\(D \subseteq \mathbb{R}\)</span> and let <span class="math notranslate nohighlight">\(x_0 \in D\)</span> be an interior point of <span class="math notranslate nohighlight">\(D\)</span>. The derivative of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(x_0\)</span> is</p>
<div class="math notranslate nohighlight">
\[
f'(x_0) 
= \frac{\mathrm{d} f (x_0)}{\mathrm{d} x}
= \lim_{h \to 0} \frac{f(x_0 + h) - f(x_0)}{h}
\]</div>
<p>provided the limit exists. <span class="math notranslate nohighlight">\(\natural\)</span></p>
<p><strong>Figure:</strong> The derivative at the blue point is the slope of the line tangent the curve there (with help from ChatGPT; inspired by <a class="reference external" href="https://commons.wikimedia.org/wiki/File:Tangent_to_a_curve.svg">Source</a>).</p>
<p><img alt="The derivative at the red point is the slope of the line tangent the curve there (with help from ChatGPT; inspired by Source)" src="../../_images/tangent.png" /></p>
<p><span class="math notranslate nohighlight">\(\bowtie\)</span></p>
<p>The following lemma encapsulates a key insight about the derivative of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(x_0\)</span>: it tells us where to find smaller values.</p>
<p><strong>LEMMA</strong> <strong>(Descent Direction)</strong> <span class="math notranslate nohighlight">\(\idx{descent direction lemma}\xdi\)</span> Let <span class="math notranslate nohighlight">\(f : D \to \mathbb{R}\)</span> with <span class="math notranslate nohighlight">\(D \subseteq \mathbb{R}\)</span> and let <span class="math notranslate nohighlight">\(x_0 \in D\)</span> be an interior point of <span class="math notranslate nohighlight">\(D\)</span> where <span class="math notranslate nohighlight">\(f'(x_0)\)</span> exists. If <span class="math notranslate nohighlight">\(f'(x_0) &gt; 0\)</span>, then there is an open ball <span class="math notranslate nohighlight">\(B_\delta(x_0) \subseteq D\)</span> around <span class="math notranslate nohighlight">\(x_0\)</span>
such that for each <span class="math notranslate nohighlight">\(x\)</span> in <span class="math notranslate nohighlight">\(B_\delta(x_0)\)</span>:</p>
<p>a) <span class="math notranslate nohighlight">\(f(x) &gt; f(x_0)\)</span> if <span class="math notranslate nohighlight">\(x &gt; x_0\)</span>,</p>
<p>b) <span class="math notranslate nohighlight">\(f(x) &lt; f(x_0)\)</span> if <span class="math notranslate nohighlight">\(x &lt; x_0\)</span>.</p>
<p>If instead <span class="math notranslate nohighlight">\(f'(x_0) &lt; 0\)</span>, the opposite holds. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof idea:</em> It follows from the definition of the derivative by taking <span class="math notranslate nohighlight">\(\varepsilon\)</span> small enough that <span class="math notranslate nohighlight">\(f'(x_0) - \varepsilon &gt; 0\)</span>.</p>
<p><em>Proof:</em> Take <span class="math notranslate nohighlight">\(\varepsilon = f'(x_0)/2\)</span>. By definition of the derivative, there is <span class="math notranslate nohighlight">\(\delta &gt; 0\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
f'(x_0)
-
\frac{f(x_0 + h) - f(x_0)}{h}
&lt; \varepsilon
\]</div>
<p>for all <span class="math notranslate nohighlight">\(0 &lt; h &lt; \delta\)</span>. Rearranging gives</p>
<div class="math notranslate nohighlight">
\[
f(x_0 + h) 
&gt; f(x_0) + [f'(x_0) - \varepsilon] h
&gt; f(x_0)
\]</div>
<p>by our choice of <span class="math notranslate nohighlight">\(\varepsilon\)</span>. The other direction is similar. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>One implication of the <em>Descent Direction Lemma</em> is the <em>Mean Value Theorem</em>, which will lead us later to <em>Taylor’s Theorem</em>. First, an important special case:</p>
<p><strong>THEOREM</strong> <strong>(Rolle)</strong> <span class="math notranslate nohighlight">\(\idx{Rolle's theorem}\xdi\)</span> Let <span class="math notranslate nohighlight">\(f : [a,b] \to \mathbb{R}\)</span> be a continuous function and assume that its derivative exists on <span class="math notranslate nohighlight">\((a,b)\)</span>. If <span class="math notranslate nohighlight">\(f(a) = f(b)\)</span> then there is <span class="math notranslate nohighlight">\(a &lt; c &lt; b\)</span> such that <span class="math notranslate nohighlight">\(f'(c) = 0\)</span>. <span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p><em>Proof idea:</em> Look at an extremum and use the <em>Descent Direction Lemma</em> to get a contradiction.</p>
<p><em>Proof:</em> If <span class="math notranslate nohighlight">\(f(x) = f(a)\)</span> for all <span class="math notranslate nohighlight">\(x \in (a, b)\)</span>, then <span class="math notranslate nohighlight">\(f'(x) = 0\)</span> on <span class="math notranslate nohighlight">\((a, b)\)</span> and we are done. So assume there is <span class="math notranslate nohighlight">\(y \in (a, b)\)</span> such that <span class="math notranslate nohighlight">\(f(y) \neq f(a)\)</span>. Assume without loss of generality that <span class="math notranslate nohighlight">\(f(y) &gt; f(a)\)</span> (otherwise consider the function <span class="math notranslate nohighlight">\(-f\)</span>). By the <em>Extreme Value Theorem</em>, <span class="math notranslate nohighlight">\(f\)</span> attains a maximum value at some <span class="math notranslate nohighlight">\(c \in [a,b]\)</span>. By our assumption, <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> cannot be the location of the maximum and it must be that <span class="math notranslate nohighlight">\(c \in (a, b)\)</span>.</p>
<p>We claim that <span class="math notranslate nohighlight">\(f'(c) = 0\)</span>. We argue by contradiction. Suppose <span class="math notranslate nohighlight">\(f'(c) &gt; 0\)</span>. By the <em>Descent Direction Lemma</em>, there is a <span class="math notranslate nohighlight">\(\delta &gt; 0\)</span> such that <span class="math notranslate nohighlight">\(f(x) &gt; f(c)\)</span> for some <span class="math notranslate nohighlight">\(x \in B_\delta(c)\)</span>, a contradiction. A similar argument holds if <span class="math notranslate nohighlight">\(f'(c) &lt; 0\)</span>. That concludes the proof. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>THEOREM</strong> <strong>(Mean Value)</strong> <span class="math notranslate nohighlight">\(\idx{mean value theorem}\xdi\)</span> Let <span class="math notranslate nohighlight">\(f : [a,b] \to \mathbb{R}\)</span> be a continuous function and assume that its derivative exists on <span class="math notranslate nohighlight">\((a,b)\)</span>. Then there is <span class="math notranslate nohighlight">\(a &lt; c &lt; b\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
f(b) = f(a) + (b-a)f'(c),
\]</div>
<p>or put differently</p>
<div class="math notranslate nohighlight">
\[
\frac{f(b) - f(a)}{b-a} = f'(c).
\]</div>
<p><span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p><strong>Figure:</strong> Illustration of the Mean Value Theorem (with help from ChatGPT; inspired by (<a class="reference external" href="https://commons.wikimedia.org/wiki/File:Mvt2.svg">Source</a>))</p>
<p><img alt="Illustration of the Mean Value Theorem (with help from ChatGPT; inspired by (Source))" src="../../_images/mean-value.png" /></p>
<p><span class="math notranslate nohighlight">\(\bowtie\)</span></p>
<p><em>Proof idea:</em> Apply <em>Rolle’s Theorem</em> to</p>
<div class="math notranslate nohighlight">
\[
\phi(x) = f(x) - \left[f(a) + \frac{f(b) - f(a)}{b - a} (x-a)\right].
\]</div>
<p><em>Proof:</em> Let <span class="math notranslate nohighlight">\(\phi(x) = f(x) - f(a) - \frac{f(b) - f(a)}{b - a} (x-a)\)</span>. Note that <span class="math notranslate nohighlight">\(\phi(a) = \phi(b) = 0\)</span> and <span class="math notranslate nohighlight">\(\phi'(x) = f'(x) - \frac{f(b) - f(a)}{b - a}\)</span> for all <span class="math notranslate nohighlight">\(x \in (a, b)\)</span>. Thus, by <em>Rolle’s Theorem</em>, there is <span class="math notranslate nohighlight">\(c \in (a, b)\)</span> such that <span class="math notranslate nohighlight">\(\phi'(c) = 0\)</span>. That implies <span class="math notranslate nohighlight">\(\frac{f(b) - f(a)}{b - a} = \phi'(c)\)</span> and plugging into <span class="math notranslate nohighlight">\(\phi(b)\)</span> gives the result. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>We will also use <em>Taylor’s Theorem</em>, a generalization of the <em>Mean Value Theorem</em> that provides a polynomial approximation to a function around a point. We will restrict ourselves to the case of a linear approximation with second-order error term, which will suffice for our purposes.</p>
<p><strong>THEOREM</strong> <strong>(Taylor)</strong> <span class="math notranslate nohighlight">\(\idx{Taylor's theorem}\xdi\)</span> Let <span class="math notranslate nohighlight">\(f: D \to \mathbb{R}\)</span> where <span class="math notranslate nohighlight">\(D \subseteq \mathbb{R}\)</span>. Suppose <span class="math notranslate nohighlight">\(f\)</span> has a continuous derivative on <span class="math notranslate nohighlight">\([a,b]\)</span> and that its second derivative exists on <span class="math notranslate nohighlight">\((a,b)\)</span>. Then for any <span class="math notranslate nohighlight">\(x \in [a, b]\)</span></p>
<div class="math notranslate nohighlight">
\[
f(x)
= f(a) + (x-a) f'(a) + \frac{1}{2} (x-a)^2 f''(\xi)
\]</div>
<p>for some <span class="math notranslate nohighlight">\(a &lt; \xi &lt; x\)</span>. <span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p><em>Proof idea:</em> The <em>Mean Value Theorem</em> implies that there is <span class="math notranslate nohighlight">\(a &lt; \xi&lt; x\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
f(x) = f(a) + (x - a)f'(\xi).
\]</div>
<p>One way to think of the proof of that result is the following: we constructed an affine function that agrees with <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(x\)</span>, then used <em>Rolle’s Theorem</em> to express the coefficient of the linear term using <span class="math notranslate nohighlight">\(f'\)</span>. Here we do the same with a polynomial of degree <span class="math notranslate nohighlight">\(2\)</span>. But we now have an extra degree of freedom in choosing this polynomial. Because we are looking for a good approximation close to <span class="math notranslate nohighlight">\(a\)</span>, we choose to make the first derivative at <span class="math notranslate nohighlight">\(a\)</span> also agree. Applying <em>Rolle’s Theorem</em> twice gives the claim.</p>
<p><em>Proof:</em> Let</p>
<div class="math notranslate nohighlight">
\[
P(t) = \alpha_0 + \alpha_1 (t-a) + \alpha_2 (t-a)^2.
\]</div>
<p>We choose the <span class="math notranslate nohighlight">\(\alpha_i\)</span>’s so that <span class="math notranslate nohighlight">\(P(a) = f(a)\)</span>, <span class="math notranslate nohighlight">\(P'(a) = f'(a)\)</span>, and <span class="math notranslate nohighlight">\(P(x) = f(x)\)</span>. The first two lead to the conditions</p>
<div class="math notranslate nohighlight">
\[
\alpha_0 = f(a), \quad \alpha_1 = f'(a). 
\]</div>
<p>Let <span class="math notranslate nohighlight">\(\phi(t) = f(t) - P(t)\)</span>. By construction <span class="math notranslate nohighlight">\(\phi(a) = \phi(x) = 0\)</span>. By <em>Rolle’s Theorem</em>, there is  a <span class="math notranslate nohighlight">\(\xi' \in (a, x)\)</span> such that <span class="math notranslate nohighlight">\(\phi'(\xi') = 0\)</span>. Moreover, <span class="math notranslate nohighlight">\(\phi'(a) = 0\)</span>. Hence we can apply <em>Rolle’s Theorem</em> again – this time to <span class="math notranslate nohighlight">\(\phi'\)</span> on <span class="math notranslate nohighlight">\([a, \xi']\)</span>. It implies that there is <span class="math notranslate nohighlight">\(\xi \in (a, \xi')\)</span> such that <span class="math notranslate nohighlight">\(\phi''(\xi) = 0\)</span>.</p>
<p>The second derivative of <span class="math notranslate nohighlight">\(\phi\)</span> at <span class="math notranslate nohighlight">\(\xi\)</span> is</p>
<div class="math notranslate nohighlight">
\[
0 = \phi''(\xi) 
= f''(\xi) - P''(\xi) 
= f''(\xi) - 2 \alpha_2
\]</div>
<p>so <span class="math notranslate nohighlight">\(\alpha_2 = f''(\xi)/2\)</span>. Plugging into <span class="math notranslate nohighlight">\(P\)</span> and using <span class="math notranslate nohighlight">\(\phi(x) = 0\)</span> gives the claim. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>Optimization</strong> As we mentioned before, optimization problems play a ubiquitous role in data science. Here we look at unconstrained optimization problems, that is, problems of the form:</p>
<div class="math notranslate nohighlight">
\[
\min_{\mathbf{x} \in \mathbb{R}^d} f(\mathbf{x})
\]</div>
<p>where <span class="math notranslate nohighlight">\(f : \mathbb{R}^d \to \mathbb{R}\)</span>.</p>
<p>Ideally, we would like to find a global minimizer to the optimization problem above.</p>
<p><strong>DEFINITION</strong> <strong>(Global Minimizer)</strong> <span class="math notranslate nohighlight">\(\idx{global minimizer or maximizer}\xdi\)</span> Let <span class="math notranslate nohighlight">\(f : \mathbb{R}^d \to \mathbb{R}\)</span>. The point <span class="math notranslate nohighlight">\(\mathbf{x}^* \in \mathbb{R}^d\)</span> is a global minimizer of <span class="math notranslate nohighlight">\(f\)</span> over <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span> if</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{x}) 
\geq f(\mathbf{x}^*), \quad \forall \mathbf{x} \in \mathbb{R}^d. 
\]</div>
<p><span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>Global maximizers are defined similarly.</p>
<p><strong>NUMERICAL CORNER:</strong> The function <span class="math notranslate nohighlight">\(f(x) = x^2\)</span> over <span class="math notranslate nohighlight">\(\mathbb{R}\)</span> has a global minimizer at <span class="math notranslate nohighlight">\(x^* = 0\)</span>. Indeed, we clearly have <span class="math notranslate nohighlight">\(f(x) \geq 0\)</span> for all <span class="math notranslate nohighlight">\(x\)</span> while <span class="math notranslate nohighlight">\(f(0) = 0\)</span>. To plot the function, we use the Matplotlib package, and specifically its function <a class="reference external" href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html"><code class="docutils literal notranslate"><span class="pre">matplotlib.pyplot.plot</span></code></a>. We also use the function <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.linspace.html"><code class="docutils literal notranslate"><span class="pre">numpy.linspace</span></code></a> to create an array of evenly spaced numbers where we evaluate <span class="math notranslate nohighlight">\(f\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.25</span><span class="p">,</span><span class="mf">4.25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/43fc4c0565df93407d1041b2d921cda79e545b3fd041aa7eb0495f3dcc3617d8.png" src="../../_images/43fc4c0565df93407d1041b2d921cda79e545b3fd041aa7eb0495f3dcc3617d8.png" />
</div>
</div>
<p>The function <span class="math notranslate nohighlight">\(f(x) = e^x\)</span> over <span class="math notranslate nohighlight">\(\mathbb{R}\)</span> does not have a global minimizer. Indeed, <span class="math notranslate nohighlight">\(f(x) &gt; 0\)</span> but no <span class="math notranslate nohighlight">\(x\)</span> achieves <span class="math notranslate nohighlight">\(0\)</span>. And, for any <span class="math notranslate nohighlight">\(m &gt; 0\)</span>, there is <span class="math notranslate nohighlight">\(x\)</span> small enough such that <span class="math notranslate nohighlight">\(f(x) &lt; m\)</span>. Note that <span class="math notranslate nohighlight">\(\mathbb{R}\)</span> is <em>not</em> bounded, therefore the <em>Extreme Value Theorem</em> does not apply here.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.25</span><span class="p">,</span><span class="mf">4.25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/e0cb1df0efdec1b50742a67e8d77d2ad94c795b81369e21cc7536319451d42d9.png" src="../../_images/e0cb1df0efdec1b50742a67e8d77d2ad94c795b81369e21cc7536319451d42d9.png" />
</div>
</div>
<p>The function <span class="math notranslate nohighlight">\(f(x) = (x+1)^2 (x-1)^2\)</span> over <span class="math notranslate nohighlight">\(\mathbb{R}\)</span> has two global minimizers at <span class="math notranslate nohighlight">\(x^* = -1\)</span> and <span class="math notranslate nohighlight">\(x^{**} = 1\)</span>. Indeed, <span class="math notranslate nohighlight">\(f(x) \geq 0\)</span> and <span class="math notranslate nohighlight">\(f(x) = 0\)</span> if and only <span class="math notranslate nohighlight">\(x = x^*\)</span> or <span class="math notranslate nohighlight">\(x = x^{**}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">((</span><span class="n">x</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="p">((</span><span class="n">x</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.25</span><span class="p">,</span><span class="mf">4.25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/e4765d88924019d9534f3f5a12007febc468cb88ef9d60cabba976d38478100f.png" src="../../_images/e4765d88924019d9534f3f5a12007febc468cb88ef9d60cabba976d38478100f.png" />
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p>In general, finding a global minimizer and certifying that one has been found can be difficult unless some special structure is present. Therefore weaker notions of solution have been introduced.</p>
<p><strong>DEFINITION</strong> <strong>(Local Minimizer)</strong> <span class="math notranslate nohighlight">\(\idx{local minimizer or maximizer}\xdi\)</span> Let <span class="math notranslate nohighlight">\(f : \mathbb{R}^d \to \mathbb{R}\)</span>. The point <span class="math notranslate nohighlight">\(\mathbf{x}^* \in \mathbb{R}^d\)</span> is a local minimizer of <span class="math notranslate nohighlight">\(f\)</span> over <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span> if there is <span class="math notranslate nohighlight">\(\delta &gt; 0\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{x}) 
\geq f(\mathbf{x}^*), \quad \forall \mathbf{x} \in B_{\delta}(\mathbf{x}^*) \setminus \{\mathbf{x}^*\}. 
\]</div>
<p>If the inequality is strict, we say that <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span> is a strict local minimizer. <span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>In words, <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span> is a local minimizer if there is open ball around <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span> where it attains the minimum value. Local maximizers are defined similarly. In the final example above, <span class="math notranslate nohighlight">\(x = 0\)</span> is a local maximizer.</p>
<p><strong>Figure:</strong> Local and global optima. (<a class="reference external" href="https://commons.wikimedia.org/wiki/File:Extrema_example_original.svg">Source</a>)</p>
<p><img alt="Local and global optima" src="https://upload.wikimedia.org/wikipedia/commons/thumb/6/68/Extrema_example_original.svg/600px-Extrema_example_original.svg.png" /></p>
<p><span class="math notranslate nohighlight">\(\bowtie\)</span></p>
<p>Local minimizers can be characterized in terms of derivatives.</p>
<p><strong>THEOREM</strong> <strong>(First-Order Necessary Optimality Condition)</strong> <span class="math notranslate nohighlight">\(\idx{first-order necessary optimality condition}\xdi\)</span> Let <span class="math notranslate nohighlight">\(f : \mathbb{R} \to \mathbb{R}\)</span> be differentiable (i.e., its derivative exists) on <span class="math notranslate nohighlight">\(\mathbb{R}\)</span>. If <span class="math notranslate nohighlight">\(x_0\)</span> is a local minimizer, then <span class="math notranslate nohighlight">\(f'(x_0) = 0\)</span>. <span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p><em>Proof:</em> We argue by contradiction. Suppose that <span class="math notranslate nohighlight">\(f'(x_0) \neq 0\)</span>. Say <span class="math notranslate nohighlight">\(f'(x_0) &gt; 0\)</span> (the other case being similar). By the <em>Descent Direction Lemma</em>, there is a <span class="math notranslate nohighlight">\(\delta &gt; 0\)</span> such that, for each <span class="math notranslate nohighlight">\(x\)</span> in <span class="math notranslate nohighlight">\(B_\delta(x_0)\)</span>, <span class="math notranslate nohighlight">\(f(x) &lt; f(x_0)\)</span> if <span class="math notranslate nohighlight">\(x &lt; x_0\)</span>. So every open ball around <span class="math notranslate nohighlight">\(x_0\)</span> has a point achieving a smaller value than <span class="math notranslate nohighlight">\(f(x_0)\)</span>. Thus <span class="math notranslate nohighlight">\(x_0\)</span> is not a local minimizer, a contradiction. So it must be that <span class="math notranslate nohighlight">\(f'(x_0) = 0\)</span>. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>Functions of several variables</strong> The previous condition generalizes naturally to functions of several variables. The derivative is replaced by the gradient.</p>
<p><strong>DEFINITION</strong> <strong>(Partial Derivative)</strong> <span class="math notranslate nohighlight">\(\idx{partial derivative}\xdi\)</span> Let <span class="math notranslate nohighlight">\(f : D \to \mathbb{R}\)</span> where <span class="math notranslate nohighlight">\(D \subseteq \mathbb{R}^d\)</span> and let <span class="math notranslate nohighlight">\(\mathbf{x}_0 = (x_{0,1},\ldots,x_{0,d}) \in D\)</span> be an interior point of <span class="math notranslate nohighlight">\(D\)</span>. The partial derivative of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> with respect to <span class="math notranslate nohighlight">\(x_i\)</span> is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{\partial f (\mathbf{x}_0)}{\partial x_i} 
&amp;= \lim_{h \to 0} \frac{f(\mathbf{x}_0 + h \mathbf{e}_i) - f(\mathbf{x}_0)}{h}\\
&amp;= \lim_{h \to 0} \frac{f(x_{0,1},\ldots,x_{0,i-1},x_{0,i} + h,x_{0,i+1},\ldots,x_{0,d}) 
- f(x_{0,1},\ldots,x_{0,d})}{h}
\end{align*}\]</div>
<p>provided the limit exists. If <span class="math notranslate nohighlight">\(\frac{\partial f (\mathbf{x}_0)}{\partial x_i}\)</span> exists and is continuous in an open ball around <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> for all <span class="math notranslate nohighlight">\(i\)</span>, then we say that <span class="math notranslate nohighlight">\(f\)</span> continuously differentiable at <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>. <span class="math notranslate nohighlight">\(\natural\)</span></p>
<p><strong>DEFINITION</strong> <strong>(Gradient)</strong> <span class="math notranslate nohighlight">\(\idx{gradient}\xdi\)</span> Let <span class="math notranslate nohighlight">\(f : D \to \mathbb{R}\)</span> where <span class="math notranslate nohighlight">\(D \subseteq \mathbb{R}^d\)</span> and let <span class="math notranslate nohighlight">\(\mathbf{x}_0 \in D\)</span> be an interior point of <span class="math notranslate nohighlight">\(D\)</span>. Assume <span class="math notranslate nohighlight">\(f\)</span> is continuously differentiable at <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>. The (column) vector</p>
<div class="math notranslate nohighlight">
\[
\nabla f(\mathbf{x}_0) 
= \left(\frac{\partial f (\mathbf{x}_0)}{\partial x_1}, \ldots, \frac{\partial f (\mathbf{x}_0)}{\partial x_d}\right)
\]</div>
<p>is called the gradient of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>. <span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>Note that the gradient is itself a function of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. In fact, unlike <span class="math notranslate nohighlight">\(f\)</span>, it is a vector-valued function.</p>
<p>We generalize the <em>Descent Direction Lemma</em> to the multivariable case. We first need to define what a descent direction is.</p>
<p><strong>DEFINITION</strong> <strong>(Descent Direction)</strong> <span class="math notranslate nohighlight">\(\idx{descent direction}\xdi\)</span> Let <span class="math notranslate nohighlight">\(f : \mathbb{R}^d \to \mathbb{R}\)</span>. A vector <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> is a descent direction for <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> if there is <span class="math notranslate nohighlight">\(\alpha^* &gt; 0\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{x}_0 + \alpha \mathbf{v})
&lt; f(\mathbf{x}_0), \quad \forall \alpha \in (0,\alpha^*).
\]</div>
<p><span class="math notranslate nohighlight">\(\natural\)</span></p>
<p><strong>LEMMA</strong> <strong>(Descent Direction)</strong> <span class="math notranslate nohighlight">\(\idx{descent direction lemma}\xdi\)</span> Let <span class="math notranslate nohighlight">\(f : \mathbb{R}^d \to \mathbb{R}\)</span> be continuously differentiable at <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> and assume that <span class="math notranslate nohighlight">\(\nabla f(\mathbf{x}_0) \neq 0\)</span>. Then <span class="math notranslate nohighlight">\(f\)</span> has a descent direction at <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><strong>THEOREM</strong> <strong>(First-Order Necessary Optimality Condition)</strong> <span class="math notranslate nohighlight">\(\idx{first-order necessary optimality condition}\xdi\)</span> Let <span class="math notranslate nohighlight">\(f : \mathbb{R}^d \to \mathbb{R}\)</span> be continuously differentiable on <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span>. If <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> is a local minimizer, then <span class="math notranslate nohighlight">\(\nabla f(\mathbf{x}_0) = \mathbf{0}\)</span>. <span class="math notranslate nohighlight">\(\sharp\)</span></p>
</section>
<section id="probability">
<h2><span class="section-number">1.2.3. </span>Probability<a class="headerlink" href="#probability" title="Link to this heading">#</a></h2>
<p>Finally, we review a few key definitions and results from probability theory.</p>
<p><strong>Expectation, variance and Chebyshev’s inequality</strong> Recall that the <a class="reference external" href="https://en.wikipedia.org/wiki/Expected_value">expectation</a> (or mean) of a function <span class="math notranslate nohighlight">\(h\)</span> of a discrete random variable <span class="math notranslate nohighlight">\(X\)</span> taking values in <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[h(X)]
= \sum_{x \in \mathcal{X}} h(x)\,p_X(x)
\]</div>
<p>where <span class="math notranslate nohighlight">\(p_X(x) = \mathbb{P}[X = x]\)</span> is the <a class="reference external" href="https://en.wikipedia.org/wiki/Probability_mass_function">probability mass function</a> (PMF)<span class="math notranslate nohighlight">\(\idx{probability mass function}\xdi\)</span> of <span class="math notranslate nohighlight">\(X\)</span>.
In the continuous case, we have</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[h(X)]
= \int h(x) f_X(x)\,\mathrm{d}x
\]</div>
<p>if <span class="math notranslate nohighlight">\(f_X\)</span> is the <a class="reference external" href="https://en.wikipedia.org/wiki/Probability_density_function">probability density function</a> (PDF)<span class="math notranslate nohighlight">\(\idx{probability density function}\xdi\)</span> of <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>These definitions extend to functions of multiple variables by using instead the joint PMF or PDF.</p>
<p>We sometimes denote the expectation of <span class="math notranslate nohighlight">\(X\)</span> by <span class="math notranslate nohighlight">\(\mu_X\)</span>.</p>
<p>Two key properties of the expectation are:</p>
<ul class="simple">
<li><p><em>linearity</em>, that is,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[\alpha_1 h_1(X) + \alpha_2 h_2(Y) + \beta] = \alpha_1 \,\mathbb{E}[h_1(X)] + \alpha_2 \,\mathbb{E}[h_2(Y)] + \beta
\]</div>
<ul class="simple">
<li><p><em>monotonicity</em>, that is, if <span class="math notranslate nohighlight">\(h_1(x) \leq h_2(x)\)</span> for all <span class="math notranslate nohighlight">\(x\)</span> then</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[h_1(X)] \leq \mathbb{E}[h_2(X)].
\]</div>
<p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Variance">variance</a><span class="math notranslate nohighlight">\(\idx{variance}\xdi\)</span> of a real-valued random variable <span class="math notranslate nohighlight">\(X\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\mathrm{Var}[X] = \mathbb{E}[(X - \mathbb{E}[X])^2]
\]</div>
<p>and its standard deviation is <span class="math notranslate nohighlight">\(\sigma_X = \sqrt{\mathrm{Var}[X]}\)</span>. The variance does not satisfy linearity, but we have the following property</p>
<div class="math notranslate nohighlight">
\[
\mathrm{Var}[\alpha X + \beta] = \alpha^2 \,\mathrm{Var}[X].
\]</div>
<p>The standard deviation is a measure of the typical deviation of <span class="math notranslate nohighlight">\(X\)</span> around its mean, that is, of the spread of the distribution.</p>
<p>A quantitative version of this statement is given by <em>Chebyshev’s Inequality</em>.</p>
<p><strong>THEOREM</strong> <strong>(Chebyshev)</strong> <span class="math notranslate nohighlight">\(\idx{Chebyshev's inequality}\xdi\)</span> For a random variable <span class="math notranslate nohighlight">\(X\)</span> with finite variance, we have for any <span class="math notranslate nohighlight">\(\alpha &gt; 0\)</span></p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}[|X - \mathbb{E}[X]| \geq \alpha] 
\leq \frac{\mathrm{Var}[X]}{\alpha^2}.
\]</div>
<p><span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p>The intuition is the following: if the expected squared deviation from the mean is small, then the absolute deviation from the mean is unlikely to be large.</p>
<p>To formalize this we prove a more general inequality, <em>Makov’s Inequality</em>. In words, if a non-negative random variable has a small expectation then it is unlikely to be large.</p>
<p><strong>LEMMA</strong> <strong>(Markov)</strong> <span class="math notranslate nohighlight">\(\idx{Markov's inequality}\xdi\)</span> Let <span class="math notranslate nohighlight">\(Z\)</span> be a non-negative random variable with finite expectation. Then, for any <span class="math notranslate nohighlight">\(\beta &gt; 0\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}[Z \geq \beta] \leq \frac{\mathbb{E}[Z]}{\beta}.
\]</div>
<p><span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof idea:</em> The quantity <span class="math notranslate nohighlight">\(\beta \,\mathbb{P}[Z \geq \beta]\)</span> is a lower bound on the expectation of <span class="math notranslate nohighlight">\(Z\)</span> restricted to the range <span class="math notranslate nohighlight">\(\{Z\geq \beta\}\)</span>, which by non-negativity is itself lower bounded by <span class="math notranslate nohighlight">\(\mathbb{E}[Z]\)</span>.</p>
<p><em>Proof:</em> Formally, let <span class="math notranslate nohighlight">\(\mathbf{1}_A\)</span> be the indicator of the event <span class="math notranslate nohighlight">\(A\)</span>, that is, it is the random variable that is <span class="math notranslate nohighlight">\(1\)</span> when <span class="math notranslate nohighlight">\(A\)</span> occurs and <span class="math notranslate nohighlight">\(0\)</span> otherwise. By definition, the expectation of <span class="math notranslate nohighlight">\(\mathbf{1}_A\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[A] = 0\,\mathbb{P}[\mathbf{1}_A = 0] + 1\,\mathbb{P}[\mathbf{1}_A = 1] = \mathbb{P}[A]
\]</div>
<p>where <span class="math notranslate nohighlight">\(A^c\)</span> is the complement of <span class="math notranslate nohighlight">\(A\)</span>. Hence, by linearity and monotonicity,</p>
<div class="math notranslate nohighlight">
\[
\beta \,\mathbb{P}[Z \geq \beta] 
= \beta \,\mathbb{E}[\mathbf{1}_{Z \geq \beta}]
= \mathbb{E}[\beta \mathbf{1}_{Z \geq \beta}]
\leq \mathbb{E}[Z].
\]</div>
<p>Rearranging gives the claim. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>Finally we return to the proof of <em>Chebyshev’s Inequality</em>.</p>
<p><em>Proof idea (Chebyshev):</em> Simply apply <em>Markov’s Inequality</em> to the squared deviation of <span class="math notranslate nohighlight">\(X\)</span> from its mean.</p>
<p><em>Proof:</em> <em>(Chebyshev)</em> <span class="math notranslate nohighlight">\(\idx{Chebyshev's inequality}\xdi\)</span> Let <span class="math notranslate nohighlight">\(Z = (X - \mathbb{E}[X])^2\)</span>, which is non-negative by definition. Hence, by <em>Markov’s Inequality</em>, for any <span class="math notranslate nohighlight">\(\beta = \alpha^2 &gt; 0\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb{P}[|X - \mathbb{E}[X]| \geq \alpha]
&amp;= \mathbb{P}[(X - \mathbb{E}[X])^2 \geq \alpha^2]\\
&amp;= \mathbb{P}[Z \geq \beta]\\
&amp;\leq \frac{\mathbb{E}[Z]}{\beta}\\
&amp;= \frac{\mathrm{Var}[X]}{\alpha^2}
\end{align*}\]</div>
<p>where we used the definition of the variance in the last equality. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>A few important remarks about <em>Chebyshev’s Inequality</em>:</p>
<p>(1) We sometimes need a one-sided bound of the form</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}[X - \mathbb{E}[X] \geq \alpha].
\]</div>
<p>Note the absence of absolute values compared to the two-sided form appearing in <em>Chebyshev’s Inequality</em>. In this case, we can use the fact that the event <span class="math notranslate nohighlight">\(\{X - \mathbb{E}[X] \geq \alpha\}\)</span> implies a fortiori that
<span class="math notranslate nohighlight">\(\{|X - \mathbb{E}[X]| \geq \alpha\}\)</span>, so that the probability of the former is smaller than that of the latter by monotonicity, namely,</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}[X - \mathbb{E}[X] \geq \alpha]
\leq \mathbb{P}[|X - \mathbb{E}[X]| \geq \alpha].
\]</div>
<p>We can then use <em>Chebyshev’s Inequality</em> on the right-hand side to obtain</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}[X - \mathbb{E}[X] \geq \alpha]
\leq \frac{\mathrm{Var}[X]}{\alpha^2}.
\]</div>
<p>Similarly, for the same reasons, we also have</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}[X - \mathbb{E}[X] \leq - \alpha]
\leq \frac{\mathrm{Var}[X]}{\alpha^2}.
\]</div>
<p>(2) In terms of the standard deviation <span class="math notranslate nohighlight">\(\sigma_X = \sqrt{\mathrm{Var}[X]}\)</span> of <span class="math notranslate nohighlight">\(X\)</span>, the inequality can be re-written as</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}[|X - \mathbb{E}[X]| \geq \alpha] 
\leq \frac{\mathrm{Var}[X]}{\alpha^2}
= \left(\frac{\sigma_X}{\alpha}\right)^2.
\]</div>
<p>So to get a small bound on the right-hand side, one needs the deviation from the mean <span class="math notranslate nohighlight">\(\alpha\)</span> to be significantly larger than the standard deviation. In words, a random variable is unlikely to be away from its mean by much more than its standard deviation. This observation is consistent with the interpretation of the standard deviation as the typical spread of a random variable.</p>
<p><em>Chebyshev’s Inequality</em> is particularly useful when combined with independence.</p>
<p><strong>Independence and limit theorems</strong> <span class="math notranslate nohighlight">\(\idx{independence}\xdi\)</span> Recall that discrete random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent if their joint PMF factorizes, that is</p>
<div class="math notranslate nohighlight">
\[
p_{X,Y}(x,y) = p_X(x) \,p_Y(y), \qquad \forall x, y
\]</div>
<p>where <span class="math notranslate nohighlight">\(p_{X,Y}(x,y) = \mathbb{P}[X=x, Y=y]\)</span>. Similarly, continuous random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent if their joint PDF factorizes. One consequence is that expectations of products of single-variable functions factorize as well, that is, for functions <span class="math notranslate nohighlight">\(g\)</span> and <span class="math notranslate nohighlight">\(h\)</span> we have</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[g(X) h(Y)]
= \mathbb{E}[g(X)] \,\mathbb{E}[h(Y)],
\]</div>
<p>provided the expectations exist.</p>
<p>An important way to quantify the lack of independence of two random variables is the covariance.</p>
<p><strong>DEFINITION</strong> <strong>(Covariance)</strong> <span class="math notranslate nohighlight">\(\idx{covariance}\xdi\)</span> The covariance of random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> with finite means and variances is defined as</p>
<div class="math notranslate nohighlight">
\[
\mathrm{Cov}[X,Y]
= \mathbb{E}\left[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])\right].
\]</div>
<p><span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>Note that, by definition, the covariance is symmetric: <span class="math notranslate nohighlight">\(\mathrm{Cov}[X,Y] = \mathrm{Cov}[Y,X]\)</span>.</p>
<p>When <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent, their covariance is <span class="math notranslate nohighlight">\(0\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathrm{Cov}[X,Y]
&amp;= \mathbb{E}\left[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])\right]\\
&amp;= \mathbb{E}\left[X - \mathbb{E}[X]\right]\mathbb{E}\left[Y - \mathbb{E}[Y]\right]\\
&amp;= \left(\mathbb{E}[X] - \mathbb{E}[X]\right)\left(\mathbb{E}[Y] - \mathbb{E}[Y]\right)\\
&amp;= 0,
\end{align*}\]</div>
<p>where we used independence on the second line and the linearity of expectations on the third one.</p>
<p>A related quantity of interest in data science is the correlation coefficient which is obtained by dividing the covariance by the product of the standard deviations</p>
<div class="math notranslate nohighlight">
\[
\rho_{X, Y}
= \frac{\mathrm{Cov}[X,Y]}{\sigma_X \sigma_Y}.
\]</div>
<p>By the <em>Cauchy-Schwarz Inequality</em>, it lies in <span class="math notranslate nohighlight">\([-1,1]\)</span> (prove it!).</p>
<p>The covariance leads to a useful identity for the variance of a sum of random variables.</p>
<p><strong>LEMMA</strong> <strong>(Variance of a Sum)</strong> <span class="math notranslate nohighlight">\(\idx{variance of a sum}\xdi\)</span> Let <span class="math notranslate nohighlight">\(X_1,\ldots,X_n\)</span> be random variables with finite means and variances. Then we have</p>
<div class="math notranslate nohighlight">
\[
\mathrm{Var}[X_1 + \cdots + X_n]
= \sum_{i=1} \mathrm{Var}[X_i] + 2 \sum_{i &lt; j} \mathrm{Cov}[X_i, X_j].
\]</div>
<p><span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> By definition of the variance and linearity of expectations,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\mathrm{Var}[X_1 + \cdots + X_n]\\
&amp;= \mathbb{E}\left[(X_1 + \cdots + X_n - \mathbb{E}[X_1 + \cdots + X_n])^2\right]\\
&amp;= \mathbb{E}\left[(X_1 + \cdots + X_n - \mathbb{E}[X_1] - \cdots - \mathbb{E}[X_n])^2\right]\\
&amp;= \mathbb{E}\left[(X_1 - \mathbb{E}[X_1]) + \cdots + (X_n  - \mathbb{E}[X_n]))^2\right]\\
&amp;= \sum_{i=1}^n \mathbb{E}\left[(X_i - \mathbb{E}[X_i])^2\right] 
+ \sum_{i \neq j} \mathbb{E}\left[(X_i - \mathbb{E}[X_i]) (X_j - \mathbb{E}[X_j])\right].
\end{align*}\]</div>
<p>The claim follows from the definition of the variance and covariance, and the symmetry of the covariance. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>The previous lemma has the following important implication. If <span class="math notranslate nohighlight">\(X_1, \ldots, X_n\)</span> are pairwise independent, real-valued random variables, then</p>
<div class="math notranslate nohighlight">
\[
\mathrm{Var}[X_1 + \cdots + X_n]
= \mathrm{Var}[X_1] + \cdots + \mathrm{Var}[X_n].
\]</div>
<p>Notice that, unlike the case of the expectation, this linearity property for the variance requires independence.</p>
<p>Applied to the sample mean of <span class="math notranslate nohighlight">\(n\)</span> independent, identically distributed (i.i.d.) random variables <span class="math notranslate nohighlight">\(X_1,\ldots,X_n\)</span>, we obtain</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathrm{Var}
\left[\frac{1}{n} \sum_{i=1}^n X_i\right]
&amp;= \frac{1}{n^2} \sum_{i=1}^n \mathrm{Var}[X_i]\\
&amp;= \frac{1}{n^2} n \,\mathrm{Var}[X_1]\\
&amp;= \frac{\mathrm{Var}[X_1]}{n}.
\end{align*}\]</div>
<p>So the variance of the sample mean decreases as <span class="math notranslate nohighlight">\(n\)</span> gets large, while its expectation remains the same by linearity</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb{E}
\left[\frac{1}{n} \sum_{i=1}^n X_i\right]
&amp;= \frac{1}{n} \sum_{i=1}^n \mathbb{E}[X_i]\\
&amp;= \frac{1}{n} n \,\mathbb{E}[X_1]\\
&amp;= \mathbb{E}[X_1].
\end{align*}\]</div>
<p>Together with <em>Chebyshev’s Inequality</em>, we immediately get that the sample mean approaches its expectation in the following probabilistic sense.</p>
<p><strong>THEOREM</strong> <strong>(Weak Law of Large Numbers)</strong> <span class="math notranslate nohighlight">\(\idx{weak law of large numbers}\xdi\)</span> Let <span class="math notranslate nohighlight">\(X_1, \ldots, X_n\)</span> be i.i.d. For any <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span>, as <span class="math notranslate nohighlight">\(n \to +\infty\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}\left[\left|\frac{1}{n} \sum_{i=1}^n X_i 
- \mathbb{E}[X_1]\right| \geq \varepsilon\right]
\to 0.
\]</div>
<p><span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p><em>Proof:</em> By <em>Chebyshev’s Inequality</em> and the formulas above,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb{P}\left[\left|\frac{1}{n} \sum_{i=1}^n X_i 
- \mathbb{E}[X_1]\right| \geq \varepsilon\right]
&amp;= \mathbb{P}\left[\left|\frac{1}{n} \sum_{i=1}^n X_i 
- \mathbb{E} \left[\frac{1}{n} \sum_{i=1}^n X_i\right]\right| \geq \varepsilon\right]\\
&amp;\leq \frac{\mathrm{Var}\left[\frac{1}{n} \sum_{i=1}^n X_i\right]}{\varepsilon^2}\\
&amp;= \frac{\mathrm{Var}[X_1]}{n \varepsilon^2}\\
&amp;\to 0
\end{align*}\]</div>
<p>as <span class="math notranslate nohighlight">\(n \to +\infty\)</span>. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>NUMERICAL CORNER:</strong> We can use simulations to confirm the <em>Weak Law of Large Numbers</em>. Recall that a uniform random variable over the interval <span class="math notranslate nohighlight">\([a,b]\)</span> has density</p>
<div class="math notranslate nohighlight">
\[\begin{split}
f_{X}(x)
= \begin{cases}
\frac{1}{b-a} &amp; x \in [a,b] \\
0 &amp; \text{o.w.}
\end{cases}
\end{split}\]</div>
<p>We write <span class="math notranslate nohighlight">\(X \sim \mathrm{U}[a,b]\)</span>. We can obtain a sample from <span class="math notranslate nohighlight">\(\mathrm{U}[0,1]\)</span> by using the function
<a class="reference external" href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.uniform.html"><code class="docutils literal notranslate"><span class="pre">numpy.random.Generator.uniform</span></code></a>. We must first instantiate a random number generator (RNG) with <a class="reference external" href="https://numpy.org/doc/stable/reference/random/generator.html"><code class="docutils literal notranslate"><span class="pre">numpy.random.default_rng</span></code></a> in Numpy. We provide a <a class="reference external" href="https://numpy.org/doc/stable/reference/random/bit_generators/index.html#seeding-and-entropy">seed</a> as an initial state for the RNG. Using the same seed again ensures reproducibility.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">seed</span> <span class="o">=</span> <span class="mi">535</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.9836159914889122
</pre></div>
</div>
</div>
</div>
<p>Now we take <span class="math notranslate nohighlight">\(n\)</span> samples from <span class="math notranslate nohighlight">\(\mathrm{U}[0,1]\)</span> and compute their sample mean. We repeat <span class="math notranslate nohighlight">\(k\)</span> times and display the empirical distribution of the sample means using an <a class="reference external" href="https://en.wikipedia.org/wiki/Histogram">histogram</a>. We start with <span class="math notranslate nohighlight">\(n=10\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1000</span>
<span class="n">sample_mean</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">)]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">sample_mean</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;lightblue&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/368a213cd28a694fbc731bfe0632303fa7c078a1189c6a5429594f7d9ab657e0.png" src="../../_images/368a213cd28a694fbc731bfe0632303fa7c078a1189c6a5429594f7d9ab657e0.png" />
</div>
</div>
<p>Taking <span class="math notranslate nohighlight">\(n\)</span> much larger leads to more concentration around the mean.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span>
<span class="n">sample_mean</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">)]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">sample_mean</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;lightblue&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/9b5ed2c3b51d02468f577deb693c038bbf5606606407cb167c6ff5813c100685.png" src="../../_images/9b5ed2c3b51d02468f577deb693c038bbf5606606407cb167c6ff5813c100685.png" />
</div>
</div>
<p><strong>TRY IT!</strong> Recall that the cumulative distribution function (CDF)<span class="math notranslate nohighlight">\(\idx{cumulative distribution function}\xdi\)</span> of a random variable <span class="math notranslate nohighlight">\(X\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[
F_X(z) = \mathbb{P}[X \leq z], \qquad \forall z \in \mathbb{R}.
\]</div>
<p>a) Let <span class="math notranslate nohighlight">\(\mathcal{Z}\)</span> be the interval where <span class="math notranslate nohighlight">\(F_X(z) \in (0,1)\)</span> and assume that <span class="math notranslate nohighlight">\(F_X\)</span> is strictly increasing on <span class="math notranslate nohighlight">\(\mathcal{Z}\)</span>. Let <span class="math notranslate nohighlight">\(U \sim \mathrm{U}[0,1]\)</span>. Show that</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}[F_X^{-1}(U) \leq z] = F_X(z).
\]</div>
<p>b) Generate a sample from <span class="math notranslate nohighlight">\(\mathrm{U}[a,b]\)</span> for arbitrary <span class="math notranslate nohighlight">\(a\)</span>, <span class="math notranslate nohighlight">\(b\)</span> using <code class="docutils literal notranslate"><span class="pre">numpy.random.Generator.uniform</span></code> and the observation in a). This is called the inverse transform sampling method<span class="math notranslate nohighlight">\(\idx{inverse transform sampling method}\xdi\)</span>. (<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_intro_notebook.ipynb">Open in Colab</a>)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="n">b</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">()</span>
<span class="c1"># EDIT THIS LINE: transform X to obtain a random variable Y ~ U[a,b]</span>
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>Random vectors and matrices</strong> A random vector <span class="math notranslate nohighlight">\(\bX = (X_1,\ldots,X_d)\)</span> in <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span> is a <span class="math notranslate nohighlight">\(d\)</span>-dimensional vector whose coordinates <span class="math notranslate nohighlight">\(X_1,\ldots,X_d\)</span> are correlated random variables; formally, they live on the same probability space.</p>
<p>The mean <span class="math notranslate nohighlight">\(\bmu_\bX\)</span> of a random vector <span class="math notranslate nohighlight">\(\bX\)</span> is itself a vector, whose coordinates are the means of the coordinates,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\bmu_\bX
=
\E[\bX]
=
\begin{pmatrix}
\E[X_1]\\
\vdots\\
\E[X_d]
\end{pmatrix}.
\end{split}\]</div>
<p>The linearity of expectation generalizes to (check it!)</p>
<div class="math notranslate nohighlight">
\[
\E[A \bX + \mathbf{b}]
= A\,\E[\bX] + \mathbf{b}
\]</div>
<p>for a deterministic (i.e., non-random) matrix <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{\ell \times d}\)</span> and vector <span class="math notranslate nohighlight">\(\mathbf{b} \in \mathbb{R}^{\ell}\)</span>.</p>
<p>A random matrix <span class="math notranslate nohighlight">\(\mathbf{M} = (M_{i,j})_{i,j} \in \mathbb{R}^{\ell \times d}\)</span> is a matrix whose entries are correlated random variables. The expectation of a random matrix is the (deterministic) matrix whose entries are the expectations of the entries of <span class="math notranslate nohighlight">\(\mathbf{M}\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbb{E}[\mathbf{M}]
= \begin{pmatrix}
\E[M_{1,1}] &amp; \cdots &amp; \E[M_{1,d}]\\
\vdots &amp; \ddots &amp; \vdots\\
\E[M_{\ell,1}] &amp; \cdots &amp; \E[M_{\ell,d}]
\end{pmatrix}.
\end{split}\]</div>
<p>The linearity of expectation generalizes to (check it!)</p>
<div class="math notranslate nohighlight">
\[
\E[A \mathbf{M} + B]
= A\,\E[\mathbf{M}] + B
\]</div>
<p>for a deterministic matrices <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{k \times \ell}\)</span> and <span class="math notranslate nohighlight">\(B \in \mathbb{R}^{k \times d}\)</span>.</p>
<p>The covariance matrix <span class="math notranslate nohighlight">\(\bSigma_{\bX}\)</span> of a random vector <span class="math notranslate nohighlight">\(\bX\)</span> (also known as variance matrix or variance-covariance matrix) is the matrix whose <span class="math notranslate nohighlight">\((i,j)\)</span>-entry is the covariance of coordinates <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span></p>
<div class="math notranslate nohighlight">
\[
(\bSigma_{\bX})_{i,j} = \mathrm{cov}[X_i,X_j] = \mathbb{E}\left[(X_i - \mathbb{E}[X_i])(X_j - \mathbb{E}[X_j])\right].
\]</div>
<p>We will also sometimes denote it as <span class="math notranslate nohighlight">\(\mathrm{K}_{\bX, \bX} := \bSigma_{\bX}\)</span>, or simply as <span class="math notranslate nohighlight">\(\mathrm{Cov}[\bX]\)</span>.</p>
<p>Using a previous example, this can be written in a more compact matrix form as the following</p>
<div class="math notranslate nohighlight">
\[
\bSigma_\bX
= \E\left[
(\bX - \bmu_\bX) (\bX - \bmu_\bX)^T 
\right],
\]</div>
<p>where we think of <span class="math notranslate nohighlight">\(\bX\)</span> as a column vector. Observe that in this calculation, <span class="math notranslate nohighlight">\((\bX - \bmu_\bX) (\bX - \bmu_\bX)^T\)</span> is a random matrix.</p>
<p>Covariance matrices have two special properties: they are symmetric and positive semidefinite.</p>
<p>Symmetry comes from the definition of the covariance:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathrm{Cov}[X_i,X_j]
&amp;= \mathbb{E}\left[(X_i - \mathbb{E}[X_i])(X_j - \mathbb{E}[X_j])\right]\\
&amp;= \mathbb{E}\left[(X_j - \mathbb{E}[X_j])(X_i - \mathbb{E}[X_i])\right]\\
&amp;=\mathrm{Cov}[X_j,X_i].
\end{align*}\]</div>
<p><strong>THEOREM</strong> <strong>(Positive Semidefiniteness of the Covariance)</strong> <span class="math notranslate nohighlight">\(\idx{positive semidefiniteness of the covariance}\xdi\)</span> The covariance matrix <span class="math notranslate nohighlight">\(\bSigma_\bX\)</span> of a random vector <span class="math notranslate nohighlight">\(\bX\)</span> is positive semidefinite. <span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p><em>Proof idea:</em> The expression <span class="math notranslate nohighlight">\(\langle \mathbf{z}, \bSigma_\bX \mathbf{z} \rangle\)</span>
can be re-written as the variance of a sum of random variables. Variances are always non-negative.</p>
<p><em>Proof:</em> By definition of the covariance,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\langle \mathbf{z}, \bSigma_\bX \mathbf{z} \rangle 
&amp;= \sum_{i,j} z_i z_j \mathrm{Cov}[X_i, X_j]\\
&amp;= \sum_{i,j} z_i z_j \mathbb{E}\left[(X_i - \mathbb{E}[X_i])(X_j - \mathbb{E}[X_j])\right]\\
&amp;= \sum_{i,j} \mathbb{E}\left[(z_i X_i - \mathbb{E}[ z_iX_i])(z_j X_j - \mathbb{E}[z_j X_j])\right]\\
&amp;= \sum_{i,j} \mathrm{Cov}[z_i X_i, z_j X_j].
\end{align*}\]</div>
<p>Using the fact that <span class="math notranslate nohighlight">\(\mathrm{Cov}[X, X] = \mathrm{Var}[X]\)</span> and <span class="math notranslate nohighlight">\(\mathrm{Cov}[X, Y] = \mathrm{Cov}[Y, X]\)</span>, this last sum can be rearranged as</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^d \mathrm{Var}[z_i X_i]
+ 2 \sum_{i &lt; j} \mathrm{Cov}[z_i X_i, z_j X_j].
\]</div>
<p>We have encountered this expression previously! By the <em>Variance of a Sum</em>, this is</p>
<div class="math notranslate nohighlight">
\[
\mathrm{Var}\left[\sum_{i=1}^d z_i X_i\right],
\]</div>
<p>which is non-negative. That concludes the proof. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>KNOWLEDGE CHECK:</strong> Give a shorter proof of the <em>Positive Semidefiniteness of the Covariance</em> using the matrix form of <span class="math notranslate nohighlight">\(\bSigma_\bX\)</span>. <span class="math notranslate nohighlight">\(\checkmark\)</span></p>
<p>Later on, we will need covariance matrix of a linear transformation. We first note that the covariance has convenient linearity properties:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathrm{Cov}[\alpha X + \beta, Y]
&amp;= \mathbb{E}\left[(\alpha X + \beta - \mathbb{E}[\alpha X + \beta])(Y - \mathbb{E}[Y])\right]\\
&amp;= \mathbb{E}\left[(\alpha X - \mathbb{E}[\alpha X])(Y - \mathbb{E}[Y])\right]\\
&amp;= \alpha \,\mathbb{E}\left[( X - \mathbb{E}[X])(Y - \mathbb{E}[Y])\right]\\
&amp;= \alpha \,\mathrm{Cov}[X, Y].
\end{align*}\]</div>
<p>Moreover,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathrm{Cov}[X + Z, Y]
&amp;= \mathbb{E}\left[(X + Z - \mathbb{E}[X + Z])(Y - \mathbb{E}[Y])\right]\\
&amp;= \mathbb{E}\left[(X - \mathbb{E}[X] + Z - \mathbb{E}[Z])(Y - \mathbb{E}[Y])\right]\\
&amp;= \mathbb{E}\left[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])\right] + \mathbb{E}\left[(Z - \mathbb{E}[Z])(Y - \mathbb{E}[Y])\right]\\
&amp;=  \mathrm{Cov}[X, Y] + \mathrm{Cov}[Z, Y].
\end{align*}\]</div>
<p><strong>LEMMA</strong> <strong>(Covariance of a Linear Transformation)</strong> <span class="math notranslate nohighlight">\(\idx{covariance of a linear transformation}\xdi\)</span> Let <span class="math notranslate nohighlight">\(\bX = (X_1,\ldots,X_d)\)</span> be a random vector in <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span> with finite variances (i.e., <span class="math notranslate nohighlight">\(\mathrm{Var}[X_i] &lt; +\infty\)</span> for all <span class="math notranslate nohighlight">\(i\)</span>). For a deterministic (i.e., non-random) <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{\ell \times d}\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
\mathrm{Cov}[A \mathbf{X}] 
= A \,\mathrm{Cov}[\mathbf{X}] \,A^T 
\]</div>
<p><span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> In matrix form, letting <span class="math notranslate nohighlight">\(\bmu_{\mathbf{X}}\)</span> and <span class="math notranslate nohighlight">\(\bmu_{\mathbf{Y}} = A \bmu_{\mathbf{X}}\)</span> be respectively the means of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span>, we have</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathrm{Cov}[\mathbf{Y}]
&amp;= \mathbb{E}\left[(\mathbf{Y} - \bmu_{\mathbf{Y}})(\mathbf{Y} - \bmu_{\mathbf{Y}})^T\right]\\
&amp;= \mathbb{E}\left[(A \mathbf{X} - A \bmu_{\mathbf{X}})(A \mathbf{X} - A \bmu_{\mathbf{X}})^T\right]\\
&amp;= \mathbb{E}\left[A(\mathbf{X} - \bmu_{\mathbf{X}})(\mathbf{X} - \bmu_{\mathbf{X}})^T A^T\right]\\
&amp;= A \,\mathbb{E}\left[(\mathbf{X} - \bmu_{\mathbf{X}})(\mathbf{X} - \bmu_{\mathbf{X}})^T\right] A^T\\
\end{align*}\]</div>
<p>where we used linearity of expectation twice. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>For two random vectors <span class="math notranslate nohighlight">\(\mathbf{X} \in \mathbb{R}^{n}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{Y} \in \mathbb{R}^m\)</span> defined on the same probability space, we define the cross covariance matrix as</p>
<div class="math notranslate nohighlight">
\[
\mathrm{Cov}[\mathbf{X}, \mathbf{Y}]
= \E[(\mathbf{X} - \E[\mathbf{X}])(\mathbf{Y} - \E[\mathbf{Y}])^T].
\]</div>
<p>This is a matrix of dimension <span class="math notranslate nohighlight">\(n \times m\)</span>.</p>
<p>The cross covariance matrix of a random vector with itself is the covariance matrix.</p>
<p><strong>Normal distribution</strong> <span class="math notranslate nohighlight">\(\idx{normal or Gaussian distribution}\xdi\)</span> Recall that a standard Normal variable <span class="math notranslate nohighlight">\(X\)</span> has PDF</p>
<div class="math notranslate nohighlight">
\[
f_X(x) 
= \frac{1}{\sqrt{2 \pi}}
\exp\left(
- x^2/2
\right).
\]</div>
<p>Its mean is <span class="math notranslate nohighlight">\(0\)</span> and its variance is <span class="math notranslate nohighlight">\(1\)</span>.</p>
<!--ONLINE ONLY

![Probability density function of a standard normal variable (with help of ChatGPT; conde converted and adapted from ([Source](https://commons.wikimedia.org/wiki/File:Standard_Normal_Distribution.svg)))](./figs/pdf-norm.png)

--><p>To construct a <span class="math notranslate nohighlight">\(d\)</span>-dimensional version, we take <span class="math notranslate nohighlight">\(d\)</span> independent standard Normal variables <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_d\)</span> and form the vector <span class="math notranslate nohighlight">\(\mathbf{X} = (X_1,\ldots,X_d)\)</span>. We will say that <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is a standard Normal <span class="math notranslate nohighlight">\(d\)</span>-vector. By independence, its joint PDF is given by the product of the PDFs of the <span class="math notranslate nohighlight">\(X_i\)</span>’s, that is,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
f_{\mathbf{X}}(\mathbf{x})
&amp;= \prod_{i=1}^d \frac{1}{\sqrt{2 \pi}}
\exp\left(
- x_i^2/2
\right)\\
&amp;= \frac{1}{\prod_{i=1}^d \sqrt{2 \pi}}
\exp\left(
- \sum_{i=1}^d x_i^2/2
\right)\\
&amp;= \frac{1}{(2 \pi)^{d/2}} 
\exp(-\|\mathbf{x}\|^2/2).
\end{align*}\]</div>
<p>We can also shift and scale it.</p>
<p><strong>DEFINITION</strong> <strong>(Spherical Gaussian)</strong> <span class="math notranslate nohighlight">\(\idx{spherical Gaussian}\xdi\)</span> Let <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span> be a standard Normal <span class="math notranslate nohighlight">\(d\)</span>-vector,
let <span class="math notranslate nohighlight">\(\bmu \in \mathbb{R}^d\)</span> and let <span class="math notranslate nohighlight">\(\sigma \in \mathbb{R}_+\)</span>. Then we will refer to the transformed random variable <span class="math notranslate nohighlight">\(\mathbf{X} = \bmu + \sigma \mathbf{Z}\)</span> as a spherical Gaussian with mean <span class="math notranslate nohighlight">\(\bmu\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>. We use the notation <span class="math notranslate nohighlight">\(\mathbf{Z} \sim N_d(\bmu, \sigma^2 I)\)</span>. <span class="math notranslate nohighlight">\(\natural\)</span></p>
<p><strong>NUMERICAL CORNER:</strong> The following function generates <span class="math notranslate nohighlight">\(n\)</span> data points from a spherical <span class="math notranslate nohighlight">\(d\)</span>-dimensional Gaussians with variance <span class="math notranslate nohighlight">\(\sigma^2\)</span> and mean <span class="math notranslate nohighlight">\(\bmu\)</span>.</p>
<p>Below, <a class="reference external" href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.normal.html"><code class="docutils literal notranslate"><span class="pre">rng.normal(0,1,(n,d))</span></code></a> generates a <code class="docutils literal notranslate"><span class="pre">n</span></code> independent <code class="docutils literal notranslate"><span class="pre">d</span></code>-dimensional spherical Gaussian with mean <span class="math notranslate nohighlight">\(\mathbf{0}\)</span> (as row vectors).</p>
<p>Throughout, when defining a function that uses a random number generator (RNG), we initialize the RNG outside the function and pass the RNG to it. It allows us to maintain control over the random number generation process at a higher level and ensures consistent results across multiple runs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">spherical_gaussian</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sig</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">sig</span> <span class="o">*</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,(</span><span class="n">n</span><span class="p">,</span><span class="n">d</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>We generate <span class="math notranslate nohighlight">\(100\)</span> data points in dimension <span class="math notranslate nohighlight">\(d=2\)</span>. We take <span class="math notranslate nohighlight">\(\sigma^2 = 1\)</span> and <span class="math notranslate nohighlight">\(\bmu = w \mathbf{e}_1\)</span>. Below we use the function <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.hstack.html"><code class="docutils literal notranslate"><span class="pre">numpy.hstack</span></code></a> to create a vector by concatenating two given vectors. We use <code class="docutils literal notranslate"><span class="pre">[w]</span></code> to create a vector with a single entry <code class="docutils literal notranslate"><span class="pre">w</span></code>.  We also use the function <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.zeros.html"><code class="docutils literal notranslate"><span class="pre">numpy.zeros</span></code></a> to create an all-zero vector.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">sig</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">1.</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">(([</span><span class="n">w</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">d</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">spherical_gaussian</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sig</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/15d79e434eed510f0315b6f8bc6827fba6064a0b11928b663d6aa127311d5b95.png" src="../../_images/15d79e434eed510f0315b6f8bc6827fba6064a0b11928b663d6aa127311d5b95.png" />
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p>More generally, we consider mixtures of spherical Gaussians<span class="math notranslate nohighlight">\(\idx{mixture of spherical Gaussians}\xdi\)</span>, a special case of the Gaussian Mixture Model (GMM). To keep things simple, we will restrict ourselves to mixtures of <span class="math notranslate nohighlight">\(2\)</span> Gaussians, but this can easily be generalized.</p>
<p>This model has a number of parameters. For <span class="math notranslate nohighlight">\(i=0,1\)</span>, we have a mean <span class="math notranslate nohighlight">\(\bmu_i \in \mathbb{R}^d\)</span> and a positive variance <span class="math notranslate nohighlight">\(\sigma_i \in \mathbb{R}_+\)</span>. We also have mixture weights <span class="math notranslate nohighlight">\(\phi_0, \phi_1 \in (0,1)\)</span> such that <span class="math notranslate nohighlight">\(\phi_0 + \phi_1 = 1\)</span>. Suppose we want to generate a total of <span class="math notranslate nohighlight">\(n\)</span> samples.</p>
<p>For each sample <span class="math notranslate nohighlight">\(j=1,\ldots, n\)</span>, independently from everything else:</p>
<ol class="arabic simple">
<li><p>We first pick a component <span class="math notranslate nohighlight">\(i \in \{0,1\}\)</span> at random according to the mixture weights, that is, <span class="math notranslate nohighlight">\(i=0\)</span> is chosen with probability <span class="math notranslate nohighlight">\(\phi_0\)</span> and <span class="math notranslate nohighlight">\(i=1\)</span> is chosen with probability <span class="math notranslate nohighlight">\(\phi_1\)</span>.</p></li>
<li><p>We generate a sample <span class="math notranslate nohighlight">\(\bX_j = (X_{j,1},\ldots,X_{j,d})\)</span> according to a spherical Gaussian with mean <span class="math notranslate nohighlight">\(\bmu_i\)</span> and variance <span class="math notranslate nohighlight">\(\sigma_i^2\)</span>.</p></li>
</ol>
<p>This is straightforward to implement by using <a class="reference external" href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.choice.html"><code class="docutils literal notranslate"><span class="pre">numpy.random.Generator.choice</span></code></a> to choose the component of each sample.</p>
<p>The code is the following. It returns an <code class="docutils literal notranslate"><span class="pre">d</span></code> by <code class="docutils literal notranslate"><span class="pre">n</span></code> array <code class="docutils literal notranslate"><span class="pre">X</span></code>, where each row is a sample from a 2-component spherical Gaussian mixture.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gmm2spherical</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">phi0</span><span class="p">,</span> <span class="n">phi1</span><span class="p">,</span> <span class="n">mu0</span><span class="p">,</span> <span class="n">sig0</span><span class="p">,</span> <span class="n">mu1</span><span class="p">,</span> <span class="n">sig1</span><span class="p">):</span>
    
    <span class="n">phi</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sig</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">phi0</span><span class="p">,</span> <span class="n">phi1</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">mu0</span><span class="p">,</span> <span class="n">mu1</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">sig0</span><span class="p">,</span><span class="n">sig1</span><span class="p">))</span>

    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="n">d</span><span class="p">))</span>
    <span class="n">component</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">phi</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">spherical_gaussian</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">mu</span><span class="p">[</span><span class="n">component</span><span class="p">[</span><span class="n">i</span><span class="p">],:],</span> <span class="n">sig</span><span class="p">[</span><span class="n">component</span><span class="p">[</span><span class="n">i</span><span class="p">]])</span>
    
    <span class="k">return</span> <span class="n">X</span>
</pre></div>
</div>
</div>
</div>
<p><strong>NUMERICAL CORNER:</strong> Let us try it with following parameters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">sig0</span><span class="p">,</span> <span class="n">sig1</span><span class="p">,</span> <span class="n">phi0</span><span class="p">,</span> <span class="n">phi1</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.8</span>
<span class="n">mu0</span><span class="p">,</span> <span class="n">mu1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">(([</span><span class="n">w</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">d</span><span class="o">-</span><span class="mi">1</span><span class="p">))),</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">(([</span><span class="o">-</span><span class="n">w</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">d</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">gmm2spherical</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">phi0</span><span class="p">,</span> <span class="n">phi1</span><span class="p">,</span> <span class="n">mu0</span><span class="p">,</span> <span class="n">sig0</span><span class="p">,</span> <span class="n">mu1</span><span class="p">,</span> <span class="n">sig1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/3f89851a75a74a4de8076330f3680dccba3225031d7cde81abef9d09bf19abcf.png" src="../../_images/3f89851a75a74a4de8076330f3680dccba3225031d7cde81abef9d09bf19abcf.png" />
</div>
</div>
<p>As expected, we observe two clusters. The one on the right (component <span class="math notranslate nohighlight">\(0\)</span>) is sparser (i.e., it contains fewer data points) since <code class="docutils literal notranslate"><span class="pre">phi0</span></code> is much smaller than <code class="docutils literal notranslate"><span class="pre">phi1</span></code>. It is also larger as its variance is larger.</p>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><em><strong>Self-assessment quiz</strong></em> <em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>1</strong> Let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times m}\)</span> and <span class="math notranslate nohighlight">\(B \in \mathbb{R}^{m \times p}\)</span>. What are the dimensions of the matrix product <span class="math notranslate nohighlight">\(AB\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(n \times m\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(m \times p\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(n \times p\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(p \times n\)</span></p>
<p><strong>2</strong> Which of the following is NOT a property of the transpose of a matrix?</p>
<p>a) <span class="math notranslate nohighlight">\((A^T)^T = A\)</span></p>
<p>b) <span class="math notranslate nohighlight">\((\gamma A)^T = \gamma A^T\)</span> for any scalar <span class="math notranslate nohighlight">\(\gamma \in \mathbb{R}\)</span></p>
<p>c) <span class="math notranslate nohighlight">\((A + B)^T = A^T + B^T\)</span></p>
<p>d) <span class="math notranslate nohighlight">\((AB)^T = A^T B^T\)</span></p>
<p><strong>3</strong> Let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times m}\)</span> be a matrix and <span class="math notranslate nohighlight">\(\bx \in \mathbb{R}^m\)</span> be a column vector. Which of the following is true about the matrix-vector product <span class="math notranslate nohighlight">\(A\bx\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(A\bx\)</span> is a linear combination of the rows of <span class="math notranslate nohighlight">\(A\)</span> where the coefficients are the entries of <span class="math notranslate nohighlight">\(\bx\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(A\bx\)</span> is a linear combination of the columns of <span class="math notranslate nohighlight">\(A\)</span> where the coefficients are the entries of <span class="math notranslate nohighlight">\(\bx\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(A\bx\)</span> is a linear combination of the rows of <span class="math notranslate nohighlight">\(A\)</span> where the coefficients are the entries of the first row of <span class="math notranslate nohighlight">\(A\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(A\bx\)</span> is a linear combination of the columns of <span class="math notranslate nohighlight">\(A\)</span> where the coefficients are the entries of the first column of <span class="math notranslate nohighlight">\(A\)</span></p>
<p><strong>4</strong> Which of the following is NOT a property of the variance of a random variable?</p>
<p>a) <span class="math notranslate nohighlight">\(\mathrm{Var}[\alpha X + \beta] = \alpha^2 \mathrm{Var}[X]\)</span> for <span class="math notranslate nohighlight">\(\alpha, \beta \in \mathbb{R}\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(\mathrm{Var}[X] = \E[(X - \E[X])^2]\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(\mathrm{Var}[X] \ge 0\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(\mathrm{Var}[X + Y] = \mathrm{Var}[X] + \mathrm{Var}[Y]\)</span> for any random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span></p>
<p><strong>5</strong> If <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is a random vector in <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span> with mean vector <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_{\mathbf{X}}\)</span> and covariance matrix <span class="math notranslate nohighlight">\(\bSigma_{\mathbf{X}}\)</span>, which of the following expressions represents the covariance matrix <span class="math notranslate nohighlight">\(\bSigma_{\mathbf{X}}\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(\E[(\mathbf{X} - \boldsymbol{\mu}_{\mathbf{X}})^2]\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(\E[(\mathbf{X} - \boldsymbol{\mu}_{\mathbf{X}})(\mathbf{X} - \boldsymbol{\mu}_{\mathbf{X}})^T]\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(\E[(\mathbf{X} - \boldsymbol{\mu}_{\mathbf{X}})^T (\mathbf{X} - \boldsymbol{\mu}_{\mathbf{X}})]\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(\E[(\mathbf{X} - \boldsymbol{\mu}_{\mathbf{X}})^T] \E[(\mathbf{X} - \boldsymbol{\mu}_{\mathbf{X}})]\)</span></p>
<p>Answer for 1: c. Justification: Taking the number of rows of the first matrix and the number of columns of the second one, we see that the dimension of <span class="math notranslate nohighlight">\(AB\)</span> is <span class="math notranslate nohighlight">\(n \times p\)</span>.</p>
<p>Answer for 2: d. Justification: The text states that <span class="math notranslate nohighlight">\((BC)^T = C^T B^T\)</span>, not <span class="math notranslate nohighlight">\(B^T C^T\)</span>.</p>
<p>Answer for 3: b. Justification: The text states, “<span class="math notranslate nohighlight">\(A\bx\)</span> is a linear combination of the columns of <span class="math notranslate nohighlight">\(A\)</span> where the coefficients are the entries of <span class="math notranslate nohighlight">\(\bx\)</span>. In matrix form, <span class="math notranslate nohighlight">\(A\bx = \sum_{j=1}^m A_{\cdot,j}x_j\)</span>.”</p>
<p>Answer for 4: d. Justification: The text states that the variance of a sum of random variables is given by <span class="math notranslate nohighlight">\(\mathrm{Var}[X_1 + ... + X_n] = \sum_{i=1}^n \mathrm{Var}[X_i] + 2 \sum_{i&lt;j} \mathrm{Cov}[X_i, X_j]\)</span>, which is only equal to <span class="math notranslate nohighlight">\(\mathrm{Var}[X] + \mathrm{Var}[Y]\)</span> when the covariance term is zero (e.g., when <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent).</p>
<p>Answer for 5: b. Justification: The covariance matrix <span class="math notranslate nohighlight">\(\bSigma_{\mathbf{X}}\)</span> of a random vector <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is given by <span class="math notranslate nohighlight">\(\bSigma_{\mathbf{X}} = \E[(\mathbf{X} - \boldsymbol{\mu}_{\mathbf{X}})(\mathbf{X} - \boldsymbol{\mu}_{\mathbf{X}})^T]\)</span>.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chap01_intro/02_review"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../01_motiv/roch-mmids-intro-motiv.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">1.1. </span>Motivating example: identifying penguin species</p>
      </div>
    </a>
    <a class="right-next"
       href="../03_clustering/roch-mmids-intro-clustering.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">1.3. </span>Clustering: an objective, an algorithm and a guarantee</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vectors-and-matrices">1.2.1. Vectors and matrices</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#differential-calculus">1.2.2. Differential calculus</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability">1.2.3. Probability</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Sebastien Roch
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>