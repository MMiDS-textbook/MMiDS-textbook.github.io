{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b749b43",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell",
     "colab-keep"
    ]
   },
   "source": [
    "***\n",
    "\n",
    "*Course:* [Math 535](https://people.math.wisc.edu/~roch/mmids/) - Mathematical Methods in Data Science (MMiDS)  \n",
    "*Chapter:* 1-Introduction   \n",
    "*Author:* [Sebastien Roch](https://people.math.wisc.edu/~roch/), Department of Mathematics, University of Wisconsin-Madison  \n",
    "*Updated:* Jan 4, 2024   \n",
    "*Copyright:* &copy; 2024 Sebastien Roch\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd62aef",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# IF RUNNING ON GOOGLE COLAB, UNCOMMENT THE FOLLOWING CODE CELL\n",
    "# When prompted, upload: \n",
    "#     * mmids.py\n",
    "#     * penguins-measurements.csv\n",
    "#     * penguins-species.csv\n",
    "# from your local file system\n",
    "# Files at: https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/tree/main/utils\n",
    "# Alternative instructions: https://colab.research.google.com/notebooks/io.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e1a1c6",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell",
     "colab-uncomment"
    ]
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25b5804",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# PYTHON 3\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "from numpy.random import default_rng\n",
    "rng = default_rng(535)\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import mmids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bcdc12",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Motivating example: species identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ad6fe9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Here is a [penguin dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set) collected and made available by [Dr. Kristen Gorman](https://www.uaf.edu/cfos/people/faculty/detail/kristen-gorman.php) and the [Palmer Station, Antarctica LTER](https://pallter.marine.rutgers.edu/). We will upload the data in the form of a data table (similar to a spreadsheet) called [`DataFrame`](https://pandas.pydata.org/docs/reference/frame.html) in [`pandas`](https://pandas.pydata.org/docs/), where the columns are different measurements (or features) and the rows are different samples. Below, we load the data using [`pandas.read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html?highlight=read_csv#) and show the first $5$ lines of the dataset (see [`DataFrame.head`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html)). This dataset is a simplified version (i.e., with some columns removed) of the full dataset, maintained by [Allison Horst](https://allisonhorst.com/) at this [GitHub page](https://github.com/allisonhorst/palmerpenguins/blob/main/README.md). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eceafca",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('penguins-measurements.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d36b7a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Observe that this dataset has missing values (i.e., the entries `NaN` above). A common way to deal with this issue is to remove all rows with missing values. This can be done using [`pandas.DataFrame.dropna`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4344973d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7095e7cd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "There are $342$ samples, as can be seen by using [`pandas.DataFrame.shape`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.shape.html) which gives the dimensions of the DataFrame as a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3965d6e",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52db3f2b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Here is a summary of the data (see [`pandas.DataFrame.describe`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cf3b54",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343aa25f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Let's first extract the columns into a Numpy array using [`pandas.DataFrame.to_numpy()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_numpy.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83faa33",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = df[['bill_length_mm', 'bill_depth_mm', \n",
    "        'flipper_length_mm', 'body_mass_g']].to_numpy()\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73aa850",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We visualize two measurements in the data, the bill depth and flipper length. (The original dataset used the more precise term [culmen](https://en.wikipedia.org/wiki/Beak#Culmen) depth.) Below, each point is a sample. This is called a [scatter plot](https://en.wikipedia.org/wiki/Scatter_plot). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d2e7a1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:,1], X[:,2], s=10)\n",
    "plt.xlabel('bill_depth_mm')\n",
    "plt.ylabel('flipper_length_mm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e61139",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Now let's look at the full dataset. Visualizing the full $4$-dimensional data is not straightforward. One way to do this is to consider all pairwise scatter plots. We use the function [`seaborn.pairplot`](https://seaborn.pydata.org/generated/seaborn.pairplot.html) from the library [Seaborn](https://seaborn.pydata.org/index.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2ec13e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.pairplot(df, vars=['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g'], height=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2543eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Background: review of basic linear algebra, calculus, and probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c26d95",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** In Numpy, a vector is defined as a 1d array. We first must import the [Numpy](https://numpy.org) package, which is often abbreviated by `np`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd7d6fc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "u = np.array([1., 3., 5. ,7.])\n",
    "print(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eaed85",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To obtain the norm of a vector, we can use the function [`linalg.norm`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html) (which requires the `numpy.linalg` package):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44dbb2d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "LA.norm(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993a3b4a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "which we check next \"by hand\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9036190",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.sqrt(np.sum(u ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af68856",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "In Numpy, [`**`](https://numpy.org/doc/stable/reference/generated/numpy.power.html) indicates element-wise exponentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ef4e9c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee0cb3f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We will often work with collections of $n$ vectors $\\mathbf{x}_1, \\ldots, \\mathbf{x}_n$ in $\\mathbb{R}^d$ and it will be convenient to stack them up into a matrix\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{x}_1^T \\\\\n",
    "\\mathbf{x}_2^T \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{x}_n^T \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x_{11} & x_{12} & \\cdots & x_{1d} \\\\\n",
    "x_{21} & x_{22} & \\cdots & x_{2d} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{n1} & x_{n2} & \\cdots & x_{nd} \\\\\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad8bbef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To create a matrix out of two vectors, we use the function [`numpy.stack`](https://numpy.org/doc/stable/reference/generated/numpy.stack.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a38041",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "u = np.array([1., 3., 5., 7.])\n",
    "v = np.array([2., 4., 6., 8.])\n",
    "X = np.stack((u,v),axis=0)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43c81b4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Quoting the documentation:\n",
    "\n",
    "> The axis parameter specifies the index of the new axis in the dimensions of the result. For example, if axis=0 it will be the first dimension and if axis=-1 it will be the last dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496bab0b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The same scheme still works with more than two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ebcfdd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "u = np.array([1., 3., 5., 7.])\n",
    "v = np.array([2., 4., 6., 8.])\n",
    "w = np.array([9., 8., 7., 6.])\n",
    "X = np.stack((u,v,w))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a6f2f2",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d9798f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** In Numpy, the Frobenius norm of a matrix can be computed using the function [`numpy.linalg.norm`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a5f711",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "A = np.array([[1., 0.],[0., 1.],[0., 0.]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1c28df",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "LA.norm(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e69775",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07ea7f4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** The function $f(x) = x^2$ over $\\mathbb{R}$ has a global minimizer at $x^* = 0$. Indeed, we clearly have $f(x) \\geq 0$ for all $x$ while $f(0) = 0$. To plot the function, we use the [matplotlib](https://matplotlib.org) package, and specifically its function [`matplotlib.pyplot.plot`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html). We also use the function [`numpy.linspace`](https://numpy.org/doc/stable/reference/generated/numpy.linspace.html) to create an array of evenly spaced numbers where we evaluate $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1097ad43",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = np.linspace(-2,2,100)\n",
    "y = x ** 2\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6a5e73",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The function $f(x) = e^x$ over $\\mathbb{R}$ does not have a global minimizer. Indeed, $f(x) > 0$ but no $x$ achieves $0$. And, for any $m > 0$, there is $x$ small enough such that $f(x) < m$. Note that $\\mathbb{R}$ is *not* bounded, therefore the *Extreme Value Theorem* does not apply here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7233567a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-2,2,100)\n",
    "y = np.exp(x)\n",
    "plt.plot(x,y)\n",
    "plt.ylim(0,5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f528e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The function $f(x) = (x+1)^2 (x-1)^2$ over $\\mathbb{R}$ has two global minimizers at $x^* = -1$ and $x^{**} = 1$. Indeed, $f(x) \\geq 0$ and $f(x) = 0$ if and only $x = x^*$ or $x = x^{**}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7115b23",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-2,2,100)\n",
    "y = ((x+1)**2) * ((x-1)**2)\n",
    "plt.plot(x,y)\n",
    "plt.ylim(0,5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c054b95e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105fe7cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We can use simulations to confirm the *Weak Law of Large Numbers*. Recall that a uniform random variable over the interval $[a,b]$ has density\n",
    "\n",
    "$$\n",
    "f_{X}(x)\n",
    "= \\begin{cases}\n",
    "\\frac{1}{b-a} & x \\in [a,b] \\\\\n",
    "0 & \\text{o.w.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "We write $X \\sim \\mathrm{U}[a,b]$. We can obtain a sample from $\\mathrm{U}[0,1]$ by using the function [`numpy.random`](https://numpy.org/doc/stable/reference/random/generator.html) in Numpy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621c62c2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from numpy.random import default_rng\n",
    "rng = default_rng(535)\n",
    "rng.random()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2111493",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Now we take $n$ samples from $\\mathrm{U}[0,1]$ and compute their sample mean. We repeat $k$ times and display the empirical distribution of the sample means using an [histogram](https://en.wikipedia.org/wiki/Histogram)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b3c376",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def lln_unif(n, k):\n",
    "    sample_mean = [np.mean(rng.random(n)) for i in range(k)]\n",
    "    plt.hist(sample_mean,bins=15)\n",
    "    plt.xlim(0,1)\n",
    "    plt.title(f'n={n}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09105775",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We start with $n=10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8636c3a9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lln_unif(10, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3f7692",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Taking $n$ much larger leads to more concentration around the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be0c218",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lln_unif(100, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4766e9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ececaced",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We plot the PDF of a standard normal distribution. We use the function [scipy.stats.norm](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html) from the [SciPy library](https://scipy.org), which outputs the PDF. The following code was adapted from [here](https://commons.wikimedia.org/wiki/File:Standard_Normal_Distribution.svg) with the help of ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c69431",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "# Plot the normal distribution curve\n",
    "x = np.linspace(-4, 4, 100)\n",
    "y = norm.pdf(x)\n",
    "plt.plot(x, y, color='black')\n",
    "\n",
    "# Fill areas under the curve for different standard deviations\n",
    "plt.fill_between(x, y, where=(x > -1) & (x < 1), color='red', alpha=0.25)\n",
    "plt.fill_between(x, y, where=(x > -2) & (x < 2), color='red', alpha=0.25)\n",
    "plt.hlines(norm.pdf(1), -1, 1, color='black', linestyle='dashed')\n",
    "plt.hlines(norm.pdf(2), -2, 2, color='black', linestyle='dashed')\n",
    "plt.text(0, norm.pdf(1) + 0.01, \"68.3%\", ha='center')\n",
    "plt.text(0, norm.pdf(2) + 0.01, \"95.4%\", ha='center')\n",
    "\n",
    "# Set labels, title, and xticks\n",
    "plt.xlabel(\"Standard Deviations from Mean\")\n",
    "plt.ylabel(\"PDF\")\n",
    "plt.xticks(range(-4, 5, 2), [f'{i}' for i in range(-4, 5, 2)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95023a83",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f717304",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** The following function generates $n$ data points from a spherical $d$-dimensional Gaussians with variance $1$ and mean $w \\mathbf{e}_1$. We will use it later in the chapter to simulate interesting datasets. \n",
    "\n",
    "Below, `rng.normal(0,1,d)` generates a `d`-dimensional spherical Gaussian with mean $\\mathbf{0}$. Below we use the function [`numpy.concatenate`](https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html) to create a vector by concatenating two given vectors. We use `[w]` to create a vector with a single entry `w`. We also use the function [`numpy.zeros`](https://numpy.org/doc/stable/reference/generated/numpy.zeros.html) to create an all-zero vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cb8b9c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def one_cluster(d, n, w):\n",
    "    X = np.stack(\n",
    "        [np.concatenate(([w], np.zeros(d-1))) + rng.normal(0,1,d) for _ in range(n)]\n",
    "    )\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49dbd4b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We generate $100$ data points in dimension $d=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819736b9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "d, n, w = 2, 100, 3.\n",
    "X = one_cluster(d, n, w)\n",
    "plt.scatter(X[:,0], X[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114a2d41",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25feba7b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Clustering: an objective, an algorithm and a guarantee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc3d629",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** Here's a numerical example. We first define a quadratic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814dafb6",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def q(a,b,c,x):\n",
    "    return a * (x**2) + b * x + c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be073280",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We plot it for different values of the coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4028a2",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-2, 2, 100)\n",
    "\n",
    "plt.plot(x, q(2,4,-1,x))\n",
    "plt.plot(x, q(2,-4,4,x))\n",
    "plt.plot(x, q(-2,0,4,x))\n",
    "\n",
    "plt.legend(['y1', 'y2', 'y3'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5a76a8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2606b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We are now ready to describe the <a href=\"https://en.wikipedia.org/wiki/K-means_clustering\">$k$-means algorithm</a>, also known as Lloyd's algorithm. We start from a random assignment of clusters. (An alternative [initialization strategy](https://en.wikipedia.org/wiki/K-means_clustering#Initialization_methods) is to choose $k$ representatives at random among the data points.) We then alternate between the optimal choices in the lemmas. In lieu of pseudo-code, we write out the algorithm in Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac689725",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The input `X` is assumed to be a collection of $n$ vectors $\\mathbf{x}_1, \\ldots, \\mathbf{x}_n \\in \\mathbb{R}^d$ stacked into a matrix, with one row for each data point. The other input, `k`, is the desired number of clusters. There is an optional input `maxiter` for the maximum number of iterations, which is set to $10$ by default.\n",
    "\n",
    "We first define separate functions for the two main steps. To find the minimum of an array, we use the function [`numpy.argmin`](https://numpy.org/doc/stable/reference/generated/numpy.argmin.html). We also use [`numpy.linalg.norm`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html) to compute the Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e070fe6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def opt_reps(X, k, assign):\n",
    "    (n, d) = X.shape\n",
    "    reps = np.zeros((k, d))\n",
    "    for i in range(k):\n",
    "        in_i = [j for j in range(n) if assign[j] == i]             \n",
    "        reps[i,:] = np.sum(X[in_i,:],axis=0) / len(in_i)\n",
    "    return reps\n",
    "\n",
    "def opt_clust(X, k, reps):\n",
    "    (n, d) = X.shape\n",
    "    dist = np.zeros(n)\n",
    "    assign = np.zeros(n, dtype=int)\n",
    "    for j in range(n):\n",
    "        dist_to_i = np.array([LA.norm(X[j,:] - reps[i,:]) for i in range(k)])\n",
    "        assign[j] = np.argmin(dist_to_i)\n",
    "        dist[j] = dist_to_i[assign[j]]\n",
    "    G = np.sum(dist ** 2)\n",
    "    print(G)\n",
    "    return assign"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c948994",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The main function follows. Below, `rng.integers(0,k,n)` is an array of `n` uniformly chosen integers between `0` and `k-1` (inclusive). (See [random.Generator.integers](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.integers.html) for details.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34d3b7a",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def kmeans(X, k, maxiter=10):\n",
    "    (n, d) = X.shape\n",
    "    assign = rng.integers(0,k,n)\n",
    "    reps = np.zeros((k, d), dtype=int)\n",
    "    for iter in range(maxiter):\n",
    "        # Step 1: Optimal representatives for fixed clusters\n",
    "        reps = opt_reps(X, k, assign) \n",
    "        # Step 2: Optimal clusters for fixed representatives\n",
    "        assign = opt_clust(X, k, reps) \n",
    "    return assign"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7b441f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We apply our implementation of $k$-means to the example above. We fix `k` to $3$. Here the data matrix `X` is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339bcf80",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([[1., 0.],[-2., 0.],[-2.,1.],[1.,-3.],[-10.,10.],[2.,-2.],[-3.,1.],[3.,-1.]])\n",
    "assign = kmeans(X, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064dcb24",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We vizualize the output by coloring the points according to their cluster assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea946bd4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1], c=assign, s=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d9d100",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We can compute the final representatives (optimal for the final assignment) by using the subroutine `opt_reps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82383899",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(opt_reps(X, 3, assign))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa77fda",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Each row is the center of the corresponding cluster. Note these match with the ones we previously computed. Indeed, the clustering is the same (although not necessarily in the same order)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1356e8fd",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b4d56d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We will test our implementation of $k$-means on the penguins dataset introduced earlier in the chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b18204",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We first extract the columns and combine them into a data matrix `X`. As we did previously, we also remove the rows with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2775120b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('penguins-measurements.csv')\n",
    "df = df.dropna()\n",
    "X = df[['bill_length_mm', 'bill_depth_mm', \n",
    "        'flipper_length_mm', 'body_mass_g']].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d366d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We  visualize a two-dimensional slice of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7449aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:,1], X[:,3], s=10)\n",
    "plt.xlabel('bill_depth_mm')\n",
    "plt.ylabel('body_mass_g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d195d60c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Observe that the features have quite different scales (tens versus thousands in the plot above). In such a case, it is common to standardize the data so that each feature has roughly the same scale. That is accomplished by, for each column of `X`, subtracting its empirical mean and dividing by its empirical standard deviation. We do this next. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ef368d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mean = np.mean(X, axis=0)  # Compute mean for each column\n",
    "std = np.std(X, axis=0)  # Compute standard deviation for each column\n",
    "X = (X - mean) / std # Standardize each column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17b8aaa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Now we run the $k$-means algorithm with $k=2$ clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7b3126",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "assign = kmeans(X, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67a8419",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We vizualize the output as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca889dd9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:,1], X[:,3], c=assign, s=10)\n",
    "plt.xlabel('bill_depth_mm (standardized)')\n",
    "plt.ylabel('body_mass_g (standardized)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa062a08",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "This clustering looks quite good. Nevertheless recall that:\n",
    "\n",
    "1. in this plot we are looking at only two of the four variables while $k$-means uses all of them, \n",
    "\n",
    "2. we are not guaranteed to find the best solution, \n",
    "\n",
    "3. our objective function is somewhat arbitrary, and \n",
    "\n",
    "4. it is not clear what the right choice of $k$ is. \n",
    "\n",
    "In fact, the original dataset provided the correct answer. Despite what the figure above may lead us to believe, there are in reality three separate species. So let's try with $k=3$ clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9f3d8b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "assign = kmeans(X, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de45a22d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The output does not seem quite right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a8943d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:,1], X[:,3], c=assign, s=10)\n",
    "plt.xlabel('bill_depth_mm (standardized)')\n",
    "plt.ylabel('body_mass_g (standardized)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c30bf3",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "But, remembering the warnings mentioned previously, let's look at a different two-dimensional slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bb75c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,3], c=assign, s=10)\n",
    "plt.xlabel('bill_length_mm (standardized)')\n",
    "plt.ylabel('body_mass_g (standardized)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ac6af1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Let's load up the truth and compare. We only keep those samples that were not removed because of missing values (see [`pandas.DataFrame.iloc`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837ac103",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_truth = pd.read_csv('penguins-species.csv') \n",
    "df_truth = df_truth.iloc[df.index]\n",
    "df_truth.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8d6660",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The species are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8efb8c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "species = df_truth['species']\n",
    "print(species.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c36d4b9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To plot the outcome, we color the species blue-green-red using a [dictionary](https://docs.python.org/3/tutorial/datastructures.html#dictionaries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e3d865",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "species2color = {'Adelie': 'b', 'Chinstrap': 'g', 'Gentoo': 'r'}\n",
    "truth = species.replace(species2color)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e2d6b1",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Finally, we can compare the output to the truth. The match is quite excellent -- but not perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b886e87",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "ax1.scatter(X[:,0], X[:,3], c=truth, s=3)\n",
    "ax1.set_title('truth')\n",
    "ax2.scatter(X[:,0], X[:,3], c=assign, s=3)\n",
    "ax2.set_title('kmeans')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f75098",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**TRY IT!** Run the analysis again, but this time *without the standardization step*. What do you observe? Is one feature more influential than the others on the final output? Why do you think that is?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d45a053",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c37728",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Some observations about high-dimensional data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce930dec",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The following function generates $n$ data points from two spherical $d$-dimensional Gaussians with variance $1$, one with mean $-w\\mathbf{e}_1$ and one with mean $w \\mathbf{e}_1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6909fa8f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def one_cluster(d, n, w):\n",
    "    X = np.stack(\n",
    "        [np.concatenate(([w], np.zeros(d-1))) + rng.normal(0,1,d) for _ in range(n)]\n",
    "    )\n",
    "    return X\n",
    "\n",
    "def two_clusters(d, n, w):\n",
    "    X1 = one_cluster(d, n, -w)\n",
    "    X2 = one_cluster(d, n, w)\n",
    "    return X1, X2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c77164",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We will mix these two datasets to form an interesting case for clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909b7e5c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**Two dimensions** We start with $d=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f72c79",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "d, n, w = 2, 100, 3.\n",
    "X1, X2 = two_clusters(d, n, w)\n",
    "X = np.concatenate((X1, X2), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc98795e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We use a scatterplot to vizualize the data. Each dot corresponds to one data point. Observe the two clearly delineated clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5ba599",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111,aspect='equal')\n",
    "ax.scatter(X[:,0], X[:,1], s=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c702f013",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Let's run $k$-means on this dataset using $k=2$. We use `kmeans()` from the `mmids.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3239af1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "assign = mmids.kmeans(X, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630d27fa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Our default of $10$ iterations seem to have been enough for the algorithm to converge. We can visualize the result by [coloring](https://matplotlib.org/stable/api/_as_gen/matplotlib.lines.Line2D.html) the points according to the assignment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39579ae3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111,aspect='equal')\n",
    "ax.scatter(X[:,0], X[:,1], c=assign, s=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08313289",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**General dimension** Let's see what happens in higher dimension. We repeat our experiment with $d=1000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788d1478",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "outputs": [],
   "source": [
    "d, n, w = 1000, 100, 3.\n",
    "X1, X2 = two_clusters(d, n, w)\n",
    "X = np.concatenate((X1, X2), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418e9577",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Again, we observe two clearly delineated clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b06ed05",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111,aspect='equal')\n",
    "ax.scatter(X[:,0], X[:,1], s=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae73c1d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "This dataset is in $1000$ dimensions, but we've plotted the data in only the first two dimensions. If we plot in any two dimensions not including the first one instead, we see only one cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c572a3a3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111,aspect='equal')\n",
    "ax.scatter(X[:,1], X[:,2], s=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84d147e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Let's see how $k$-means fares on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a0abfb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "assign = mmids.kmeans(X, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d122b3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Our attempt at clustering does not appear to have been successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9272111",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111,aspect='equal')\n",
    "ax.scatter(X[:,0], X[:,1], c=assign, s=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f6e577",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Our attempt at clustering does not appear to have been successful. What happened? While these clusters are easy to tease apart *if we know to look at the first coordinate only*, in the full space the within-cluster and between-cluster distances become harder to distinguish: the noise overwhelms the signal. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1310c9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The function below plots the histograms of within-cluster and between-cluster distances for a sample of size $n$ in $d$ dimensions with a given offset. As $d$ increases, the two distributions become increasingly indistinguishable. Later in the course, we will develop dimension-reduction techniques that help deal with this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c03577",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def highdim_2clusters(d, n, w):\n",
    "    # generate datasets\n",
    "    X1, X2 = two_clusters(d, n, w)\n",
    "    \n",
    "    # within-cluster distances for X1\n",
    "    intra = np.stack(\n",
    "        [LA.norm(X1[i,:] - X1[j,:]) for i in range(n) for j in range(n) if j>i]\n",
    "    )\n",
    "    plt.hist(intra, density=True, label='within-cluster')\n",
    "    plt.title(f'dim={d}')\n",
    " \n",
    "    # between-cluster distances\n",
    "    inter = np.stack(\n",
    "        [LA.norm(X1[i,:] - X2[j,:]) for i in range(n) for j in range(n)]\n",
    "    )\n",
    "    plt.hist(inter, density=True, alpha=0.75, label='between-cluster')\n",
    "\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(f'dim={d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d6e6d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Next we plot the results for dimensions $d=2, 100, 1000$. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335b91c5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "highdim_2clusters(2, 100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbc9839",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "highdim_2clusters(100, 100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd11971f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "highdim_2clusters(1000, 100, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a78866",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "As the dimension increases, the distributions of intra-cluster and inter-cluster distances overlap significantly and become more or less indistinguishable. That provides some insights into why clustering may fail here. Note that we used the same offset for all simulations. On the other hand, if the separation between the clusters is sufficiently large, one would expect clustering to work even in high dimension. \n",
    "\n",
    "**TRY IT!** What precedes (and what follows in the next subsection) is not a formal proof that $k$-means clustering will be unsuccessful here. The behavior of the algorithm is quite complex and depends, in particular, on the initialization and the density of points. Here, increasing the number of data points eventually leads to a much better performance. Explore this behavior on your own by modifying the code. (For some theoretical justifications (beyond this course), see [here](https://arxiv.org/pdf/0912.0086.pdf) and [here](http://www.stat.yale.edu/~pollard/Papers/Pollard81AS.pdf).)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212f0dd5",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We can check the theorem in a simulation. Here we pick $n$ points uniformly at random in the $d$-cube $\\mathcal{C}$, for a range of dimensions up to `dmax`. We then plot the frequency of landing in the inscribed $d$-ball $\\mathcal{B}$ and see that it rapidly converges to $0$. Alternatively, we could just plot the formula for the volume of $\\mathcal{B}$. But knowing how to do simulations is useful in situations where explicit formulas are unavailable or intractable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352a1365",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def highdim_cube(dmax, n):\n",
    "    \n",
    "    in_ball = np.zeros(dmax)\n",
    "    for d in range(dmax):\n",
    "        # recall that d starts at 0 so we add 1 below\n",
    "        in_ball[d] = np.mean(\n",
    "            [(LA.norm(rng.random(d+1) - 1/2) < 1/2) for _ in range(n)]\n",
    "        )\n",
    "    \n",
    "    plt.plot(np.arange(1,dmax+1), in_ball) \n",
    "    plt.xlabel('dim')\n",
    "    plt.ylabel('in-ball freq')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7663542",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We plot the result up to dimension $10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13994b51",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "highdim_cube(10, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd9df61",
   "metadata": {
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7979ac2c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We check our claim in a simulation. We generate standard Normal $d$-vectors using the `rng.normal(0,1,d)` function and plot the histogram of their $2$-norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebbc58c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def normal_shell(d, n):\n",
    "    one_sample_norm = [LA.norm(rng.normal(0,1,d)) for _ in range(n)]\n",
    "    plt.hist(one_sample_norm, bins=20)\n",
    "    plt.xlim=(0,np.stack(one_sample_norm).max())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa09ef81",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We first plot it in one dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f861a6",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "normal_shell(1, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b972f8c6",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "In higher dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6ad861",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "normal_shell(100, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98422c02",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
