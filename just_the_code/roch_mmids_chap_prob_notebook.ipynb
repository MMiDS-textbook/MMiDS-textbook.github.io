{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46ada739",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell",
     "colab-keep"
    ]
   },
   "source": [
    "***\n",
    "\n",
    "*Course:* [Math 535](https://people.math.wisc.edu/~roch/mmids/) - Mathematical Methods in Data Science (MMiDS)  \n",
    "*Chapter:* 7-Probabilistic models   \n",
    "*Author:* [Sebastien Roch](https://people.math.wisc.edu/~roch/), Department of Mathematics, University of Wisconsin-Madison  \n",
    "*Updated:* Jan 8, 2024   \n",
    "*Copyright:* &copy; 2024 Sebastien Roch\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a034d13b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# IF RUNNING ON GOOGLE COLAB, UNCOMMENT THE FOLLOWING CODE CELL\n",
    "# When prompted, upload: \n",
    "#     * mmids.py\n",
    "# from your local file system\n",
    "# Files at: https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/tree/main/utils\n",
    "# Alternative instructions: https://colab.research.google.com/notebooks/io.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad09a0c7",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell",
     "colab-uncomment"
    ]
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53001e9a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# PYTHON 3\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "from numpy.random import default_rng\n",
    "rng = default_rng(535)\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import mmids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d47ab2a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Motivating example: location tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a45836",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Suppose we let loose a cyborg corgi in a large park. We would like to know where it is at all time. For this purpose, it has an implanted location device that sends a signal to a tracking app.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1832ea",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Here is an example of the data we might have. The red dots are recorded locations at reguler time intervals. The dotted line helps keep track of the time order of the recordings. (We will explain later how this dataset is generated.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06367114",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "ss = 4\n",
    "os = 2\n",
    "F = np.array([[1., 0., 1., 0.],[0., 1., 0., 1.],[0., 0., 1., 0.],[0., 0., 0., 1.]]) \n",
    "H = np.array([[1., 0., 0., 0.],[0., 1, 0., 0.]])\n",
    "Q = 0.1 * np.diag(np.ones(ss))\n",
    "R = 10 * np.diag(np.ones(os))\n",
    "x_0 = np.array([0., 0., 1., 1.])\n",
    "T = 50\n",
    "x, y = mmids.lgSamplePath(ss, os, F, H, Q, R, x_0, T)\n",
    "plt.plot(y[0,:], y[1,:], marker='o', c='r', linestyle='dotted')\n",
    "plt.xlim((np.min(y[0,:])-5, np.max(y[0,:])+5)) \n",
    "plt.ylim((np.min(y[1,:])-5, np.max(y[1,:])+5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6ec3eb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "By convention, we start at $(0,0)$. Notice how squiggly the trajectory is. One issue might be that the times  at which the location is recorded are too far between. But, in fact, there is another issue: the tracking device is *inaccurate*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6a7397",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To get a better estimate of the true trajectory, it is natural to try to model the noise in the measurement as well as the dynamics itself. Probabilistic models are perfectly suite for this. \n",
    "\n",
    "In this chapter, we will encounter of variety of such models and show how to take advantage of them to estimate unknown states (or parameters). In particular, conditional independence will play a key role.\n",
    "\n",
    "We will come back to location tracking later in the chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e704110",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\newcommand{\\P}{\\mathbb{P}}$\n",
    "$\\newcommand{\\E}{\\mathbb{E}}$\n",
    "$\\newcommand{\\S}{\\mathcal{S}}$\n",
    "$\\newcommand{\\X}{\\mathcal{X}}$\n",
    "$\\newcommand{\\var}{\\mathrm{Var}}$\n",
    "$\\newcommand{\\btheta}{\\boldsymbol{\\theta}}$\n",
    "$\\newcommand{\\bbeta}{\\boldsymbol{\\beta}}$\n",
    "$\\newcommand{\\bphi}{\\boldsymbol{\\phi}}$\n",
    "$\\newcommand{\\bpi}{\\boldsymbol{\\pi}}$\n",
    "$\\newcommand{\\bmu}{\\boldsymbol{\\mu}}$\n",
    "$\\newcommand{\\bSigma}{\\boldsymbol{\\Sigma}}$\n",
    "$\\newcommand{\\balpha}{\\boldsymbol{\\alpha}}$\n",
    "$\\newcommand{\\indep}{\\perp\\!\\!\\!\\perp}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c885ff08",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Background: introduction to parametric families, generalized linear models and maximum likelihood estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84b10c4",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER** In Numpy, the module [numpy.random](https://numpy.org/doc/stable/reference/random/index.html) provides a way to sample from a variety of standard distributions. We first initialize the [pseudorandom number generator](https://en.wikipedia.org/wiki/Pseudorandom_number_generator) with a [random seed](https://en.wikipedia.org/wiki/Random_seed). In particular it allows the results to be reproducible: using the same seed produces the same results again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c32ca6",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "seed = 535\n",
    "rng = np.random.default_rng(535)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4e3735",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We then set the distribution and its parameters. Here's are lists of available [probability distributions](https://numpy.org/doc/stable/reference/random/generator.html#distributions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fcc530",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "p = 0.1 # probability of success\n",
    "N = 5 # number of samples\n",
    "rng.binomial(1,p,size=N) # Bernoulli is special case of binomial with 1 trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7b0397",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Here are a few other examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebee4e4b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "p = [0.1, 0.2, 0.7]\n",
    "n = 100\n",
    "rng.multinomial(n,p,size=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e426e21",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "mu = np.array([0.1, -0.3])\n",
    "sig = np.array([[2., 0.],[0., 3.]])\n",
    "rng.multivariate_normal(mu,sig,size=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546a591c",
   "metadata": {
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4dfd61",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\newcommand{\\P}{\\mathbb{P}}$\n",
    "$\\newcommand{\\E}{\\mathbb{E}}$\n",
    "$\\newcommand{\\S}{\\mathcal{S}}$\n",
    "$\\newcommand{\\var}{\\mathrm{Var}}$\n",
    "$\\newcommand{\\bmu}{\\boldsymbol{\\mu}}$\n",
    "$\\newcommand{\\bSigma}{\\boldsymbol{\\Sigma}}$\n",
    "$\\newcommand{\\btheta}{\\boldsymbol{\\theta}}$\n",
    "$\\newcommand{\\bpi}{\\boldsymbol{\\pi}}$\n",
    "$\\newcommand{\\indep}{\\perp\\!\\!\\!\\perp}$\n",
    "$\\newcommand{\\bp}{\\mathbf{p}}$\n",
    "$\\newcommand{\\bx}{\\mathbf{x}}$\n",
    "$\\newcommand{\\bX}{\\mathbf{X}}$\n",
    "$\\newcommand{\\by}{\\mathbf{y}}$\n",
    "$\\newcommand{\\bY}{\\mathbf{Y}}$\n",
    "$\\newcommand{\\bz}{\\mathbf{z}}$\n",
    "$\\newcommand{\\bZ}{\\mathbf{Z}}$\n",
    "$\\newcommand{\\bw}{\\mathbf{w}}$\n",
    "$\\newcommand{\\bW}{\\mathbf{W}}$\n",
    "$\\newcommand{\\bv}{\\mathbf{v}}$\n",
    "$\\newcommand{\\bV}{\\mathbf{V}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf46c87",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Linear-Gaussian models and Kalman filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851e605f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**Implementing the Kalman filter** We implement the Kalman filter as described above with known covariance matrices. We take $\\Delta = 1$ for simplicity. The code is adapted from [[Mur](https://github.com/probml)]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6f1794",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We will test Kalman filtering on a simulated path drawn from the linear-Gaussian model above. The following function creates such a path and its noisy observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b58693a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "seed = 535\n",
    "rng = np.random.default_rng(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a637fb1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def lgSamplePath(ss, os, F, H, Q, R, x_0, T):\n",
    "    x = np.zeros((ss,T)) \n",
    "    y = np.zeros((os,T))\n",
    "    x[:,0] = x_0\n",
    "    ey = np.zeros(os)\n",
    "    ey = rng.multivariate_normal(np.zeros(os),R) \n",
    "    y[:,0] = H @ x[:,0] + ey\n",
    "    \n",
    "    for t in range(1,T):\n",
    "        ex = np.zeros(ss)\n",
    "        ex = rng.multivariate_normal(np.zeros(ss),Q) # noise on x_t\n",
    "        x[:,t] = F @ x[:,t-1] + ex\n",
    "        ey = np.zeros(os)\n",
    "        ey = rng.multivariate_normal(np.zeros(os),R) # noise on y_t\n",
    "        y[:,t] = H @ x[:,t] + ey\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867320cd",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Here is an example. Here $\\bSigma$ is denoted as $V$. In the plot, the blue crosses are the unobserved true path and the orange dots are the noisy observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfa5fae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ss = 4 # state size\n",
    "os = 2 # observation size\n",
    "F = np.array([[1., 0., 1., 0.],[0., 1., 0., 1.],[0., 0., 1., 0.],[0., 0., 0., 1.]]) \n",
    "H = np.array([[1., 0., 0., 0.],[0., 1, 0., 0.]])\n",
    "Q = 0.1 * np.diag(np.ones(ss))\n",
    "R = 10 * np.diag(np.ones(os))\n",
    "x_0 = np.array([0., 0., 1., 1.]) # initial state\n",
    "T = 50\n",
    "x, y = lgSamplePath(ss, os, F, H, Q, R, x_0, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc76386e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.plot(y[0,:], y[1,:], marker='o', c='r', linestyle='dotted')\n",
    "plt.xlim((np.min(y[0,:])-5, np.max(y[0,:])+5)) \n",
    "plt.ylim((np.min(y[1,:])-5, np.max(y[1,:])+5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb55d339",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.plot(x[0,:], x[1,:], marker='x', c='g', linestyle='dashed', alpha=0.5)\n",
    "plt.xlim((np.min(x[0,:])-5, np.max(x[0,:])+5)) \n",
    "plt.ylim((np.min(x[1,:])-5, np.max(x[1,:])+5))\n",
    "plt.scatter(y[0,:], y[1,:], c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bac11e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The following function implements the Kalman filter. Here $A$ is $F$ and $C$ is $H$. The full recursion is broken up into several steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abe9ce8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def kalmanUpdate(ss, A, C, Q, R, y_t, mu_prev, Sig_prev):\n",
    "    mu_pred = A @ mu_prev\n",
    "    Sig_pred = A @ Sig_prev @ A.T + Q\n",
    "    e_t = y_t - C @ mu_pred # error at time t\n",
    "    S = C @ Sig_pred @ C.T + R\n",
    "    Sinv = LA.inv(S)\n",
    "    K = Sig_pred @ C.T @ Sinv # Kalman gain matrix\n",
    "    mu_new = mu_pred + K @ e_t\n",
    "    Sig_new = (np.diag(np.ones(ss)) - K @ C) @ Sig_pred\n",
    "    return mu_new, Sig_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4414960d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def kalmanFilter(ss, os, y, A, C, Q, R, init_mu, init_Sig, T):\n",
    "    mu = np.zeros((ss, T))\n",
    "    Sig = np.zeros((ss, ss, T))\n",
    "    mu[:,0] = init_mu\n",
    "    Sig[:,:,0] = init_Sig\n",
    "\n",
    "    for t in range(1,T):\n",
    "        mu[:,t], Sig[:,:,t] = kalmanUpdate(ss, A, C, Q, R, y[:,t], mu[:,t-1], Sig[:,:,t-1])\n",
    "\n",
    "    return mu, Sig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e94625",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We apply this to the example above. The inferred unobserved states are in green."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6777cb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "init_mu = x_0\n",
    "init_Sig = 1 * np.diag(np.ones(ss))\n",
    "mu, Sig = kalmanFilter(ss, os, y, F, H, Q, R, init_mu, init_Sig, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3320d0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.plot(x[0,:], x[1,:], marker='x', c='g', linestyle='dashed', alpha=0.5)\n",
    "plt.xlim((np.min(x[0,:])-5, np.max(x[0,:])+5)) \n",
    "plt.ylim((np.min(x[1,:])-5, np.max(x[1,:])+5))\n",
    "plt.scatter(y[0,:], y[1,:], c='r')\n",
    "plt.plot(mu[0,:], mu[1,:], marker='s', linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1f88db",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To quantify the improvement in the inference compared to the observations, we compute the mean squared error in both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c81f67b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dobs = x[0:1,:] - y[0:1,:]\n",
    "mse_obs = np.sqrt(np.sum(dobs**2))\n",
    "print(mse_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36475fc4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dfilt = x[0:1,:] - mu[0:1,:]\n",
    "mse_filt = np.sqrt(np.sum(dfilt**2))\n",
    "print(mse_filt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6ef857",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We indeed observe a reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067f9994",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**Missing data** We can also allow for the possibility that some observations are missing. Imagine for instance losing GPS signal while going through a tunnel. The recursions above are still valid, with the only modification that the $\\bY_t$ and $H$ terms are dropped at those times $t$ where there is no observation. In Numpy, we can use [`NaN`](https://numpy.org/doc/stable/reference/constants.html#numpy.nan). (Alternatively, one can use the [numpy.ma](https://numpy.org/doc/stable/reference/maskedarray.generic.html) module.) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9875a70",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We use a same sample path as above, but mask observations at times $t=10,\\ldots,20$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab2dff7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ss = 4\n",
    "os = 2\n",
    "F = np.array([[1., 0., 1., 0.],[0., 1., 0., 1.],[0., 0., 1., 0.],[0., 0., 0., 1.]]) \n",
    "H = np.array([[1., 0., 0., 0.],[0., 1, 0., 0.]])\n",
    "Q = 0.01 * np.diag(np.ones(ss))\n",
    "R = 10 * np.diag(np.ones(os))\n",
    "x_0 = np.array([0., 0., 1., 1.])\n",
    "T = 30\n",
    "x, y = lgSamplePath(ss, os, F, H, Q, R, x_0, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b1d54f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(10,20):\n",
    "    y[0,i] = np.nan\n",
    "    y[1,i] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cbdf81",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Here is the sample we are aiming to infer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d441ead2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.plot(x[0,:], x[1,:], marker='x', c='g', linestyle='dashed', alpha=0.5)\n",
    "plt.xlim((np.min(x[0,:])-5, np.max(x[0,:])+5)) \n",
    "plt.ylim((np.min(x[1,:])-5, np.max(x[1,:])+5))\n",
    "plt.scatter(y[0,:], y[1,:], c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084a2204",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We modify the recursion accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca1c00a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def kalmanUpdate(ss, A, C, Q, R, y_t, mu_prev, Sig_prev):\n",
    "    mu_pred = A @ mu_prev\n",
    "    Sig_pred = A @ Sig_prev @ A.T + Q\n",
    "    if np.isnan(y_t[0]) or np.isnan(y_t[1]):\n",
    "        return mu_pred, Sig_pred\n",
    "    else:\n",
    "        e_t = y_t - C @ mu_pred # error at time t\n",
    "        S = C @ Sig_pred @ C.T + R\n",
    "        Sinv = LA.inv(S)\n",
    "        K = Sig_pred @ C.T @ Sinv # Kalman gain matrix\n",
    "        mu_new = mu_pred + K @ e_t\n",
    "        Sig_new = (np.diag(np.ones(ss)) - K @ C) @ Sig_pred\n",
    "        return mu_new, Sig_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93e4a3d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "init_mu = x_0\n",
    "init_Sig = 1 * np.diag(np.ones(ss))\n",
    "mu, Sig = kalmanFilter(ss, os, y, F, H, Q, R, init_mu, init_Sig, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe32e5d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.plot(x[0,:], x[1,:], marker='x', c='g', alpha=0.5)\n",
    "plt.xlim((np.min(x[0,:])-5, np.max(x[0,:])+5)) \n",
    "plt.ylim((np.min(x[1,:])-5, np.max(x[1,:])+5))\n",
    "plt.scatter(y[0,:], y[1,:], c='r', alpha=0.5)\n",
    "plt.plot(mu[0,:], mu[1,:], marker='s', linewidth=2)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
