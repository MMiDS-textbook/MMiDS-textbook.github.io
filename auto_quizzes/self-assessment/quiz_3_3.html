<!DOCTYPE html>
<html>
<head>
    <title>Self-Assessment Questions for Section 3.3</title>
    <style>
        .question {
            margin-bottom: 20px;
        }
        .options {
            list-style-type: none;
            padding: 0;
        }
        .options li {
            margin-bottom: 10px;
        }
        .feedback {
            margin-top: 10px;
            font-weight: bold;
        }
    </style>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <h1>Self-Assessment Questions for Section 3.3</h1>

    <p>Before moving on, you may wish to quickly test your knowledge by going over the following review questions.</p>

    <div id="quiz-container">
        <div class="question">
            <p>Q3.3.1. Which of the following is the correct definition of a global minimizer \( x^* \) of a function \( f: \mathbb{R}^d \to \mathbb{R} \)?</p>
            <ul class="options">
                <li><label><input type="radio" name="q0" value="0"> a) \( f(x) \geq f(x^*) \) for all \( x \) in some open ball around \( x^* \).</label></li>
                <li><label><input type="radio" name="q0" value="1"> b) \( f(x) \geq f(x^*) \) for all \( x \in \mathbb{R}^d \).</label></li>
                <li><label><input type="radio" name="q0" value="2"> c) \( \nabla f(x^*) = 0 \).</label></li>
                <li><label><input type="radio" name="q0" value="3"> d) \( v^T H_f(x^*) v > 0 \) for all \( v \in \mathbb{R}^d \).</label></li>
            </ul>
            <div class="feedback" id="feedback0"></div>
        </div>

        <div class="question">
            <p>Q3.3.2. Which of the following statements about descent directions is true?</p>
            <ul class="options">
                <li><label><input type="radio" name="q1" value="0"> a) A descent direction for a function \( f \) at a point \( x_0 \) is any direction in which \( f \) increases.</label></li>
                <li><label><input type="radio" name="q1" value="1"> b) A descent direction for a function \( f \) at a point \( x_0 \) is any direction in which \( f \) does not change.</label></li>
                <li><label><input type="radio" name="q1" value="2"> c) A descent direction for a function \( f \) at a point \( x_0 \) is any direction in which \( f \) decreases.</label></li>
                <li><label><input type="radio" name="q1" value="3"> d) A descent direction for a function \( f \) at a point \( x_0 \) is always the direction of steepest descent.</label></li>
            </ul>
            <div class="feedback" id="feedback1"></div>
        </div>

        <div class="question">
            <p>Q3.3.3. Which of the following is the correct definition of a local minimizer \( x^* \) of a function \( f: \mathbb{R}^d \to \mathbb{R} \)?</p>
            <ul class="options">
                <li><label><input type="radio" name="q2" value="0"> a) \( f(x^*) \leq f(x) \) for all \( x \in \mathbb{R}^d \).</label></li>
                <li><label><input type="radio" name="q2" value="1"> b) \( f(x^*) < f(x) \) for all \( x \in \mathbb{R}^d \).</label></li>
                <li><label><input type="radio" name="q2" value="2"> c) There exists \( \delta > 0 \) such that \( f(x^*) \leq f(x) \) for all \( x \in B_\delta(x^*) \setminus \{x^*\} \).</label></li>
                <li><label><input type="radio" name="q2" value="3"> d) There exists \( \delta > 0 \) such that \( f(x^*) < f(x) \) for all \( x \in B_\delta(x^*) \setminus \{x^*\} \).</label></li>
            </ul>
            <div class="feedback" id="feedback2"></div>
        </div>

        <div class="question">
            <p>Q3.3.4. Which of the following statements about stationary points is true?</p>
            <ul class="options">
                <li><label><input type="radio" name="q3" value="0"> a) All stationary points are local minimizers.</label></li>
                <li><label><input type="radio" name="q3" value="1"> b) All local minimizers are stationary points.</label></li>
                <li><label><input type="radio" name="q3" value="2"> c) A stationary point is a point where the gradient of the function is nonzero.</label></li>
                <li><label><input type="radio" name="q3" value="3"> d) A stationary point is always a saddle point.</label></li>
            </ul>
            <div class="feedback" id="feedback3"></div>
        </div>

        <div class="question">
            <p>Q3.3.5. Which of the following is a necessary condition for a point \( x^* \) to be a local minimizer of a continuously differentiable function \( f: \mathbb{R}^d \to \mathbb{R} \)?</p>
            <ul class="options">
                <li><label><input type="radio" name="q4" value="0"> a) \( \nabla f(x^*) \neq 0 \).</label></li>
                <li><label><input type="radio" name="q4" value="1"> b) \( \nabla f(x^*) = 0 \).</label></li>
                <li><label><input type="radio" name="q4" value="2"> c) \( H_f(x^*) \) is positive definite.</label></li>
                <li><label><input type="radio" name="q4" value="3"> d) \( H_f(x^*) \) is negative definite.</label></li>
            </ul>
            <div class="feedback" id="feedback4"></div>
        </div>

        <div class="question">
            <p>Q3.3.6. Consider a continuously differentiable function \( f: \mathbb{R}^d \to \mathbb{R} \). If \( v \in \mathbb{R}^d \) is a descent direction for \( f \) at \( x_0 \), then which of the following is true?</p>
            <ul class="options">
                <li><label><input type="radio" name="q5" value="0"> a) \( \nabla f(x_0)^T v > 0 \).</label></li>
                <li><label><input type="radio" name="q5" value="1"> b) \( \nabla f(x_0)^T v = 0 \).</label></li>
                <li><label><input type="radio" name="q5" value="2"> c) \( \nabla f(x_0)^T v < 0 \).</label></li>
                <li><label><input type="radio" name="q5" value="3"> d) None of the above.</label></li>
            </ul>
            <div class="feedback" id="feedback5"></div>
        </div>

        <div class="question">
            <p>Q3.3.7. Let \( f: \mathbb{R}^d \to \mathbb{R} \) be continuously differentiable at \( x_0 \). The directional derivative of \( f \) at \( x_0 \) in the direction \( v \in \mathbb{R}^d \) is NOT given by:</p>
            <ul class="options">
                <li><label><input type="radio" name="q6" value="0"> a) \( \frac{\partial f(x_0)}{\partial v} = \nabla f(x_0)^T v \).</label></li>
                <li><label><input type="radio" name="q6" value="1"> b) \( \frac{\partial f(x_0)}{\partial v} = v^T \nabla f(x_0) \).</label></li>
                <li><label><input type="radio" name="q6" value="2"> c) \( \frac{\partial f(x_0)}{\partial v} = v^T H_f(x_0) v \).</label></li>
                <li><label><input type="radio" name="q6" value="3"> d) \( \frac{\partial f(x_0)}{\partial v} = \lim_{h \to 0} \frac{f(x_0 + hv) - f(x_0)}{h} \).</label></li>
            </ul>
            <div class="feedback" id="feedback6"></div>
        </div>

        <div class="question">
            <p>Q3.3.8. Let \( f: \mathbb{R}^d \to \mathbb{R} \) be twice continuously differentiable at \( x_0 \). The second directional derivative of \( f \) at \( x_0 \) in the direction \( v \in \mathbb{R}^d \) is given by:</p>
            <ul class="options">
                <li><label><input type="radio" name="q7" value="0"> a) \( \frac{\partial^2 f(x_0)}{\partial v^2} = \lim_{h \to 0} \frac{\nabla f(x_0 + hv) - \nabla f(x_0)}{h} \).</label></li>
                <li><label><input type="radio" name="q7" value="1"> b) \( \frac{\partial^2 f(x_0)}{\partial v^2} = \lim_{h \to 0} \frac{1}{h} [\frac{\partial f(x_0 + hv)}{\partial v} - \frac{\partial f(x_0)}{\partial v}] \).</label></li>
                <li><label><input type="radio" name="q7" value="2"> c) \( \frac{\partial^2 f(x_0)}{\partial v^2} = v^T H_f(x_0) v \).</label></li>
                <li><label><input type="radio" name="q7" value="3"> d) Both b and c.</label></li>
            </ul>
            <div class="feedback" id="feedback7"></div>
        </div>

        <div class="question">
            <p>Q3.3.9. Let \( f: \mathbb{R}^d \to \mathbb{R} \) be twice continuously differentiable. If \( \nabla f(x_0) = 0 \) and \( H_f(x_0) \) is positive definite, then \( x_0 \) is:</p>
            <ul class="options">
                <li><label><input type="radio" name="q8" value="0"> a) A global minimizer of \( f \).</label></li>
                <li><label><input type="radio" name="q8" value="1"> b) A local minimizer of \( f \), but not necessarily a strict local minimizer.</label></li>
                <li><label><input type="radio" name="q8" value="2"> c) A strict local minimizer of \( f \).</label></li>
                <li><label><input type="radio" name="q8" value="3"> d) A saddle point of \( f \).</label></li>
            </ul>
            <div class="feedback" id="feedback8"></div>
        </div>

        <div class="question">
            <p>Q3.3.10. What is the Lagrangian function used for in constrained optimization?</p>
            <ul class="options">
                <li><label><input type="radio" name="q9" value="0"> a) It transforms a constrained optimization problem into an unconstrained one.</label></li>
                <li><label><input type="radio" name="q9" value="1"> b) It provides a way to find the feasible set of the constrained problem.</label></li>
                <li><label><input type="radio" name="q9" value="2"> c) It directly gives the optimal solution to the constrained problem.</label></li>
                <li><label><input type="radio" name="q9" value="3"> d) It is used to calculate the gradient of the objective function.</label></li>
            </ul>
            <div class="feedback" id="feedback9"></div>
        </div>

        <div class="question">
            <p>Q3.3.11. In the method of Lagrange multipliers, what is the geometric interpretation of the condition \( \nabla f(x^*) + \sum_{i=1}^l \lambda_i^* \nabla h_i(x^*) = 0 \)?</p>
            <ul class="options">
                <li><label><input type="radio" name="q10" value="0"> a) The gradient of the objective function is parallel to the gradient of each constraint function.</label></li>
                <li><label><input type="radio" name="q10" value="1"> b) The gradient of the objective function is orthogonal to the subspace of first-order feasible directions at \( x^* \).</label></li>
                <li><label><input type="radio" name="q10" value="2"> c) The gradient of the objective function is zero at \( x^* \).</label></li>
                <li><label><input type="radio" name="q10" value="3"> d) The Lagrange multipliers are all equal to zero.</label></li>
            </ul>
            <div class="feedback" id="feedback10"></div>
        </div>

        <div class="question">
            <p>Q3.3.12. Consider the optimization problem \( \min_x f(x) \) subject to \( h(x) = 0 \), where \( f: \mathbb{R}^d \to \mathbb{R} \) and \( h: \mathbb{R}^d \to \mathbb{R}^\ell \) are continuously differentiable. Let \( x^* \) be a local minimizer and assume that the vectors \( \nabla h_i(x^*), i \in [\ell] \), are linearly independent. According to the Lagrange Multipliers theorem, which of the following must be true?</p>
            <ul class="options">
                <li><label><input type="radio" name="q11" value="0"> a) \( \nabla f(x^*) = 0 \).</label></li>
                <li><label><input type="radio" name="q11" value="1"> b) \( \nabla f(x^*) + \sum_{i=1}^\ell \lambda_i^* \nabla h_i(x^*) = 0 \) for some \( \lambda^* \in \mathbb{R}^\ell \).</label></li>
                <li><label><input type="radio" name="q11" value="2"> c) \( h(x^*) = 0 \).</label></li>
                <li><label><input type="radio" name="q11" value="3"> d) Both b and c.</label></li>
            </ul>
            <div class="feedback" id="feedback11"></div>
        </div>

        <div class="question">
            <p>Q3.3.13. Consider the optimization problem \( \min_x f(x) \) subject to \( h(x) = 0 \), where \( f: \mathbb{R}^d \to \mathbb{R} \) and \( h: \mathbb{R}^d \to \mathbb{R}^\ell \) are continuously differentiable. Let \( x^* \) be a local minimizer satisfying the first-order necessary conditions with Lagrange multipliers \( \lambda^* \). The subspace of first-order feasible directions at \( x^* \) is defined as:</p>
            <ul class="options">
                <li><label><input type="radio" name="q12" value="0"> a) \( F_h(x^*) = \{v \in \mathbb{R}^d: \nabla h_i(x^*)^T v = 0, \forall i \in [\ell]\} \).</label></li>
                <li><label><input type="radio" name="q12" value="1"> b) \( F_h(x^*) = \{v \in \mathbb{R}^d: \nabla h_i(x^*)^T v > 0, \forall i \in [\ell]\} \).</label></li>
                <li><label><input type="radio" name="q12" value="2"> c) \( F_h(x^*) = \{v \in \mathbb{R}^d: \nabla f(x^*)^T v = 0\} \).</label></li>
                <li><label><input type="radio" name="q12" value="3"> d) \( F_h(x^*) = \{v \in \mathbb{R}^d: \nabla f(x^*)^T v + \sum_{i=1}^\ell \lambda_i^* \nabla h_i(x^*)^T v = 0\} \).</label></li>
            </ul>
            <div class="feedback" id="feedback12"></div>
        </div>

        <div class="question">
            <p>Q3.3.14. Which of the following is a correct statement of Taylor's Theorem (to second order) for a twice continuously differentiable function \( f: D \to \mathbb{R} \), where \( D \subseteq \mathbb{R}^d \), at an interior point \( x_0 \in D \)?</p>
            <ul class="options">
                <li><label><input type="radio" name="q13" value="0"> a) For any \( x \in B_\delta(x_0) \), \( f(x) = f(x_0) + \nabla f(x_0)^T (x - x_0) + \frac{1}{2}(x - x_0)^T Hf(x_0 + \xi(x - x_0))(x - x_0) \) for some \( \xi \in (0,1) \).</label></li>
                <li><label><input type="radio" name="q13" value="1"> b) For any \( x \in B_\delta(x_0) \), \( f(x) = f(x_0) + \nabla f(x_0 + \xi(x - x_0))^T (x - x_0) + \frac{1}{2}(x - x_0)^T Hf(x_0 + \xi(x - x_0))(x - x_0) \).</label></li>
                <li><label><input type="radio" name="q13" value="2"> c) For any \( x \in B_\delta(x_0) \), \( f(x) = f(x_0) + \nabla f(x_0 + \xi(x - x_0))^T (x - x_0) \).</label></li>
                <li><label><input type="radio" name="q13" value="3"> d) For any \( x \in B_\delta(x_0) \), \( f(x) = f(x_0) + \frac{1}{2}(x_0 + \xi(x - x_0))^T Hf(x_0)(x_0 + \xi(x - x_0)) \).</label></li>
            </ul>
            <div class="feedback" id="feedback13"></div>
        </div>
    </div>

    <script>
        document.querySelectorAll('.options').forEach((optionsList, index) => {
            const correctOptions = [1, 2, 2, 1, 1, 2, 2, 3, 2, 0, 1, 3, 0, 0];

            optionsList.addEventListener('change', (event) => {
                const selectedOption = parseInt(event.target.value);
                const feedbackDiv = document.getElementById('feedback' + index);
                let feedbackText = '';

                if (selectedOption === correctOptions[index]) {
                    feedbackText = "Excellent! " + getCorrectFeedback(index);
                } else {
                    feedbackText = "Try again.";
                }

                feedbackDiv.innerHTML = feedbackText;
                feedbackDiv.style.color = selectedOption === correctOptions[index] ? 'green' : 'red';

                MathJax.typesetPromise([feedbackDiv]); 
            });
        });
    </script>

    <script>
        function getCorrectFeedback(index) {
            switch (index) {
                case 0:
                    return "The text states that 'The point \\( x^* \\in \\mathbb{R}^d \\) is a global minimizer of \\( f \\) over \\( \\mathbb{R}^d \\) if \\( f(x) \\geq f(x^*), \\forall x \\in \\mathbb{R}^d \\).'";
                case 1:
                    return "The text defines a descent direction as a vector \\( v \\) such that there exists \\( \\alpha^* > 0 \\) where \\( f(x_0 + \\alpha v) < f(x_0) \\) for all \\( \\alpha \\in (0, \\alpha^*) \\). This implies that \\( f \\) decreases in the direction \\( v \\).";
                case 2:
                    return "The text defines a local minimizer as follows: 'The point \\( x^* \\) is a local minimizer of \\( f \\) over \\( \\mathbb{R}^d \\) if there is \\( \\delta > 0 \\) such that \\( f(x) \\geq f(x^*) \\) for all \\( x \\in B_\\delta(x^*) \\setminus \\{x^*\\}.'";
                case 3:
                    return "The text defines a stationary point as a point where the gradient is zero. The first-order necessary conditions state that all local minimizers must be stationary points.";
                case 4:
                    return "The text states the First-Order Necessary Conditions theorem: 'If \\( x_0 \\) is a local minimizer, then \\( \\nabla f(x_0) = 0 \\).'";
                case 5:
                    return "The text defines a descent direction \\( v \\) for \\( f \\) at \\( x_0 \\) as one for which there exists \\( \\alpha^* > 0 \\) such that \\( f(x_0 + \\alpha v) < f(x_0), \\forall \\alpha \\in (0, \\alpha^*) \\). It then proves that this is equivalent to \\( \\nabla f(x_0)^T v < 0 \\).";
                case 6:
                    return "The text states the Directional Derivative from Gradient theorem: 'Assume that \\( f \\) is continuously differentiable at \\( x_0 \\). Then the directional derivative of \\( f \\) at \\( x_0 \\) in the direction \\( v \\) is given by \\( \\frac{\\partial f(x_0)}{\\partial v} = \\nabla f(x_0)^T v \\).'";
                case 7:
                    return "The text defines the second directional derivative as \\( \\frac{\\partial^2 f(x_0)}{\\partial v^2} = \\lim_{h \\to 0} \\frac{1}{h} [\\frac{\\partial f(x_0 + hv)}{\\partial v} - \\frac{\\partial f(x_0)}{\\partial v}] \\) and then proves the Second Directional Derivative from Hessian theorem, which states that \\( \\frac{\\partial^2 f(x_0)}{\\partial v^2} = v^T H_f(x_0) v \\).";
                case 8:
                    return "The text states the Second-Order Sufficient Condition theorem: 'If \\( \\nabla f(x_0) = 0 \\) and \\( H_f(x_0) \\) is positive definite, then \\( x_0 \\) is a strict local minimizer.'";
                case 9:
                    return "The text explains that the Lagrangian function is used to incorporate the constraints into the objective function, effectively converting a constrained problem into an unconstrained one.";
                case 10:
                    return "The text explains that this condition means the gradient of the objective function is orthogonal to the set of first-order feasible directions at \\( x^* \\).";
                case 11:
                    return "The Lagrange Multipliers theorem states that under the given conditions, there exists a unique vector \\( \\lambda^* = (\\lambda_1^*, \\ldots, \\lambda_\\ell^*) \\) satisfying \\( \\nabla f(x^*) + \\sum_{i=1}^\\ell \\lambda_i^* \\nabla h_i(x^*) = 0 \\) and \\( h(x^*) = 0 \\).";
                case 12:
                    return "The text defines the subspace of first-order feasible directions at \\( x \\) as \\( F_h(x) = \\{v \\in \\mathbb{R}^d: \\nabla h_i(x)^T v = 0, \\forall i \\in [\\ell]\\}. \\) It then states that if \\( x^* \\) is a local minimizer, the gradient of \\( f \\) is orthogonal to the set of first-order feasible directions at \\( x^* \\), i.e., \\( \\nabla f(x^*)^T v = -\\sum_{i=1}^\\ell \\lambda_i^* \\nabla h_i(x^*)^T v = 0 \\) for any \\( v \\in F_h(x^*) \\).";
                case 13:
                    return "This is the statement of Taylor\\'s Theorem as presented in the text.";
                default:
                    return "";
            }
        }
    </script>
</body>
</html>
