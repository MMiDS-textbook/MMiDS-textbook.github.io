

<!DOCTYPE html>


<html data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>2.4. Overdetermined linear systems and regression analysis &#8212; MMiDS Online Book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chap02_ls/04_overdetermined/roch-mmids-ls-4overdetermined';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="2.5. QR decomposition and Householder transformations" href="../05_qr/roch-mmids-ls-5qr.html" />
    <link rel="prev" title="2.3. A key concept: orthogonality" href="../03_orthog/roch-mmids-ls-3orthog.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="None"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/mmids-cover.png" class="logo__image only-light" alt="MMiDS Online Book - Home"/>
    <script>document.write(`<img src="../../_static/mmids-cover.png" class="logo__image only-dark" alt="MMiDS Online Book - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">MMiDS</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap01_intro/00_intro/roch-mmids-intro-0intro.html">1. Introduction</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap01_intro/01_motiv/roch-mmids-intro-1motiv.html">1.1. Motivating example: species delimitation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap01_intro/02_review/roch-mmids-intro-2review.html">1.2. Background: review of basic linear algebra, calculus, and probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap01_intro/03_clustering/roch-mmids-intro-3clustering.html">1.3. Clustering: an objective, an algorithm and a guarantee</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap01_intro/04_highdim/roch-mmids-intro-4highdim.html">1.4. Some observations about high-dimensional data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap01_intro/05_adv/roch-mmids-intro-5adv.html">1.5. Advanced material</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../00_intro/roch-mmids-ls-intro.html">2. Least squares</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../01_motiv/roch-mmids-ls-1motiv.html">2.1. Motivating example: predicting sales</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02_subspaces/roch-mmids-ls-2spaces.html">2.2. Background: linear spaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03_orthog/roch-mmids-ls-3orthog.html">2.3. A key concept: orthogonality</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">2.4. Overdetermined linear systems and regression analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../05_qr/roch-mmids-ls-5qr.html">2.5. QR decomposition and Householder transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../06_adv/roch-mmids-ls-6adv.html">2.6. Advanced material</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Overdetermined linear systems and regression analysis</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-systems-and-inverses">2.4.1. Linear systems and inverses</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overdetermined-case">2.4.2. Overdetermined case</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#least-squares-via-cholesky">2.4.3. Least squares via Cholesky</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#back-to-regression-analysis">2.4.4. Back to regression analysis</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <div class="math notranslate nohighlight">
\[
\newcommand{\bfbeta}{\boldsymbol{\beta}}
\newcommand{\bflambda}{\boldsymbol{\lambda}}
\]</div>
<section id="overdetermined-linear-systems-and-regression-analysis">
<h1><span class="section-number">2.4. </span>Overdetermined linear systems and regression analysis<a class="headerlink" href="#overdetermined-linear-systems-and-regression-analysis" title="Permalink to this headline">#</a></h1>
<p>We return to one of our motivating examples, linear regression. At a high level, we are given <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n\times m}\)</span> an <span class="math notranslate nohighlight">\(n\times m\)</span> matrix and <span class="math notranslate nohighlight">\(\mathbf{b} \in \mathbb{R}^n\)</span> a vector. We are looking to solve the system</p>
<div class="math notranslate nohighlight">
\[
A \mathbf{x} \approx \mathbf{b}.
\]</div>
<p>We begin with the important special case where <span class="math notranslate nohighlight">\(A\)</span> is invertible.</p>
<section id="linear-systems-and-inverses">
<h2><span class="section-number">2.4.1. </span>Linear systems and inverses<a class="headerlink" href="#linear-systems-and-inverses" title="Permalink to this headline">#</a></h2>
<p><strong>DEFINITION</strong> <strong>(Nonsingular Matrix)</strong> A square matrix <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times n}\)</span> is nonsingular if it has rank <span class="math notranslate nohighlight">\(n\)</span>. In that case, we also say that <span class="math notranslate nohighlight">\(A\)</span> is of full rank. <span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>An implication of this is that <span class="math notranslate nohighlight">\(A\)</span> is nonsingular if and only if its columns form a basis of <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>. Indeed, suppose the columns of <span class="math notranslate nohighlight">\(A\)</span> form a basis of <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>. Then the dimension of <span class="math notranslate nohighlight">\(\mathrm{col}(A)\)</span> is <span class="math notranslate nohighlight">\(n\)</span>, which means the rank is <span class="math notranslate nohighlight">\(n\)</span>. In the other direction, suppose <span class="math notranslate nohighlight">\(A\)</span> has rank <span class="math notranslate nohighlight">\(n\)</span>.</p>
<ol class="arabic simple">
<li><p>We first prove a general statement: the columns of <span class="math notranslate nohighlight">\(Z \in \mathbb{R}^{k \times m}\)</span> form a basis of <span class="math notranslate nohighlight">\(\mathrm{col}(Z)\)</span> whenever <span class="math notranslate nohighlight">\(Z\)</span> is of full column rank. Indeed, the columns of <span class="math notranslate nohighlight">\(Z\)</span> by definition span <span class="math notranslate nohighlight">\(\mathrm{col}(Z)\)</span>. By the <em>Reducing a Spanning List Lemma</em>, they can be reduced into a basis of <span class="math notranslate nohighlight">\(\mathrm{col}(Z)\)</span>. If <span class="math notranslate nohighlight">\(Z\)</span> has full column rank, then the length of any basis of <span class="math notranslate nohighlight">\(\mathrm{col}(Z)\)</span> is equal to the number of columns of <span class="math notranslate nohighlight">\(Z\)</span>. So the columns of <span class="math notranslate nohighlight">\(Z\)</span> must already form a basis.</p></li>
<li><p>Apply the previous claim to <span class="math notranslate nohighlight">\(Z = A\)</span>. Then, since the columns of <span class="math notranslate nohighlight">\(A\)</span> form an independent list in <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>, by the <em>Completing an Independent List Lemma</em> they can be completed into a basis of <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>. But there are already <span class="math notranslate nohighlight">\(n\)</span> of them, the dimension of <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>, so they must already form a basis of <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>. In other words, we have proved another general fact: an independent list of length <span class="math notranslate nohighlight">\(n\)</span> in <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span> is a basis of <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>.</p></li>
</ol>
<p>Equivalently:</p>
<p><strong>LEMMA</strong> <strong>(Invertibility)</strong> A square matrix <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times n}\)</span> is nonsingular if and only if there exists a unique <span class="math notranslate nohighlight">\(A^{-1}\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
A A^{-1} = A^{-1} A = I_{n \times n}.
\]</div>
<p>The matrix <span class="math notranslate nohighlight">\(A^{-1}\)</span> is referred to as the inverse of <span class="math notranslate nohighlight">\(A\)</span>. We also say that <span class="math notranslate nohighlight">\(A\)</span> is invertible. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof idea:</em> We use the nonsingularity of <span class="math notranslate nohighlight">\(A\)</span> to write the columns of the identity matrix as unique linear combinations of the columns of <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p><em>Proof:</em> Suppose first that <span class="math notranslate nohighlight">\(A\)</span> has rank <span class="math notranslate nohighlight">\(n\)</span>. Then its columns are linearly independent and form a basis of <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>. In particular, for any <span class="math notranslate nohighlight">\(i\)</span> the standard basis vector <span class="math notranslate nohighlight">\(\mathbf{e}_i\)</span> can be written as a unique linear combination of the columns of <span class="math notranslate nohighlight">\(A\)</span>, i.e., there is <span class="math notranslate nohighlight">\(\mathbf{b}_i\)</span> such that <span class="math notranslate nohighlight">\(A \mathbf{b}_i =\mathbf{e}_i\)</span>. Let <span class="math notranslate nohighlight">\(B\)</span> be the matrix with columns <span class="math notranslate nohighlight">\(\mathbf{b}_i\)</span>, <span class="math notranslate nohighlight">\(i=1,\ldots,n\)</span>. By construction, <span class="math notranslate nohighlight">\(A B = I_{n\times n}\)</span>. Applying the same idea to the rows of <span class="math notranslate nohighlight">\(A\)</span> (which by the <em>Row Rank Equals Column Rank Lemma</em> also form a basis of <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>), there is a unique <span class="math notranslate nohighlight">\(C\)</span> such that <span class="math notranslate nohighlight">\(C A = I_{n\times n}\)</span>. Multiplying both sides by <span class="math notranslate nohighlight">\(B\)</span>, we get</p>
<div class="math notranslate nohighlight">
\[
C = C A B = I_{n \times n} B = B.
\]</div>
<p>So we take <span class="math notranslate nohighlight">\(A^{-1} = B = C\)</span>.</p>
<p>In the other direction, following the same argument, the equation <span class="math notranslate nohighlight">\(A A^{-1} = I_{n \times n}\)</span> implies that the standard basis of <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span> is in the column space of <span class="math notranslate nohighlight">\(A\)</span>. So the columns of <span class="math notranslate nohighlight">\(A\)</span> are a spanning list of all of <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span> and <span class="math notranslate nohighlight">\(\mathrm{rk}(A) = n\)</span>. That proves the claim. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>THEOREM</strong> <strong>(Inverting a Nonsingular System)</strong> Let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times n}\)</span> be a nonsingular square matrix. Then for any <span class="math notranslate nohighlight">\(\mathbf{b} \in \mathbb{R}^n\)</span>, there exists a unique <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^n\)</span> such that <span class="math notranslate nohighlight">\(A \mathbf{x} = \mathbf{b}\)</span>. Moreover <span class="math notranslate nohighlight">\(\mathbf{x} = A^{-1} \mathbf{b}\)</span>. <span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p><em>Proof:</em> The first claim follows immediately from the fact that the columns of <span class="math notranslate nohighlight">\(A\)</span> form a basis of <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>. For the second claim, note that</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x} = A^{-1} A \mathbf{x} = A^{-1} \mathbf{b}.
\]</div>
<p><span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>EXAMPLE:</strong> Let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times m}\)</span> with <span class="math notranslate nohighlight">\(n \geq m\)</span> have full column rank, i.e. rank <span class="math notranslate nohighlight">\(m\)</span>. We will show that the square matrix <span class="math notranslate nohighlight">\(B = A^T A\)</span> is then invertible.</p>
<p>By a claim above, the columns of <span class="math notranslate nohighlight">\(A\)</span> form a basis of its column space. In particular they are linearly independent. We will use this below.</p>
<p>Observe that <span class="math notranslate nohighlight">\(B\)</span> is an <span class="math notranslate nohighlight">\(m \times m\)</span> matrix. By definition, to show that it is nonsingular, we need to establish that it has rank <span class="math notranslate nohighlight">\(m\)</span>, or put differently that its columns are also linearly independent. By the matrix version of the <em>Equivalent Definition of Linear Independence</em>, it suffices to show that</p>
<div class="math notranslate nohighlight">
\[
B \mathbf{x} = \mathbf{0} \implies \mathbf{x} = \mathbf{0}.
\]</div>
<p>We establish this next.</p>
<p>Since <span class="math notranslate nohighlight">\(B = A^T A\)</span>, the equation <span class="math notranslate nohighlight">\(B \mathbf{x} = \mathbf{0}\)</span> implies</p>
<div class="math notranslate nohighlight">
\[
A^T A \mathbf{x} = \mathbf{0}.
\]</div>
<p>Now comes the key idea: we multiply both sides by <span class="math notranslate nohighlight">\(\mathbf{x}^T\)</span>. The left-hand side becomes</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x}^T (A^T A \mathbf{x}) 
= (A \mathbf{x})^T (A \mathbf{x}) 
= \|A \mathbf{x}\|^2,
\]</div>
<p>where we used that, for matrices <span class="math notranslate nohighlight">\(C, D\)</span>, we have <span class="math notranslate nohighlight">\((CD)^T = D^T C^T\)</span>. The right-hand side becomes <span class="math notranslate nohighlight">\(\mathbf{x}^T \mathbf{0} = 0\)</span>. Hence we have shown that <span class="math notranslate nohighlight">\(A^T A \mathbf{x} = \mathbf{0}\)</span> in fact implies <span class="math notranslate nohighlight">\(\|A \mathbf{x}\|^2 = 0\)</span>.</p>
<p>By the point-separating property of the Euclidean norm, the condition <span class="math notranslate nohighlight">\(\|A \mathbf{x}\|^2 = 0\)</span> implies <span class="math notranslate nohighlight">\(A \mathbf{x} = \mathbf{0}\)</span>. Because <span class="math notranslate nohighlight">\(A\)</span> has linearly independent columns, the <em>Equivalent Definition of Linear Independence</em> in its matrix form again implies that <span class="math notranslate nohighlight">\(\mathbf{x} = \mathbf{0}\)</span>, which is what we needed to prove. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>EXAMPLE:</strong> Let <span class="math notranslate nohighlight">\(\mathbf{q}_1,\ldots,\mathbf{q}_n\)</span> be an orthonormal basis of <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span> and form the matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
Q =
\begin{pmatrix}
| &amp;  &amp; | \\
\mathbf{q}_1 &amp; \ldots &amp; \mathbf{q}_n \\
| &amp;  &amp; | 
\end{pmatrix}.
\end{split}\]</div>
<p>We show that <span class="math notranslate nohighlight">\(Q^{-1} = Q^T\)</span>.</p>
<p>By the definition of matrix multiplication</p>
<div class="math notranslate nohighlight">
\[\begin{split}
Q^T Q
=
\begin{pmatrix}
\langle \mathbf{q}_1, \mathbf{q}_1 \rangle &amp; \cdots &amp; \langle \mathbf{q}_1, \mathbf{q}_n \rangle \\
\langle \mathbf{q}_2, \mathbf{q}_1 \rangle &amp; \cdots &amp; \langle \mathbf{q}_2, \mathbf{q}_n \rangle \\
\vdots &amp; \ddots &amp; \vdots \\
\langle \mathbf{q}_n, \mathbf{q}_1 \rangle &amp; \cdots &amp; \langle \mathbf{q}_n, \mathbf{q}_n \rangle
\end{pmatrix}
= I_{n \times n}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(I_{n \times n}\)</span> denotes the <span class="math notranslate nohighlight">\(n \times n\)</span> identity matrix. This of course follows from the fact that the <span class="math notranslate nohighlight">\(\mathbf{q}_i\)</span>’s are orthonormal.</p>
<p>In the other direction, we claim that <span class="math notranslate nohighlight">\(Q Q^T = I_{n \times n}\)</span> as well. Indeed the matrix <span class="math notranslate nohighlight">\(Q Q^T\)</span> is the orthogonal projection on the span of the <span class="math notranslate nohighlight">\(\mathbf{q}_i\)</span>’s, that is, <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>. By the <em>Orthogonal Projection Theorem</em>, the orthogonal projection <span class="math notranslate nohighlight">\(Q Q^T \mathbf{v}\)</span> finds the closest vector to <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> in the span of the <span class="math notranslate nohighlight">\(\mathbf{q}_i\)</span>’s. But that span contains all vectors, including <span class="math notranslate nohighlight">\(\mathbf{v}\)</span>, so we must have <span class="math notranslate nohighlight">\(Q Q^T \mathbf{v} = \mathbf{v}\)</span>. Since this holds for all <span class="math notranslate nohighlight">\(\mathbf{v} \in \mathbb{R}^n\)</span>, the matrix <span class="math notranslate nohighlight">\(Q Q^T\)</span> is the identity map and we have proved the claim. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p>Matrices that satisfy</p>
<div class="math notranslate nohighlight">
\[
Q^T Q = Q Q^T = I_{n \times n}
\]</div>
<p>are called orthogonal matrices.</p>
<p><strong>DEFINITION</strong> <strong>(Orthogonal Matrix)</strong> A square matrix <span class="math notranslate nohighlight">\(Q \in \mathbb{R}^{m\times m}\)</span> is orthogonal if <span class="math notranslate nohighlight">\(Q^T Q = Q Q^T = I_{m \times m}\)</span>. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
</section>
<section id="overdetermined-case">
<h2><span class="section-number">2.4.2. </span>Overdetermined case<a class="headerlink" href="#overdetermined-case" title="Permalink to this headline">#</a></h2>
<p>In this section, we discuss the least-squares problem. Let again <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n\times m}\)</span> be an <span class="math notranslate nohighlight">\(n\times m\)</span> matrix with linearly independent columns and let <span class="math notranslate nohighlight">\(\mathbf{b} \in \mathbb{R}^n\)</span> be a vector. We are looking to solve the system</p>
<div class="math notranslate nohighlight">
\[
A \mathbf{x} \approx \mathbf{b}.
\]</div>
<p>If <span class="math notranslate nohighlight">\(n=m\)</span>, we can use the <a class="reference external" href="https://en.wikipedia.org/wiki/Invertible_matrix">matrix inverse</a> to solve the system, as discussed in the previous subsection. But we are particularly interested in the overdetermined case, i.e. when <span class="math notranslate nohighlight">\(n &gt; m\)</span>: there are more equations than variables. We cannot use the matrix inverse then. Indeed, because the columns do not span all of <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>, there is a vector <span class="math notranslate nohighlight">\(\mathbf{b} \in \mathbb{R}^n\)</span> that is not in the column space of <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p>A natural way to make sense of the overdetermined problem is to cast it as the <a class="reference external" href="https://en.wikipedia.org/wiki/Least_squares">linear least squares problem</a></p>
<div class="math notranslate nohighlight">
\[
\min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\|.
\]</div>
<p>In words, we look for the best-fitting solution under the Euclidean norm. Equivalently, writing</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A =
\begin{pmatrix}
| &amp;  &amp; | \\
\mathbf{a}_1 &amp; \ldots &amp; \mathbf{a}_m \\
| &amp;  &amp; | 
\end{pmatrix}
=
\begin{pmatrix}
a_{11} &amp; \cdots &amp; a_{1m} \\
a_{21} &amp; \cdots &amp; a_{2m} \\
\vdots &amp; \ddots &amp; \vdots \\
a_{n1} &amp; \cdots &amp; a_{nm}
\end{pmatrix}
\quad
\text{and}
\quad
\mathbf{b}
=
\begin{pmatrix}
b_1 \\
\vdots \\
b_n
\end{pmatrix}
\end{split}\]</div>
<p>we seek a linear combination of the columns of <span class="math notranslate nohighlight">\(A\)</span> that minimizes the objective</p>
<div class="math notranslate nohighlight">
\[
\left\|\,\sum_{j=1}^m x_j \mathbf{a}_j - \mathbf{b}\,\right\|^2
= \sum_{i=1}^n \left(
\sum_{j=1}^m  a_{ij} x_j - b_i
\right)^2.
\]</div>
<p>We have already solved a closely related problem when we introduced the orthogonal projection. We make the connection explicit next.</p>
<p><strong>THEOREM</strong> <strong>(Normal Equations)</strong> Let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n\times m}\)</span> be an <span class="math notranslate nohighlight">\(n\times m\)</span> matrix with <span class="math notranslate nohighlight">\(n \geq m\)</span> and let <span class="math notranslate nohighlight">\(\mathbf{b} \in \mathbb{R}^n\)</span> be a vector. A solution <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span> to the linear least squares problem</p>
<div class="math notranslate nohighlight">
\[
\min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\|
\]</div>
<p>satisfies the normal equations</p>
<div class="math notranslate nohighlight">
\[
A^T A \mathbf{x}^* = A^T \mathbf{b}.
\]</div>
<p>If further the columns of <span class="math notranslate nohighlight">\(A\)</span> are linearly independent, then there exists a unique solution <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span>. <span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p><em>Proof idea:</em> Apply our characterization of the orthogonal projection onto the column space of <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p><em>Proof:</em> Let <span class="math notranslate nohighlight">\(U = \mathrm{col}(A) = \mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_m)\)</span>. By the <em>Orthogonal Projection Theorem</em>, the orthogonal projection <span class="math notranslate nohighlight">\(\mathbf{p}^* = \mathrm{proj}_{U} \mathbf{b}\)</span> of <span class="math notranslate nohighlight">\(\mathbf{b}\)</span> onto <span class="math notranslate nohighlight">\(U\)</span> is the unique, closest vector to <span class="math notranslate nohighlight">\(\mathbf{b}\)</span> in <span class="math notranslate nohighlight">\(U\)</span>, that is,</p>
<div class="math notranslate nohighlight">
\[
\mathbf{p}^* = \arg\min_{\mathbf{p} \in U} \|\mathbf{p} - \mathbf{b}\|. 
\]</div>
<p>Because <span class="math notranslate nohighlight">\(\mathbf{p}^*\)</span> is in <span class="math notranslate nohighlight">\(U = \mathrm{col}(A)\)</span>, it must be of the form <span class="math notranslate nohighlight">\(\mathbf{p}^* = A \mathbf{x}^*\)</span>. This establishes that <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span> is a solution to the linear least squares problem in the statement.</p>
<p><em>Important observation:</em> while we have shown that <span class="math notranslate nohighlight">\(\mathbf{p}^*\)</span> is unique (by the <em>Orthogonal Projection Theorem</em>), it is not clear at all that <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span> (i.e., the linear combination of columns of <span class="math notranslate nohighlight">\(A\)</span> corresponding to <span class="math notranslate nohighlight">\(\mathbf{p}^*\)</span>) is unique. By the <em>Orthogonal Projection Theorem</em>, it must satisfy <span class="math notranslate nohighlight">\(\langle \mathbf{b} - A \mathbf{x}^*, \mathbf{u}\rangle = 0\)</span> for all <span class="math notranslate nohighlight">\(\mathbf{u} \in U\)</span>. Because the columns <span class="math notranslate nohighlight">\(\mathbf{a}_i\)</span> are in <span class="math notranslate nohighlight">\(U\)</span>, that implies that</p>
<div class="math notranslate nohighlight">
\[
0 = \langle \mathbf{b} - A \mathbf{x}^*, \mathbf{a}_i\rangle 
= \mathbf{a}_i^T (\mathbf{b} - A \mathbf{x}^*) ,\qquad \forall i\in [m].
\]</div>
<p>Stacking up these equations gives in matrix form</p>
<div class="math notranslate nohighlight">
\[
A^T (\mathbf{b} - A\mathbf{x}^*) = \mathbf{0},
\]</div>
<p>as claimed.</p>
<p>We have seen in a previous example that, when <span class="math notranslate nohighlight">\(A\)</span> has full column rank, the matrix <span class="math notranslate nohighlight">\(A^T A\)</span> is invertible. That proves the uniqueness claim. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>NUMERICAL CORNER:</strong> To solve a linear system in Numpy, use <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html"><code class="docutils literal notranslate"><span class="pre">numpy.linalg.solve</span></code></a>. As an example, we consider the overdetermined system with</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A = \begin{pmatrix}
1 &amp; 0\\
0 &amp; 1\\
1 &amp; 1
\end{pmatrix}
\quad
\text{and}
\quad
\mathbf{b} = \begin{pmatrix}
0\\
0\\
2
\end{pmatrix}.
\end{split}\]</div>
<p>We use <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.T.html"><code class="docutils literal notranslate"><span class="pre">numpy.ndarray.T</span></code></a> for the transpose.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">),</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">])</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">LA</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">A</span><span class="p">,</span> <span class="n">A</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.66666667 0.66666667]
</pre></div>
</div>
</div>
</div>
<p>We can also use <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html"><code class="docutils literal notranslate"><span class="pre">numpy.linalg.lstsq</span></code></a> directly on the overdetermined system to compute the least-square solution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">LA</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">rcond</span><span class="o">=</span><span class="kc">None</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.66666667 0.66666667]
</pre></div>
</div>
</div>
</div>
</section>
<section id="least-squares-via-cholesky">
<h2><span class="section-number">2.4.3. </span>Least squares via Cholesky<a class="headerlink" href="#least-squares-via-cholesky" title="Permalink to this headline">#</a></h2>
<p>In a first linear algebra course, one learns how to solve a square linear system such as the normal equations. For this task a common approach is Gaussian elimination, or row reduction. Quoting <a class="reference external" href="https://en.wikipedia.org/wiki/Gaussian_elimination">Wikipedia</a>:</p>
<blockquote>
<div><p>To perform row reduction on a matrix, one uses a sequence of elementary row operations to modify the matrix until the lower left-hand corner of the matrix is filled with zeros, as much as possible. […] Once all of the leading coefficients (the leftmost nonzero entry in each row) are 1, and every column containing a leading coefficient has zeros elsewhere, the matrix is said to be in reduced row echelon form. […] The process of row reduction […] can be divided into two parts. The first part (sometimes called forward elimination) reduces a given system to row echelon form, from which one can tell whether there are no solutions, a unique solution, or infinitely many solutions. The second part (sometimes called back substitution) continues to use row operations until the solution is found; in other words, it puts the matrix into reduced row echelon form.</p>
</div></blockquote>
<p>Here is an example.</p>
<p><img alt="Gaussian elimination" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/65d92f997de9f7ad787b95d08fcd25dca828dd93" /></p>
<p><strong>Figure:</strong> Gaussian elimination (<a class="reference external" href="https://en.wikipedia.org/wiki/Gaussian_elimination">Source</a>)</p>
<p>We will not go over Gaussian elimination here. Instead we will derive a modified approach to solve the normal equations that takes advantage of the special structure of this system to compute the solution twice as fast. In the process, we will also obtain an important notion of matrix factorization.</p>
<p><strong>Triangular systems</strong> We will need one component of Gaussian elimination, back-substitution.</p>
<p><strong>DEFINITION</strong> <strong>(Triangular matrix)</strong> A matrix <span class="math notranslate nohighlight">\(R = (r_{i,j})_{i,j} \in \mathbb{R}^{n \times m}\)</span> is upper-triangular if all entries below the diagonal are zero, that is, if <span class="math notranslate nohighlight">\(i &gt; j\)</span> implies <span class="math notranslate nohighlight">\(r_{ij} = 0\)</span>. Similarly, a lower-triangular matrix has zeros above hte diagonal. <span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>So an upper-triangular matrix is of the following form:</p>
<p><img alt="upper-triangular" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f770c7fa4c212eac3d7d7f9a54f7decbc811276f" /></p>
<p><strong>Figure:</strong> An upper-triangular matrix (<a class="reference external" href="https://en.wikipedia.org/wiki/Triangular_matrix">Source</a>)</p>
<p>Triangular systems of equations are straightforward to solve. We start with an example.</p>
<p><strong>EXAMPLE:</strong> Here is a concrete example of back substitution. Consider the system <span class="math notranslate nohighlight">\(R \mathbf{x} = \mathbf{b}\)</span> with</p>
<div class="math notranslate nohighlight">
\[\begin{split}
R
=
\begin{pmatrix}
2 &amp; -1 &amp; 2\\
0 &amp; 1 &amp; 1\\
0 &amp; 0 &amp; 2
\end{pmatrix}
\qquad
\mathbf{b}
=
\begin{pmatrix}
0\\
-2\\
0
\end{pmatrix}.
\end{split}\]</div>
<p>That corresponds to the linear equations</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;2 x_1 - x_2 + 2x_3 = 0\\
&amp;x_2 + x_3 = -2\\
&amp;2 x_3 = 0
\end{align*}\]</div>
<p>The third equation gives <span class="math notranslate nohighlight">\(x_3 = 0/2 = 0\)</span>. Plugging into the second one, we get <span class="math notranslate nohighlight">\(x_2 = -2 - x_3 = -2\)</span>. Plugging into the first one, we finally have <span class="math notranslate nohighlight">\(x_1 = (x_2 - 2 x_3)/2 = -1\)</span>. So the solution is <span class="math notranslate nohighlight">\(\mathbf{x} = (-1,-2,0)^T\)</span>. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p>In general, solving a triangular system of equations works as follows. Let <span class="math notranslate nohighlight">\(R = (r_{i,j})_{i,j} \in \mathbb{R}^{m \times m}\)</span> be upper-triangular and let <span class="math notranslate nohighlight">\(\mathbf{b} \in \mathbb{R}^m\)</span> be the left-hand vector, i.e., we want to solve the system</p>
<div class="math notranslate nohighlight">
\[
R \mathbf{x} = \mathbf{b}.
\]</div>
<p>Starting from the last row of the system, <span class="math notranslate nohighlight">\(r_{m,m} x_m = b_m\)</span> or <span class="math notranslate nohighlight">\(x_m = b_m/r_{m,m}\)</span>, assuming that <span class="math notranslate nohighlight">\(r_{m,m} \neq 0\)</span>. Moving to the second-to-last row, <span class="math notranslate nohighlight">\(r_{m-1,m-1} x_{m-1} + r_{m-1,m} x_m = b_{m-1}\)</span> or <span class="math notranslate nohighlight">\(x_{m-1} = (b_{m-1} - r_{m-1,m} x_m)/r_{m-1,m-1}\)</span>, assuming that <span class="math notranslate nohighlight">\(r_{m-1,m-1} \neq 0\)</span>. And so on. This procedure is known as <a class="reference external" href="https://en.wikipedia.org/wiki/Triangular_matrix#Forward_and_back_substitution">back substitution</a>.</p>
<p>Analogously, in the lower triangular case <span class="math notranslate nohighlight">\(L \in \mathbb{R}^{m \times m}\)</span>, we have <a class="reference external" href="https://en.wikipedia.org/wiki/Triangular_matrix#Forward_substitution">forward substitution</a>. These procedures implicitly define an inverse for <span class="math notranslate nohighlight">\(R\)</span> and <span class="math notranslate nohighlight">\(L\)</span> <em>when the diagonal elements are all non-zero</em>. We will not write them down explicitly here.</p>
<p><strong>NUMERICAL CORNER:</strong> We implement back substitution in Python. In our naive implementation, we assume that the diagonal entries are not zero, which will suffice for our purposes. Here we use <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.dot.html"><code class="docutils literal notranslate"><span class="pre">numpy.dot</span></code></a> to compute inner products.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">backsubs</span><span class="p">(</span><span class="n">R</span><span class="p">,</span><span class="n">b</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">)):</span>
        <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">R</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span><span class="n">m</span><span class="p">],</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span><span class="n">m</span><span class="p">]))</span><span class="o">/</span><span class="n">R</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">i</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<p>Forward substitution is implemented similarly.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forwardsubs</span><span class="p">(</span><span class="n">L</span><span class="p">,</span><span class="n">b</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">L</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="n">i</span><span class="p">],</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">i</span><span class="p">]))</span><span class="o">/</span><span class="n">L</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">i</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>Positive semidefinite matrices</strong> What special structure does the matrix <span class="math notranslate nohighlight">\(A^T A\)</span> have? For one, it is square and symmetric.</p>
<p><strong>DEFINITION</strong> <strong>(Symmetric Matrix)</strong> A square matrix <span class="math notranslate nohighlight">\(B \in \mathbb{R}^{n \times n}\)</span> is symmetric if <span class="math notranslate nohighlight">\(B^T = B\)</span>. <span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>It can be shown that <span class="math notranslate nohighlight">\((A^T A)^T = A^T A\)</span>. That is, <span class="math notranslate nohighlight">\(A^T A\)</span> is symmetric.</p>
<p>The matrix <span class="math notranslate nohighlight">\(A^T A\)</span> also has a less obvious, but important, property.</p>
<p><strong>DEFINITION</strong> <strong>(Positive Semidefinite Matrix)</strong> A symmetric matrix <span class="math notranslate nohighlight">\(B \in \mathbb{R}^{n \times n}\)</span> is positive semidefinite if</p>
<div class="math notranslate nohighlight">
\[
\langle \mathbf{x}, B \mathbf{x} \rangle \geq 0, \quad \forall \mathbf{x} \neq \mathbf{0}. 
\]</div>
<p>We also write <span class="math notranslate nohighlight">\(B \succeq 0\)</span> in that case. If the inequality above is strict, we say that <span class="math notranslate nohighlight">\(B\)</span> is positive definite, in which case we write <span class="math notranslate nohighlight">\(B \succ 0\)</span>. <span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>Note that – by definition – a positive semidefinite matrix is necessarily symmetric. (For a discussion on this point, see <a class="reference external" href="https://math.stackexchange.com/questions/1954167/do-positive-semidefinite-matrices-have-to-be-symmetric">here</a>.)</p>
<p><strong>EXAMPLE:</strong> Consider a square, symmetric matrix of the form <span class="math notranslate nohighlight">\(B = \mathrm{diag}(\beta_1, \ldots, \beta_d)\)</span>, i.e., all non-diagonal entries  of <span class="math notranslate nohighlight">\(B\)</span> are <span class="math notranslate nohighlight">\(0\)</span> and its diagonal elements are <span class="math notranslate nohighlight">\(b_{ii} = \beta_i, \forall i\)</span>.</p>
<p>We claim first that</p>
<div class="math notranslate nohighlight">
\[
(*) \qquad \langle \mathbf{x}, B \mathbf{x} \rangle = \sum_{i=1}^d \beta_i x_i^2, \qquad \forall \mathbf{x} = (x_1,\ldots,x_d)^T \in \mathbb{R}^d.
\]</div>
<p>Indeed, by the diagonal structure of <span class="math notranslate nohighlight">\(B\)</span>, we have that <span class="math notranslate nohighlight">\(B \mathbf{x} = (\beta_1 x_1, \ldots, \beta_d x_d)^T\)</span>. Equation <span class="math notranslate nohighlight">\((*)\)</span> immediately follows.</p>
<p>Using <span class="math notranslate nohighlight">\((*)\)</span> we prove</p>
<div class="math notranslate nohighlight">
\[
(**) \qquad B \succeq 0 \iff \beta_i \geq 0, \forall i.
\]</div>
<p>From <span class="math notranslate nohighlight">\((*)\)</span> it is immediate that, if <span class="math notranslate nohighlight">\(\beta_i \geq 0, \forall i\)</span>, it holds that <span class="math notranslate nohighlight">\(\langle \mathbf{x}, B \mathbf{x} \rangle = \sum_{i=1}^d \beta_i x_i^2 \geq 0\)</span> for all <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> since each term in the sum is nonnegative. For the other direction, we argue by contradiction. Suppose that <span class="math notranslate nohighlight">\(B \succeq 0\)</span> and that there is a <span class="math notranslate nohighlight">\(\beta_j &lt; 0\)</span>. Then, for the unit basis vector <span class="math notranslate nohighlight">\(\mathbf{x} = \mathbf{e}_j\)</span>, we get from <span class="math notranslate nohighlight">\((**)\)</span> that</p>
<div class="math notranslate nohighlight">
\[
\langle \mathbf{x}, B \mathbf{x} \rangle = \sum_{i=1}^d \beta_i x_i^2
= \beta_j (1)^2 &lt; 0,
\]</div>
<p>a contradiction. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>EXAMPLE:</strong> Perhaps counter-intuitively, in general, <span class="math notranslate nohighlight">\(B \succeq 0\)</span> is <em>not</em> the same as <span class="math notranslate nohighlight">\(B\)</span> having only nonnegative elements. Here is an example. Consider the matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A = 
\begin{pmatrix}
1 &amp; 10\\
10 &amp; 1
\end{pmatrix}.
\end{split}\]</div>
<p>While all of its elements are nonnegative, it is not positive semidefinite. Indeed let <span class="math notranslate nohighlight">\(\mathbf{x} = (1, -1)^T\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
\langle \mathbf{x}, A \mathbf{x} \rangle 
= (1, -1)^T \, (1 - 10, 10 -1)
= (1, -1)^T \, (-9, 9)
= -18 &lt; 0.
\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p>We return to our main example.</p>
<p><strong>LEMMA</strong> <strong>(Least Squares and Postive Semidefiniteness)</strong> Let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n\times m}\)</span> be an <span class="math notranslate nohighlight">\(n\times m\)</span> matrix with <span class="math notranslate nohighlight">\(n \geq m\)</span>. The matrix <span class="math notranslate nohighlight">\(B = A^T A\)</span> is positive semidefinite. If further the columns of <span class="math notranslate nohighlight">\(A\)</span> are linearly independent, then the matrix <span class="math notranslate nohighlight">\(B\)</span> is positive definite. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> For any <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x}^T (A^T A) \mathbf{x}
= (A \mathbf{x})^T (A \mathbf{x}) 
= \|A \mathbf{x}\|^2 \geq 0.
\]</div>
<p>Hence <span class="math notranslate nohighlight">\(B \succeq 0\)</span>. If the inequality above is an equality, by the point-separating property of the Euclidean norm, that means that we must have <span class="math notranslate nohighlight">\(A \mathbf{x} = \mathbf{0}\)</span>. If <span class="math notranslate nohighlight">\(A\)</span> has full column rank, the <em>Equivalent Definition of Linear Independence</em> implies that <span class="math notranslate nohighlight">\(\mathbf{x} = \mathbf{0}\)</span>, which establishes the second claim. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>Cholesky decomposition</strong> Our key linear-algebraic result of this section is the following. The matrix factorization in the next theorem is called a Cholesky decomposition. It has many <a class="reference external" href="https://en.wikipedia.org/wiki/Cholesky_decomposition#Applications">applications</a>.</p>
<p><strong>THEOREM</strong> <strong>(Cholesky Decomposition)</strong> Any positive definite matrix <span class="math notranslate nohighlight">\(B \in \mathbb{R}^{n \times n}\)</span> can be factorized uniquely as</p>
<div class="math notranslate nohighlight">
\[
B = L L^T
\]</div>
<p>where <span class="math notranslate nohighlight">\(L \in \mathbb{R}^{n \times n}\)</span> is a lower triangular matrix with positive entries on the diagonal. <span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p>The proof is provided in a section below. An important consequence of the proof is an algorithm for computing the Cholesky decomposition. Indeed it follows from the equations in the proof that we can grow <span class="math notranslate nohighlight">\(L\)</span> starting from its top-left corner by successively computing its next row based on the previously constructed submatrix. Note that, because <span class="math notranslate nohighlight">\(L\)</span> is lower triangular, it suffices to compute its elements on and below the diagonal.</p>
<p>Here is an illustration of the flow of this construction.</p>
<p><img alt="Access pattern" src="https://upload.wikimedia.org/wikipedia/commons/b/be/Chol.gif" /></p>
<p><strong>Figure:</strong> Access pattern (<a class="reference external" href="https://en.wikipedia.org/wiki/File:Chol.gif">Source</a>)</p>
<p><strong>EXAMPLE:</strong> Before proceeeding with the general method, we give a small example to provide some intuition as to how it operates. Consider the matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A 
= 
\begin{pmatrix}
1 &amp; 2 &amp; 1\\
0 &amp; -2 &amp; 1\\
0 &amp; 0 &amp; 1\\
0 &amp; 0 &amp; 0
\end{pmatrix}.
\end{split}\]</div>
<p>By the <em>Least Squares and Postive Semidefiniteness Lemma</em>, the matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A^T A 
= 
\begin{pmatrix}
1 &amp; 2 &amp; 1\\
2 &amp; 8 &amp; 0\\
1 &amp; 0 &amp; 3
\end{pmatrix}
\end{split}\]</div>
<p>is positive semidefinite. In fact, the columns of <span class="math notranslate nohighlight">\(A\)</span> are linearly independent since <span class="math notranslate nohighlight">\(A \mathbf{x} = \mathbf{0}\)</span> can be written as</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
x_1 + 2x_2 + x_3 &amp;= 0\\
-2 x_2 + x_3 &amp;= 0\\
x_3 &amp;= 0
\end{align*}\]</div>
<p>whose solution is <span class="math notranslate nohighlight">\(x_3 = x_2 = x_1 = 0\)</span> by backward substitution, i.e., <span class="math notranslate nohighlight">\(\mathbf{x} = \mathbf{0}\)</span>. Hence <span class="math notranslate nohighlight">\(A^T A\)</span> is positive definite by the <em>Least Squares and Postive Semidefiniteness Lemma</em>.</p>
<p>Let <span class="math notranslate nohighlight">\(L = (\ell_{i,j})_{i,j=1}^3\)</span> be lower triangular. We seek to solve <span class="math notranslate nohighlight">\(L L^T = A^T A\)</span> for the nonzero entries of <span class="math notranslate nohighlight">\(L\)</span>. Observe that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{pmatrix}
\ell_{1,1} &amp; 0 &amp; 0\\
\ell_{2,1} &amp; \ell_{2,2} &amp; 0\\
\ell_{3,1} &amp; \ell_{3,2} &amp; \ell_{3,3}
\end{pmatrix}
\begin{pmatrix}
\ell_{1,1} &amp; \ell_{2,1} &amp; \ell_{3,1}\\
0 &amp; \ell_{2,2} &amp; \ell_{3,2}\\
0 &amp; 0 &amp; \ell_{3,3}
\end{pmatrix}
=
\begin{pmatrix}
\ell_{1,1}^2 &amp; \ell_{1,1}\ell_{2,1} &amp; \ell_{1,1}\ell_{3,1}\\
\ell_{1,1}\ell_{2,1} &amp; \ell_{2,1}^2 + \ell_{2,2}^2 &amp; \ell_{2,1}\ell_{3,1} + \ell_{2,2}\ell_{3,2}\\
\ell_{1,1}\ell_{3,1} &amp; \ell_{2,1}\ell_{3,1} + \ell_{2,2}\ell_{3,2} &amp; \ell_{3,1}^2 + \ell_{3,2}^2 + \ell_{3,3}
\end{pmatrix}.
\end{split}\]</div>
<p>The system</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{pmatrix}
\ell_{1,1}^2 &amp; \ell_{1,1}\ell_{2,1} &amp; \ell_{1,1}\ell_{3,1}\\
\ell_{1,1}\ell_{2,1} &amp; \ell_{2,1}^2 + \ell_{2,2}^2 &amp; \ell_{2,1}\ell_{3,1} + \ell_{2,2}\ell_{3,2}\\
\ell_{1,1}\ell_{3,1} &amp; \ell_{2,1}\ell_{3,1} + \ell_{2,2}\ell_{3,2} &amp; \ell_{3,1}^2 + \ell_{3,2}^2 + \ell_{3,3}^2
\end{pmatrix}
=
\begin{pmatrix}
1 &amp; 2 &amp; 1\\
2 &amp; 8 &amp; 0\\
1 &amp; 0 &amp; 3
\end{pmatrix}
\end{split}\]</div>
<p>turns out to be fairly simple to solve.</p>
<ol class="arabic simple">
<li><p>From the first entry, we get <span class="math notranslate nohighlight">\(\ell_{1,1} = 1\)</span> (where we took the positive solution to <span class="math notranslate nohighlight">\(\ell_{1,1}^2 = 1\)</span>).</p></li>
<li><p>Given that <span class="math notranslate nohighlight">\(\ell_{1,1}\)</span> is known, entry <span class="math notranslate nohighlight">\(\ell_{2,1}\)</span> is determined from <span class="math notranslate nohighlight">\(\ell_{1,1}\ell_{2,1} =2\)</span> in the first entry of the second row. That is, <span class="math notranslate nohighlight">\(\ell_{2,1} =2\)</span>. Then the second entry of the second row gives <span class="math notranslate nohighlight">\(\ell_{2,2}\)</span> through <span class="math notranslate nohighlight">\(\ell_{2,1}^2 + \ell_{2,2}^2  = 8\)</span>. So <span class="math notranslate nohighlight">\(\ell_{2,2} = 2\)</span> (again we take the positive solution).</p></li>
<li><p>We move to the third row. The first entry gives <span class="math notranslate nohighlight">\(\ell_{3,1} = 1\)</span>, the second entry gives <span class="math notranslate nohighlight">\(\ell_{3,2} = -1\)</span> and finally the third entry leads to <span class="math notranslate nohighlight">\(\ell_{3,3} = 1\)</span>.</p></li>
</ol>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p>Recall that a block matrix is a partitioning of the rows and columns of a matrix of the form</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A 
=
\begin{pmatrix}
A_{11} &amp; A_{12}\\
A_{21} &amp; A_{22}
\end{pmatrix}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times m}\)</span>, <span class="math notranslate nohighlight">\(A_{ij} \in \mathbb{R}^{n_i \times m_j}\)</span> for <span class="math notranslate nohighlight">\(i,j = 1, 2\)</span> with the conditions <span class="math notranslate nohighlight">\(n_1 + n_2 = n\)</span> and <span class="math notranslate nohighlight">\(m_1 + m_2 = m\)</span>. More generally, one can consider larger numbers of blocks. Block matrices have a convenient algebra that mimics the usual matrix algebra. Specifically, if <span class="math notranslate nohighlight">\(B_{ij} \in \mathbb{R}^{m_i \times p_j}\)</span> for <span class="math notranslate nohighlight">\(i,j = 1, 2\)</span>, then it holds that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{pmatrix}
A_{11} &amp; A_{12}\\
A_{21} &amp; A_{22}
\end{pmatrix}
\begin{pmatrix}
B_{11} &amp; B_{12}\\
B_{21} &amp; B_{22}
\end{pmatrix}
=
\begin{pmatrix}
A_{11} B_{11} + A_{12} B_{21} &amp; A_{11} B_{12} + A_{12} B_{22}\\
A_{21} B_{11} + A_{22} B_{21} &amp; A_{21} B_{12} + A_{22} B_{22}
\end{pmatrix}.
\end{split}\]</div>
<p>Observe that the block sizes of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> must match for this formula to make sense. You can convince yourself of this identity by trying it on a simple example. For a (somewhat tedious) proof, see e.g. <a class="reference external" href="https://sites.math.washington.edu/~morrow/498_13/blockmatrices.pdf">here</a>.</p>
<p>To detail the computation of the Cholesky decomposition <span class="math notranslate nohighlight">\(L L^T\)</span> of <span class="math notranslate nohighlight">\(B\)</span>, we will need some notation. Write <span class="math notranslate nohighlight">\(B = (b_{i,j})_{i,j=1}^n\)</span> and <span class="math notranslate nohighlight">\(L = (\ell_{i,j})_{i,j=1}^n\)</span>. Let <span class="math notranslate nohighlight">\(L_{(k)} = (\ell_{i,j})_{i,j=1}^k\)</span> be the first <span class="math notranslate nohighlight">\(k\)</span> rows and columns of <span class="math notranslate nohighlight">\(L\)</span>, let <span class="math notranslate nohighlight">\(\bflambda_{(k)}^T = (\ell_{k,1},\ldots,\ell_{k,k-1})\)</span> be the row vector corresponding to the first <span class="math notranslate nohighlight">\(k-1\)</span> entries of row <span class="math notranslate nohighlight">\(k\)</span> of <span class="math notranslate nohighlight">\(L\)</span>, and let <span class="math notranslate nohighlight">\(\bfbeta_{(k)}^T = (b_{k,1},\ldots,b_{k,k-1})\)</span> be the row vector corresponding to the first <span class="math notranslate nohighlight">\(k-1\)</span> entries of row <span class="math notranslate nohighlight">\(k\)</span> of <span class="math notranslate nohighlight">\(B\)</span>.</p>
<p>The strategy is to compute <span class="math notranslate nohighlight">\(L_{(1)}\)</span>, then <span class="math notranslate nohighlight">\(L_{(2)}\)</span>, then <span class="math notranslate nohighlight">\(L_{(3)}\)</span> and so on. With the notation above, <span class="math notranslate nohighlight">\(L_{(j)}\)</span> can be written in block form as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
L_{(j)}
=
\begin{pmatrix}
L_{(j-1)} &amp; \mathbf{0}\\
\bflambda_{(j)}^T &amp; \ell_{j,j}.
\end{pmatrix}
\end{split}\]</div>
<p>Hence, once <span class="math notranslate nohighlight">\(L_{(j-1)}\)</span> is known, in order to compute <span class="math notranslate nohighlight">\(L_{(j)}\)</span> one only needs <span class="math notranslate nohighlight">\(\bflambda_{(j)}\)</span> and <span class="math notranslate nohighlight">\(\ell_{j,j}\)</span>. We show next that they satisfy easily solvable systems of equations.</p>
<p>We first note that the <span class="math notranslate nohighlight">\((1,1)\)</span> entry of the matrix equation <span class="math notranslate nohighlight">\(L L^T = B\)</span> implies that</p>
<div class="math notranslate nohighlight">
\[
\ell_{1,1}^2 = b_{1,1}.
\]</div>
<p>So we set</p>
<div class="math notranslate nohighlight">
\[
L_{(1)}
= \ell_{1,1}
= \sqrt{b_{1,1}}.
\]</div>
<p>For this step to be well-defined, it needs to be the case that <span class="math notranslate nohighlight">\(b_{1,1} &gt; 0\)</span>. It is easy to see that it follows from the positive definiteness of <span class="math notranslate nohighlight">\(B\)</span>:</p>
<div class="math notranslate nohighlight">
\[
0 &lt; \langle \mathbf{e}_1, B \mathbf{e}_1\rangle = \mathbf{e}_1^T B_{\cdot,1} = b_{1,1}.
\]</div>
<p>Proceeding by induction, assume <span class="math notranslate nohighlight">\(L_{(j-1)}\)</span> has been constructed. The first <span class="math notranslate nohighlight">\(j-1\)</span> elements of the <span class="math notranslate nohighlight">\(j\)</span>-th row of the matrix equation <span class="math notranslate nohighlight">\(L L^T = B\)</span> translate into</p>
<div class="math notranslate nohighlight">
\[
L_{j,\cdot} (L^T)_{\cdot,1:j-1} = \bflambda_{(j)}^T L_{(j-1)}^T = \bfbeta_{(j)}^T,
\]</div>
<p>where <span class="math notranslate nohighlight">\((L^T)_{\cdot,1:j-1}\)</span> denotes the first <span class="math notranslate nohighlight">\(j-1\)</span> columns of <span class="math notranslate nohighlight">\(L^T\)</span>. In the first equality above, we used the fact that <span class="math notranslate nohighlight">\(L^T\)</span> is upper triangular. Taking a transpose, the resulting linear system of equations</p>
<div class="math notranslate nohighlight">
\[
L_{(j-1)} \bflambda_{(j)} = \bfbeta_{(j)}
\]</div>
<p>can be solved by forward substitution (since <span class="math notranslate nohighlight">\(\bfbeta_{(j)}\)</span> is part of the input and <span class="math notranslate nohighlight">\(L_{(j-1)}\)</span> was previously computed). The fact that this system has a unique solution (more specifically, that the diagonal entries of <span class="math notranslate nohighlight">\(L_{(j-1)}\)</span> are strictly positive) is established in the proof of the <em>Cholesky Decomposition Theorem</em>.</p>
<p>The <span class="math notranslate nohighlight">\((j,j)\)</span>-th entry of the matrix equation <span class="math notranslate nohighlight">\(L L^T = B\)</span> translates into</p>
<div class="math notranslate nohighlight">
\[
L_{j,\cdot} (L^T)_{\cdot,j} = \sum_{k=1}^j \ell_{j,k}^2 = b_{j,j},
\]</div>
<p>where again we used the fact that <span class="math notranslate nohighlight">\(L^T\)</span> is upper triangular. Since <span class="math notranslate nohighlight">\(\ell_{j,1}, \ldots, \ell_{j,j-1}\)</span> are the elements of <span class="math notranslate nohighlight">\(\bflambda_{(j)}\)</span>, they have already been determined. So we can set</p>
<div class="math notranslate nohighlight">
\[
\ell_{j,j}
= \sqrt{b_{j,j} - \sum_{k=1}^{j-1} \ell_{j,k}^2}.
\]</div>
<p>The fact that we are taking the square root of a positive quantity is established in the proof of the <em>Cholesky Decomposition Theorem</em>. Finally, from <span class="math notranslate nohighlight">\(L_{(j-1)}\)</span>, <span class="math notranslate nohighlight">\(\bflambda_{(j)}\)</span>, and <span class="math notranslate nohighlight">\(\ell_{j,j}\)</span>, we construct <span class="math notranslate nohighlight">\(L_{(j)}\)</span>.</p>
<p><strong>NUMERICAL CORNER:</strong> We implement the algorithm above. In our naive implementation, we assume that <span class="math notranslate nohighlight">\(B\)</span> is positive definite, and therefore that all steps are well-defined.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">cholesky</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">B</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> 
    <span class="n">L</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">L</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">forwardsubs</span><span class="p">(</span><span class="n">L</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">j</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="n">j</span><span class="p">],</span><span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="n">j</span><span class="p">])</span>
        <span class="n">L</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">-</span> <span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">L</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="n">j</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">L</span> 
</pre></div>
</div>
</div>
</div>
<p>Here is a simple example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[2. 1.]
 [1. 2.]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">L</span> <span class="o">=</span> <span class="n">cholesky</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[1.41421356 0.        ]
 [0.70710678 1.22474487]]
</pre></div>
</div>
</div>
</div>
<p>We can check that it produces the right factorization.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">L</span> <span class="o">@</span> <span class="n">L</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[2. 1.]
 [1. 2.]]
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>Using a Cholesky decomposition to solve the least squares problem</strong> In this section, we restrict ourselves to the case where <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n\times m}\)</span> has full column rank. By the <em>Least Squares and Positive Semidefiniteness Lemma</em>, we then have that <span class="math notranslate nohighlight">\(A^T A\)</span> is positive definite. By the <em>Cholesky Decomposition Theorem</em>, we can factorize this matrix as <span class="math notranslate nohighlight">\(A^T A = L L^T\)</span> where <span class="math notranslate nohighlight">\(L\)</span> is lower triangular with positive diagonal elements. The normal equations then reduce to</p>
<div class="math notranslate nohighlight">
\[
L L^T \mathbf{x} = A^T \mathbf{b}.
\]</div>
<p>This system can be solved in two steps. We first obtain the solution to</p>
<div class="math notranslate nohighlight">
\[
L \mathbf{z} = A^T \mathbf{b}
\]</div>
<p>by forward substitution. Then we obtain the solution to</p>
<div class="math notranslate nohighlight">
\[
L^T \mathbf{x} = \mathbf{z}
\]</div>
<p>by back-substitution. Note that <span class="math notranslate nohighlight">\(L^T\)</span> is indeed an upper triangular matrix.</p>
<p><strong>NUMERICAL CORNER:</strong> We implement this algorithm below. In our naive implementation, we assume that <span class="math notranslate nohighlight">\(A\)</span> has full column rank, and therefore that all steps are well-defined.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ls_by_chol</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">L</span> <span class="o">=</span> <span class="n">cholesky</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">A</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">forwardsubs</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">A</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">b</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">backsubs</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
</section>
<section id="back-to-regression-analysis">
<h2><span class="section-number">2.4.4. </span>Back to regression analysis<a class="headerlink" href="#back-to-regression-analysis" title="Permalink to this headline">#</a></h2>
<p>We return to the regression problem and apply the least squares approach.</p>
<p><strong>Linear regression</strong> We seek an affine function to fit input data points <span class="math notranslate nohighlight">\(\{(\mathbf{x}_i, y_i)\}_{i=1}^n\)</span>. The common approach involves finding coefficients <span class="math notranslate nohighlight">\(\beta_j\)</span>’s that minimize the criterion</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^n \left(y_i - \left\{\beta_0 + \sum_{j=1}^d \beta_j x_{ij}\right\}\right)^2.
\]</div>
<p>This is indeed a linear least squares problem.</p>
<p><img alt="Regression line" src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Linear_least_squares_example2.svg/489px-Linear_least_squares_example2.svg.png" /></p>
<p><strong>Figure:</strong> A regression line (<a class="reference external" href="https://commons.wikimedia.org/wiki/File:Linear_least_squares_example2.svg">Source</a>)</p>
<p>In matrix form, let</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{y} = 
\begin{pmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{pmatrix},
\quad\quad
A =
\begin{pmatrix}
1 &amp; \mathbf{x}_1^T \\
1 &amp; \mathbf{x}_2^T \\
\vdots &amp; \vdots \\
1 &amp; \mathbf{x}_n^T
\end{pmatrix}
\quad\text{and}\quad
\boldsymbol{\beta} = 
\begin{pmatrix}
\beta_0 \\
\beta_1 \\
\vdots \\
\beta_d
\end{pmatrix}.
\end{split}\]</div>
<p>Then the problem is</p>
<div class="math notranslate nohighlight">
\[
\min_{\boldsymbol{\beta}} 
\|\mathbf{y} 
- A \boldsymbol{\beta}\|^2.
\]</div>
<p>We assume that the columns of <span class="math notranslate nohighlight">\(A\)</span> are linearly independent, which is typically the case with real data. The normal equations are then</p>
<div class="math notranslate nohighlight">
\[
A^T A \boldsymbol{\beta} = A^T \mathbf{y}.
\]</div>
<p><strong>NUMERICAL CORNER:</strong> We test our least-squares method on simulated data. This has the advantage that we know the truth.</p>
<p>Suppose the truth is a linear function of one variable.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n</span><span class="p">,</span> <span class="n">b0</span><span class="p">,</span> <span class="n">b1</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="n">num</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">b0</span> <span class="o">+</span> <span class="n">b1</span><span class="o">*</span><span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/dbda620a2c5ca8b6d8ebd235ae28660f8296aacf786a0e2c5fe6a6dd03266808.png" src="../../_images/dbda620a2c5ca8b6d8ebd235ae28660f8296aacf786a0e2c5fe6a6dd03266808.png" />
</div>
</div>
<p>A perfect straight line is little too easy. So let’s add some noise. That is, to each <span class="math notranslate nohighlight">\(y_i\)</span> we add an independent random variable <span class="math notranslate nohighlight">\(\varepsilon_i\)</span> with a standard Normal distribution (mean <span class="math notranslate nohighlight">\(0\)</span>, variance <span class="math notranslate nohighlight">\(1\)</span>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">+=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/0db4286a7e5d9df1fd1e7d557ae9c4e85892d25339835a72e848e9ecb0295525.png" src="../../_images/0db4286a7e5d9df1fd1e7d557ae9c4e85892d25339835a72e848e9ecb0295525.png" />
</div>
</div>
<p>We form the matrix <span class="math notranslate nohighlight">\(A\)</span> and use our least-squares code to solve for <span class="math notranslate nohighlight">\(\boldsymbol{\hat\beta}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">),</span><span class="n">x</span><span class="p">),</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">coeff</span> <span class="o">=</span> <span class="n">ls_by_chol</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">coeff</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[-1.03381171  1.01808039]
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">coeff</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="n">coeff</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">x</span><span class="p">,</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/9a69f44f5b2046fa2ce2aacee8a4da2db2c1077c05474702b4cebbf99674d5a0.png" src="../../_images/9a69f44f5b2046fa2ce2aacee8a4da2db2c1077c05474702b4cebbf99674d5a0.png" />
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>Beyond linearity</strong> The linear assumption is not as restrictive as it may first appear. The same approach can be extended straightforwardly to fit polynomials or more complicated combination of functions. For instance, suppose <span class="math notranslate nohighlight">\(d=1\)</span>. To fit a second degree polynomial to the data <span class="math notranslate nohighlight">\(\{(x_i, y_i)\}_{i=1}^n\)</span>, we add a column to the <span class="math notranslate nohighlight">\(A\)</span> matrix with the squares of the <span class="math notranslate nohighlight">\(x_i\)</span>’s. That is, we let</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A =
\begin{pmatrix}
1 &amp; x_1 &amp; x_1^2 \\
1 &amp; x_2 &amp; x_2^2 \\
\vdots &amp; \vdots &amp; \vdots \\
1 &amp; x_n &amp; x_n^2
\end{pmatrix}.
\end{split}\]</div>
<p>Then, we are indeed fitting a degree-two polynomial as follows</p>
<div class="math notranslate nohighlight">
\[
(A \boldsymbol{\beta})_i 
= \beta_0 + \beta_1 x_i + \beta_2 x_i^2.
\]</div>
<p>The solution otherwise remains the same.</p>
<p>This idea of adding columns can also be used to model interactions between predictors. Suppose <span class="math notranslate nohighlight">\(d=2\)</span>. Then we can consider the following <span class="math notranslate nohighlight">\(A\)</span> matrix, where the last column combines both predictors into their product,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A =
\begin{pmatrix}
1 &amp; x_{11} &amp; x_{12} &amp; x_{11} x_{12} \\
1 &amp; x_{21} &amp; x_{22} &amp; x_{21} x_{22} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots\\
1 &amp; x_{n1} &amp; x_{n2} &amp; x_{n1} x_{n2}
\end{pmatrix}.
\end{split}\]</div>
<p><strong>NUMERICAL CORNER:</strong> Suppose the truth is in fact a degree-two polynomial of one variable with Gaussian noise.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n</span><span class="p">,</span> <span class="n">b0</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">b2</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="n">num</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">b0</span> <span class="o">+</span> <span class="n">b1</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b2</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">10</span><span class="o">*</span><span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/2d0154f6922165fae11830d11cff3830f32853c3ca62fc32c53d332bb5503a5f.png" src="../../_images/2d0154f6922165fae11830d11cff3830f32853c3ca62fc32c53d332bb5503a5f.png" />
</div>
</div>
<p>We form the matrix <span class="math notranslate nohighlight">\(A\)</span> and use our least-squares code to solve for <span class="math notranslate nohighlight">\(\boldsymbol{\hat\beta}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">coeff</span> <span class="o">=</span> <span class="n">ls_by_chol</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">coeff</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[-2.76266982  1.01627798  0.93554204]
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">coeff</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">coeff</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">coeff</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/705251050504bfe27dddfb7388c3a8d1bcf1cb6179e83d96d03babc4064cefb3.png" src="../../_images/705251050504bfe27dddfb7388c3a8d1bcf1cb6179e83d96d03babc4064cefb3.png" />
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chap02_ls/04_overdetermined"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../03_orthog/roch-mmids-ls-3orthog.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">2.3. </span>A key concept: orthogonality</p>
      </div>
    </a>
    <a class="right-next"
       href="../05_qr/roch-mmids-ls-5qr.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">2.5. </span>QR decomposition and Householder transformations</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-systems-and-inverses">2.4.1. Linear systems and inverses</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overdetermined-case">2.4.2. Overdetermined case</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#least-squares-via-cholesky">2.4.3. Least squares via Cholesky</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#back-to-regression-analysis">2.4.4. Back to regression analysis</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Sebastien Roch
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>