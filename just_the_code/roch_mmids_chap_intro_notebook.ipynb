{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05996271",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell",
     "colab-keep"
    ]
   },
   "source": [
    "***\n",
    "\n",
    "*Course:* [Math 535](https://people.math.wisc.edu/~roch/mmids/) - Mathematical Methods in Data Science (MMiDS)  \n",
    "*Chapter:* 1-Introduction   \n",
    "*Author:* [Sebastien Roch](https://people.math.wisc.edu/~roch/), Department of Mathematics, University of Wisconsin-Madison  \n",
    "*Updated:* Jan 25, 2024   \n",
    "*Copyright:* &copy; 2024 Sebastien Roch\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04890b9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# You will need the files:\n",
    "#     * mmids.py\n",
    "#     * penguins-measurements.csv\n",
    "#     * penguins-species.csv\n",
    "# from https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/tree/main/utils\n",
    "#\n",
    "# IF RUNNING ON GOOGLE COLAB (RECOMMENDED):\n",
    "# \"Upload to session storage\" from the Files tab on the left\n",
    "# Alternative instructions: https://colab.research.google.com/notebooks/io.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea6751b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# PYTHON 3\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import mmids\n",
    "seed = 535\n",
    "rng = np.random.default_rng(seed)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6654bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Motivating example: identifying penguin species"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9cd9cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Imagine that you are an evolutionary biologist studying penguins. You have collected measurements on a large number of individual specimens. Your goal is to identify different [species](https://en.wikipedia.org/wiki/Species) within this collection based on those measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791b8f02",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**Figure:** An Adelie penguin ([Source](https://commons.wikimedia.org/wiki/File:Hope_Bay-2016-Trinity_Peninsula%E2%80%93Ad%C3%A9lie_penguin_%28Pygoscelis_adeliae%29_04.jpg))\n",
    "\n",
    "![An Adelie penguin](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Hope_Bay-2016-Trinity_Peninsula–Adélie_penguin_%28Pygoscelis_adeliae%29_04.jpg/346px-Hope_Bay-2016-Trinity_Peninsula–Adélie_penguin_%28Pygoscelis_adeliae%29_04.jpg)\n",
    "\n",
    "$\\bowtie$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b26117",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Here is a [penguin dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set) collected and made available by [Dr. Kristen Gorman](https://www.uaf.edu/cfos/people/faculty/detail/kristen-gorman.php) and the [Palmer Station, Antarctica LTER](https://pallter.marine.rutgers.edu/). We will upload the data in the form of a data table (similar to a spreadsheet) called [`DataFrame`](https://pandas.pydata.org/docs/reference/frame.html) in [`pandas`](https://pandas.pydata.org/docs/), where the columns are different measurements (or features) and the rows are different samples. Below, we load the data using [`pandas.read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html?highlight=read_csv#) and show the first $5$ lines of the dataset (see [`DataFrame.head`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html)). This dataset is a simplified version (i.e., with some columns removed) of the full dataset, maintained by [Allison Horst](https://allisonhorst.com/) at this [GitHub page](https://github.com/allisonhorst/palmerpenguins/blob/main/README.md). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d49e030",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceaada6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('penguins-measurements.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4b92d0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Observe that this dataset has missing values (i.e., the entries `NaN` above). A common way to deal with this issue is to remove all rows with missing values. This can be done using [`pandas.DataFrame.dropna`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html). This kind of pre-processing is fundamental in data science, but we will not discuss it much in this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bbf006",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c1d43d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "There are $342$ samples, as can be seen by using [`pandas.DataFrame.shape`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.shape.html) which gives the dimensions of the DataFrame as a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56e964b",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16d199c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Here is a summary of the data (see [`pandas.DataFrame.describe`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0e764e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d045e2a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Let's first extract the columns into a Numpy array using [`pandas.DataFrame.to_numpy()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.values.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f28649a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = df.to_numpy()\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ed806a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We visualize two measurements in the data, the bill depth and flipper length. (The original dataset used the more precise term [culmen](https://en.wikipedia.org/wiki/Beak#Culmen) depth.) Below, each point is a sample. This is called a [scatter plot](https://en.wikipedia.org/wiki/Scatter_plot). We use [`matplotlib.pyplot`](https://matplotlib.org/stable/api/pyplot_summary.html) for most of our plotting needs in this book, with a few exceptions (see below). Specifically, here we use the function [`matplotlib.pyplot.scatter`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647ba059",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641f3fad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:,1], X[:,2], s=10)\n",
    "plt.xlabel('bill_depth_mm')\n",
    "plt.ylabel('flipper_length_mm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e9aef3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We observe what appears to be two fairly well-defined clusters of samples on the top left and bottom right respectively. What is a [cluster](https://en.wikipedia.org/wiki/Cluster_analysis)? Intuitively, it is a group of samples that are close to each other, but far from every other sample. In this case, it may be an indication that these samples come from a separate species."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a538fd7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Now let's look at the full dataset. Visualizing the full $4$-dimensional data is not straightforward. One way to do this is to consider all pairwise scatter plots. We use the function [`seaborn.pairplot`](https://seaborn.pydata.org/generated/seaborn.pairplot.html) from the library [Seaborn](https://seaborn.pydata.org/index.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6144b8c9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9ec4ef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df, height=1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe17e15",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERIC ANSWER:** How many species of penguins do you think there are in this dataset? $\\ddagger$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677f7f24",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "What would be useful is a method that *automatically* identifies clusters *whatever the dimension of the data*. In this chapter, we will discuss a standard way to do this: $k$-means clustering. We will come back to the penguins dataset later in the chapter. \n",
    "\n",
    "But first we need to review some basic concepts about vectors and distances in order to formulate clustering as an appropriate *optimization* problem, a perspective that will be recurring throughout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b38df64",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**LEARNING BY CHATTING** Ask your favorite AI chatbot for alternative ways to deal with missing values in a dataset. Implement one of these alternatives on the penguins dataset (ask the chatbot for the code). ([Open in Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_intro_notebook.ipynb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671edcdb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\newcommand{\\bmu}{\\boldsymbol{\\mu}}$ $\\newcommand{\\bSigma}{\\boldsymbol{\\Sigma}}$ $\\newcommand{\\E}{\\mathbb{E}}$ $\\newcommand{\\bX}{\\mathbf{X}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b88de22",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Background: quick refresher of matrix algebra, differential calculus, and elementary probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3187b3f4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** In Numpy, a vector is defined as a 1d array. We first must import the [Numpy](https://numpy.org) package, which is often abbreviated by `np`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b925284",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29df4f01",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "u = np.array([1., 3., 5. ,7.])\n",
    "print(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194bab60",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To obtain the norm of a vector, we can use the function [`linalg.norm`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html), which requires the [`numpy.linalg`](https://numpy.org/doc/stable/reference/routines.linalg.html) package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261df769",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e368031",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "LA.norm(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bd8eb2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "which we check next \"by hand\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217809e4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.sqrt(np.sum(u ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d532c936",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "In Numpy, [`**`](https://numpy.org/doc/stable/reference/generated/numpy.power.html) indicates element-wise exponentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887ebad7",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**TRY IT!** Compute the inner product of $u = (1,2,3,4)$ and $v = (5, 4, 3, 2)$ without using the function [`np.dot`](https://numpy.org/doc/stable/reference/generated/numpy.dot.html). *Hint*: The product of two real numbers $a$ and $b$ is `a * b`. ([Open in Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_intro_notebook.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ebbd43",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "u = np.array([1., 2., 3. ,4.])\n",
    "# EDIT THIS LINE: define v\n",
    "# EDIT THIS LINE: compute the inner product between u and v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954b95eb",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdb0314",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We will often work with collections of $n$ vectors $\\mathbf{x}_1, \\ldots, \\mathbf{x}_n$ in $\\mathbb{R}^d$ and it will be convenient to stack them up into a matrix\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{x}_1^T \\\\\n",
    "\\mathbf{x}_2^T \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{x}_n^T \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x_{11} & x_{12} & \\cdots & x_{1d} \\\\\n",
    "x_{21} & x_{22} & \\cdots & x_{2d} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{n1} & x_{n2} & \\cdots & x_{nd} \\\\\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47135b1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To create a matrix out of two vectors, we use the function [`numpy.stack`](https://numpy.org/doc/stable/reference/generated/numpy.stack.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1ed536",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "u = np.array([1., 3., 5., 7.])\n",
    "v = np.array([2., 4., 6., 8.])\n",
    "X = np.stack((u,v),axis=0)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0f1077",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Quoting the documentation:\n",
    "\n",
    "> The axis parameter specifies the index of the new axis in the dimensions of the result. For example, if axis=0 it will be the first dimension and if axis=-1 it will be the last dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646df992",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The same scheme still works with more than two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47852cbb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "u = np.array([1., 3., 5., 7.])\n",
    "v = np.array([2., 4., 6., 8.])\n",
    "w = np.array([9., 8., 7., 6.])\n",
    "X = np.stack((u,v,w))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c027d592",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e823a83",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** In Numpy, the Frobenius norm of a matrix can be computed using the function [`numpy.linalg.norm`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5693ce5b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A = np.array([[1., 0.],[0., 1.],[0., 0.]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1364d681",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "LA.norm(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c82ed3",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e08615",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** The function $f(x) = x^2$ over $\\mathbb{R}$ has a global minimizer at $x^* = 0$. Indeed, we clearly have $f(x) \\geq 0$ for all $x$ while $f(0) = 0$. To plot the function, we use the [matplotlib](https://matplotlib.org) package again, and specifically its function [`matplotlib.pyplot.plot`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html). We also use the function [`numpy.linspace`](https://numpy.org/doc/stable/reference/generated/numpy.linspace.html) to create an array of evenly spaced numbers where we evaluate $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cec5fc0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = np.linspace(-2,2,100)\n",
    "y = x ** 2\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b263d09",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The function $f(x) = e^x$ over $\\mathbb{R}$ does not have a global minimizer. Indeed, $f(x) > 0$ but no $x$ achieves $0$. And, for any $m > 0$, there is $x$ small enough such that $f(x) < m$. Note that $\\mathbb{R}$ is *not* bounded, therefore the *Extreme Value Theorem* does not apply here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100bc50e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-2,2,100)\n",
    "y = np.exp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a587f38b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.plot(x,y)\n",
    "plt.ylim(0,5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d90b6e5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The function $f(x) = (x+1)^2 (x-1)^2$ over $\\mathbb{R}$ has two global minimizers at $x^* = -1$ and $x^{**} = 1$. Indeed, $f(x) \\geq 0$ and $f(x) = 0$ if and only $x = x^*$ or $x = x^{**}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db02fad5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-2,2,100)\n",
    "y = ((x+1)**2) * ((x-1)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7167224e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.plot(x,y)\n",
    "plt.ylim(0,5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431a6749",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09dead4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We can use simulations to confirm the *Weak Law of Large Numbers*. Recall that a uniform random variable over the interval $[a,b]$ has density\n",
    "\n",
    "$$\n",
    "f_{X}(x)\n",
    "= \\begin{cases}\n",
    "\\frac{1}{b-a} & x \\in [a,b] \\\\\n",
    "0 & \\text{o.w.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "We write $X \\sim \\mathrm{U}[a,b]$. We can obtain a sample from $\\mathrm{U}[0,1]$ by using the function\n",
    "[`numpy.random.Generator.uniform`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.uniform.html). We must first instantiate a random number generator (RNG) with [`numpy.random.default_rng`](https://numpy.org/doc/stable/reference/random/generator.html) in Numpy. We provide a [seed](https://numpy.org/doc/stable/reference/random/bit_generators/index.html#seeding-and-entropy) as an initial state for the RNG. Using the same seed again ensures reproducibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996f38b4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "seed = 535\n",
    "rng = np.random.default_rng(seed)\n",
    "rng.uniform()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f6be19",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Now we take $n$ samples from $\\mathrm{U}[0,1]$ and compute their sample mean. We repeat $k$ times and display the empirical distribution of the sample means using an [histogram](https://en.wikipedia.org/wiki/Histogram)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00347c49",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def lln_unif(n, k):\n",
    "    sample_mean = [np.mean(rng.random(n)) for i in range(k)]\n",
    "    plt.hist(sample_mean,bins=15)\n",
    "    plt.xlim(0,1)\n",
    "    plt.title(f'n={n}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bd893b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We start with $n=10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521c0c55",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "lln_unif(10, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c606c444",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Taking $n$ much larger leads to more concentration around the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2641401",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "lln_unif(100, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5fcee1",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**TRY IT!** Recall that the cumulative distribution function (CDF) of a random variable $X$ is defined as \n",
    "\n",
    "$$\n",
    "F_X(z) = \\mathbb{P}[X \\leq z], \\qquad \\forall z \\in \\mathbb{R}.\n",
    "$$\n",
    "\n",
    "a) Let $\\mathcal{Z}$ be the interval where $F_X(z) \\in (0,1)$ and assume that $F_X$ is strictly increasing on $\\mathcal{Z}$. Let $U \\sim \\mathrm{U}[0,1]$. Show that \n",
    "\n",
    "$$\n",
    "\\mathbb{P}[F_X^{-1}(U) \\leq z] = F_X(z).\n",
    "$$\n",
    "\n",
    "b) Generate a sample from $\\mathrm{U}[a,b]$ for arbitrary $a$, $b$ using `numpy.random.Generator.uniform` and the observation in a). This is called the inverse transform sampling method. ([Open in Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_intro_notebook.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8a8ca0",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "a = -1\n",
    "b = 1\n",
    "X = rng.uniform()\n",
    "# EDIT THIS LINE: transform X to obtain a random variable Y ~ U[a,b]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36dc6b5",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b9d438",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We plot the PDF of a standard normal distribution. We use the function [scipy.stats.norm](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html) from the [SciPy library](https://scipy.org), which outputs the PDF. The following code was adapted from [here](https://commons.wikimedia.org/wiki/File:Standard_Normal_Distribution.svg) with the help of ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a28f737",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "# Plot the normal distribution curve\n",
    "x = np.linspace(-4, 4, 100)\n",
    "y = norm.pdf(x)\n",
    "plt.plot(x, y, color='black')\n",
    "\n",
    "# Fill areas under the curve for different standard deviations\n",
    "plt.fill_between(x, y, where=(x > -1) & (x < 1), color='red', alpha=0.25)\n",
    "plt.fill_between(x, y, where=(x > -2) & (x < 2), color='red', alpha=0.25)\n",
    "plt.hlines(norm.pdf(1), -1, 1, color='black', linestyle='dashed')\n",
    "plt.hlines(norm.pdf(2), -2, 2, color='black', linestyle='dashed')\n",
    "plt.text(0, norm.pdf(1) + 0.01, \"68.3%\", ha='center')\n",
    "plt.text(0, norm.pdf(2) + 0.01, \"95.4%\", ha='center')\n",
    "\n",
    "# Set labels, title, and xticks\n",
    "plt.xlabel(\"Standard Deviations from Mean\")\n",
    "plt.ylabel(\"PDF\")\n",
    "plt.xticks(range(-4, 5, 2), [f'{i}' for i in range(-4, 5, 2)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d96a2b0",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d0cfce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** The following function generates $n$ data points from a spherical $d$-dimensional Gaussians with variance $\\sigma^2$ and mean $\\bmu$.\n",
    "\n",
    "Below, [`rng.normal(0,1,d)`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.normal.html) generates a `d`-dimensional spherical Gaussian with mean $\\mathbf{0}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986b3b01",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def spherical_gaussian(d, n, mu, sig):\n",
    "    X = np.stack(\n",
    "        [mu + (sig ** 2) * rng.normal(0,1,d) for _ in range(n)]\n",
    "    )\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acea4c6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We generate $100$ data points in dimension $d=2$. We take $\\sigma^2 = 1$ and $\\bmu = w \\mathbf{e}_1$. Below we use the function [`numpy.concatenate`](https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html) to create a vector by concatenating two given vectors. We use `[w]` to create a vector with a single entry `w`.  We also use the function [`numpy.zeros`](https://numpy.org/doc/stable/reference/generated/numpy.zeros.html) to create an all-zero vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d358afdd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "d, n, w = 2, 100, 3.\n",
    "sig = 1\n",
    "mu = np.concatenate(([w], np.zeros(d-1)))\n",
    "X = spherical_gaussian(d, n, mu, sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d0cbb2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1], marker='x')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9090697d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935bd9e4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** This is straightforward to implement by using [`numpy.random.Generator.choice`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.choice.html) to choose the component of each sample. For convenience, we will stack the means and covariances into one array with a new dimension. So, for instance, the covariance matrices will now be in a 3d-array, that is, an array with $3$ indices. The first index corresponds to the component (here $0$ or $1$). Here is an example (where we use [`numpy.identity`](https://numpy.org/doc/stable/reference/generated/numpy.identity.html) to produce an identity matrix):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485d2a29",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sigma1 = np.identity(d)\n",
    "sigma2 = 0.5 * np.identity(d)\n",
    "sigma = np.stack((sigma1,sigma2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640b581f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(sigma[0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9ca868",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(sigma[1,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864fa53b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Here is a visualization borrowed from the [TensorFlow tutorial](https://www.tensorflow.org/guide/tensor#basics)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4710292",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**Figure:** Three matrices ([Source](https://www.tensorflow.org/guide/tensor#basics))\n",
    "\n",
    "![Three matrices](https://www.tensorflow.org/static/guide/images/tensor/3-axis_numpy.png)\n",
    "\n",
    "$\\bowtie$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853d46f5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**Figure:** Three matrices stacked into a 3d-array ([Source](https://www.tensorflow.org/guide/tensor#basics))\n",
    "\n",
    "\n",
    "![Three matrices stacked into a tensor](https://www.tensorflow.org/static/guide/images/tensor/3-axis_front.png)\n",
    "\n",
    "$\\bowtie$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260a4cb4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The code is the following. It returns an `d` by `n` array `X`, where each row is a sample from a 2-component Gaussian mixture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed80fb5c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def gmm2(d, n, phi1, phi2, mu1, sigma1, mu2, sigma2):\n",
    "    \n",
    "    # merge components into tensors\n",
    "    phi = np.stack((phi1, phi2))\n",
    "    mu = np.stack((mu1, mu2))\n",
    "    sigma = np.stack((sigma1,sigma2))\n",
    "    X = np.zeros((n,d))\n",
    "    \n",
    "    # choose components of each data point, then generate samples\n",
    "    component = rng.choice(2, size=n, p=[phi1, phi2])\n",
    "    for i in range(n):\n",
    "        X[i,:] = rng.multivariate_normal(\n",
    "            mu[component[i],:],\n",
    "            sigma[component[i],:,:])\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fade73e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Let us try it with following parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b9d599",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "d, n, w = 2, 1000, 3.\n",
    "phi1 = 0.9\n",
    "phi2 = 0.1\n",
    "mu1 = np.concatenate(([w], np.zeros(d-1)))\n",
    "mu2 = np.concatenate(([-w], np.zeros(d-1)))\n",
    "sigma1 = np.identity(d)\n",
    "sigma2 = 0.5 * np.identity(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1956c167",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "outputs": [],
   "source": [
    "X = gmm2(d, n, phi1, phi2, mu1, sigma1, mu2, sigma2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0685711",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1], marker='x')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aba8ec7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "As expected, we observe two clusters. The one on the right (component $0$) is denser (i.e., it contains more data points) since `phi0` is much larger than `phi1`. It is also larger as its variances (i.e., the diagonal entries of its covariance matrix) are larger. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02ec963",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47995937",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Clustering: an objective, an algorithm and a guarantee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa036c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** Here's a numerical example. We first define a quadratic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b27244",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def q(a, b, c, x):\n",
    "    return a * (x ** 2) + b * x + c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd941456",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We plot it for different values of the coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6804588e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-2, 2, 100)\n",
    "\n",
    "plt.plot(x, q(2, 4, -1, x))\n",
    "plt.plot(x, q(2, -4, 4, x))\n",
    "plt.plot(x, q(-2, 0, 4, x))\n",
    "\n",
    "plt.legend(['y1', 'y2', 'y3'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e975cd09",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c4781b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We are now ready to describe the <a href=\"https://en.wikipedia.org/wiki/K-means_clustering\">$k$-means algorithm</a>, also known as Lloyd's algorithm. We start from a random assignment of clusters. (An alternative [initialization strategy](https://en.wikipedia.org/wiki/K-means_clustering#Initialization_methods) is to choose $k$ representatives at random among the data points.) We then alternate between the optimal choices in the lemmas. In lieu of pseudo-code, we write out the algorithm in Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050aca70",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The input `X` is assumed to be a collection of $n$ vectors $\\mathbf{x}_1, \\ldots, \\mathbf{x}_n \\in \\mathbb{R}^d$ stacked into a matrix, with one row for each data point. The other input, `k`, is the desired number of clusters. There is an optional input `maxiter` for the maximum number of iterations, which is set to $10$ by default.\n",
    "\n",
    "We first define separate functions for the two main steps. To find the minimum of an array, we use the function [`numpy.argmin`](https://numpy.org/doc/stable/reference/generated/numpy.argmin.html). We also use [`numpy.linalg.norm`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html) to compute the Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df86536",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def opt_reps(X, k, assign):\n",
    "    (n, d) = X.shape\n",
    "    reps = np.zeros((k, d))\n",
    "    for i in range(k):\n",
    "        in_i = [j for j in range(n) if assign[j] == i]             \n",
    "        reps[i,:] = np.sum(X[in_i,:],axis=0) / len(in_i)\n",
    "    return reps\n",
    "\n",
    "def opt_clust(X, k, reps):\n",
    "    (n, d) = X.shape\n",
    "    dist = np.zeros(n)\n",
    "    assign = np.zeros(n, dtype=int)\n",
    "    for j in range(n):\n",
    "        dist_to_i = np.array([LA.norm(X[j,:] - reps[i,:]) for i in range(k)])\n",
    "        assign[j] = np.argmin(dist_to_i)\n",
    "        dist[j] = dist_to_i[assign[j]]\n",
    "    G = np.sum(dist ** 2)\n",
    "    print(G)\n",
    "    return assign"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96e3045",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The main function follows. Below, `rng.integers(0,k,n)` is an array of `n` uniformly chosen integers between `0` and `k-1` (inclusive). (See [random.Generator.integers](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.integers.html) for details.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56dabda",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def kmeans(X, k, maxiter=10):\n",
    "    (n, d) = X.shape\n",
    "    assign = rng.integers(0,k,n)\n",
    "    reps = np.zeros((k, d), dtype=int)\n",
    "    for iter in range(maxiter):\n",
    "        # Step 1: Optimal representatives for fixed clusters\n",
    "        reps = opt_reps(X, k, assign) \n",
    "        # Step 2: Optimal clusters for fixed representatives\n",
    "        assign = opt_clust(X, k, reps) \n",
    "    return assign"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d3f39f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**LEARNING BY CHATTING** AI chatbots can serve as great personal tutors. For instance, they can provide additional information about the code in this book. Just copy-paste a piece of code and ask \"What is this Python code doing?\" Don't hesitate to ask follow-up questions. [Here is an example using ChatGPT 4](https://chat.openai.com/share/66f9e704-c8e8-4489-9ab6-8e57740cab3b). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc707346",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "*Warning:* As you probably know, AI chatbots can be wrong so assess what they return with a critical mind and/or double-check with other sources (e.g., package documentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47008fd",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "*Prompt engineering:* [Finding the right prompt](https://en.wikipedia.org/wiki/Prompt_engineering), or series of prompts, can make a big difference. If it helps you, here is [an explanation of our $k$-means algorithm in the words of Yoda via ChatGPT 4](https://chat.openai.com/share/0c4e1f8f-83ca-4dcb-afdc-aacfbf0d4fd8):\n",
    "\n",
    "> Ah, young Padawan, what you see before you is the ancient algorithm of K-means, it is, in the language of Python. Three parts, this code has, each crucial for learning the ways of data clustering.\n",
    "> \n",
    "> 1. **Optimal Representatives, Find We Must:** `opt_reps` function, this is. Given data points in space and their cluster assignments, it calculates the center, or representative, of each cluster. The Force of averaging, it uses, to find the center of all points belonging to each cluster. Balance in each cluster, this brings.\n",
    "> 2. **Optimal Clusters, Seek We Shall:** `opt_clust` function, this is. With the cluster centers known, it assigns each data point to the nearest cluster center. The closest center to each point, it finds, using the Force of distance calculation. This creates groups, or clusters, of similar points.\n",
    "> 3. **Harmony in Iteration, Achieve We Will:** `kmeans` function, the heart of the algorithm, this is. Starting with random cluster assignments, it iteratively seeks balance. First, finding the best centers for the current clusters, then finding the best clusters for these centers. Repeat this process it does, until convergence or a maximum number of iterations reached.\n",
    ">\n",
    "> In your journey to understand data, a powerful ally, this K-means algorithm is. But remember, Padawan, the right number of clusters (k) and good initial points, important they are, for the success of your quest. Use this wisdom well, you must."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74586d3d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We apply our implementation of $k$-means to the example above. We fix `k` to $3$. Here the data matrix `X` is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cb26b9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([[1., 0.],[-2., 0.],[-2.,1.],[1.,-3.],\n",
    "              [-10.,10.],[2.,-2.],[-3.,1.],[3.,-1.]])\n",
    "assign = kmeans(X, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8bc6de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We vizualize the output by coloring the points according to their cluster assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99af666b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1], c=assign)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dcad2c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We can compute the final representatives (optimal for the final assignment) by using the subroutine `opt_reps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d5e9e2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(opt_reps(X, 3, assign))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01642da7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Each row is the center of the corresponding cluster. Note these match with the ones we previously computed. Indeed, the clustering is the same (although not necessarily in the same order)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e2e849",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292e7168",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We will test our implementation of $k$-means on the penguins dataset introduced earlier in the chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77360b1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We first extract the columns and combine them into a data matrix `X`. As we did previously, we also remove the rows with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef311416",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('penguins-measurements.csv')\n",
    "df = df.dropna()\n",
    "X = df[['bill_length_mm', 'bill_depth_mm', \n",
    "        'flipper_length_mm', 'body_mass_g']].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434a810a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We  visualize a two-dimensional slice of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f73965f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:,1], X[:,3], s=10)\n",
    "plt.xlabel('bill_depth_mm')\n",
    "plt.ylabel('body_mass_g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47a17ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Observe that the features have quite different scales (tens versus thousands in the plot above). In such a case, it is common to standardize the data so that each feature has roughly the same scale. That is accomplished by, for each column of `X`, subtracting its empirical mean and dividing by its empirical standard deviation. We do this next. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a83c41e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mean = np.mean(X, axis=0)  # Compute mean for each column\n",
    "std = np.std(X, axis=0)  # Compute standard deviation for each column\n",
    "X = (X - mean) / std # Standardize each column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf2ac1c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Now we run the $k$-means algorithm with $k=2$ clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e78131",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "assign = kmeans(X, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89e2144",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We vizualize the output as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9141441e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:,1], X[:,3], c=assign, s=10)\n",
    "plt.xlabel('bill_depth_mm (standardized)')\n",
    "plt.ylabel('body_mass_g (standardized)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a015fdb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "This clustering looks quite good. Nevertheless recall that:\n",
    "\n",
    "1. in this plot we are looking at only two of the four variables while $k$-means uses all of them, \n",
    "\n",
    "2. we are not guaranteed to find the best solution, \n",
    "\n",
    "3. our objective function is somewhat arbitrary, and \n",
    "\n",
    "4. it is not clear what the right choice of $k$ is. \n",
    "\n",
    "In fact, the original dataset provided the correct answer. Despite what the figure above may lead us to believe, there are in reality three separate species. So let's try with $k=3$ clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f35f492",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "assign = kmeans(X, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb5718d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The output does not seem quite right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79fbb7f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:,1], X[:,3], c=assign, s=10)\n",
    "plt.xlabel('bill_depth_mm (standardized)')\n",
    "plt.ylabel('body_mass_g (standardized)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6c4fa1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "But, remembering the warnings mentioned previously, let's look at a different two-dimensional slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf1caef",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,3], c=assign, s=10)\n",
    "plt.xlabel('bill_length_mm (standardized)')\n",
    "plt.ylabel('body_mass_g (standardized)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f7e72f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Let's load up the truth and compare. We only keep those samples that were not removed because of missing values (see [`pandas.DataFrame.iloc`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea219ea2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_truth = pd.read_csv('penguins-species.csv') \n",
    "df_truth = df_truth.iloc[df.index]\n",
    "df_truth.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2985243",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The species are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b0a2b1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "species = df_truth['species']\n",
    "print(species.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d064b22c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To plot the outcome, we color the species blue-green-red using a [dictionary](https://docs.python.org/3/tutorial/datastructures.html#dictionaries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e24a2f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "species2color = {'Adelie': 'b', 'Chinstrap': 'g', 'Gentoo': 'r'}\n",
    "truth = species.replace(species2color)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a3088b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Finally, we can compare the output to the truth. The match is quite excellent -- but not perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba643d6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "ax1.scatter(X[:,0], X[:,3], c=truth, s=3)\n",
    "ax1.set_title('truth')\n",
    "ax2.scatter(X[:,0], X[:,3], c=assign, s=3)\n",
    "ax2.set_title('kmeans')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e41c36e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "It is informative to revisit the all pairwise scatter plots using [`seaborn.pairplot`](https://seaborn.pydata.org/generated/seaborn.pairplot.html), updated with this new information via its option `hue`. We first concatenate the dataframes with [`pandas.concat`](https://pandas.pydata.org/docs/reference/api/pandas.concat.html#pandas.concat)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f00e9d2",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_full = pd.concat([df, df_truth], axis=1)\n",
    "df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827eab2f",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.pairplot(df_full, hue=\"species\", diag_kind=\"hist\", height=1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3861eff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Determining the appropriate number of clusters is not a straighforward problem. To quote [Wikipedia](https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set):\n",
    "\n",
    "> The correct choice of $k$ is often ambiguous, with interpretations depending on the shape and scale of the distribution of points in a data set and the desired clustering resolution of the user. In addition, increasing $k$ without penalty will always reduce the amount of error in the resulting clustering, to the extreme case of zero error if each data point is considered its own cluster (i.e., when $k$ equals the number of data points, $n$). Intuitively then, the optimal choice of $k$ will strike a balance between maximum compression of the data using a single cluster, and maximum accuracy by assigning each data point to its own cluster. If an appropriate value of $k$ is not apparent from prior knowledge of the properties of the data set, it must be chosen somehow. There are several categories of methods for making this decision.\n",
    "\n",
    "\n",
    "In practice, [several heuristics](https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set) are in use. Other approaches to clustering, e.g. [DBSCAN](https://en.wikipedia.org/wiki/DBSCAN) and [hierarchical clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering), do not require a number of clusters as input. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97abf1f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**TRY IT!** Run the analysis again, but this time *without the standardization step*. What do you observe? Is one feature more influential on the final output than the others? Why do you think that is? ([Open in Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_intro_notebook.ipynb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f613e9fa",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**TRY IT!** Modify `kmeans` to take a tolerance `tol` as input and stop when the improvement in objective value `G` falls below the tolerance. ([Open in Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_intro_notebook.ipynb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3736fd7f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b64011f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Some observations about high-dimensional data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cafc5dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "In this section, we test our implementation of $k$-means on a simple simulated dataset in high dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0bc2d3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The following function generates $n$ data points from a mixture of two equally likely, spherical $d$-dimensional Gaussians with variance $1$, one with mean $-w\\mathbf{e}_1$ and one with mean $w \\mathbf{e}_1$. We use `gmm2` from a previous section. It is found in `mmids.py`, which can be downloaded [here](https://raw.githubusercontent.com/MMiDS-textbook/MMiDS-textbook.github.io/main/utils/mmids.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf7dbad",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def two_mixed_clusters(d, n, w):\n",
    "    \n",
    "    # set parameters\n",
    "    phi1 = 0.5\n",
    "    phi2 = 0.5\n",
    "    mu1 = np.concatenate(([w], np.zeros(d-1)))\n",
    "    mu2 = np.concatenate(([-w], np.zeros(d-1)))\n",
    "    sigma1 = np.identity(d)\n",
    "    sigma2 = np.identity(d)\n",
    "    \n",
    "    return mmids.gmm2(d, n, phi1, phi2, mu1, sigma1, mu2, sigma2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020efe5d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**Two dimensions** We start with $d=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c08bd6a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "d, n, w = 2, 100, 3.\n",
    "X = two_mixed_clusters(d, n, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f988766a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We use a scatterplot to visualize the data. Each dot corresponds to one data point. Observe the two clearly delineated clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19598e80",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111,aspect='equal')\n",
    "ax.scatter(X[:,0], X[:,1], s=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4174f12b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Let's run $k$-means on this dataset using $k=2$. We use `kmeans()` from the `mmids.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c694cbc0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "assign = mmids.kmeans(X, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a831e350",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Our default of $10$ iterations seem to have been enough for the algorithm to converge. We can visualize the result by [coloring](https://matplotlib.org/stable/api/_as_gen/matplotlib.lines.Line2D.html) the points according to the assignment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0156baa1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111,aspect='equal')\n",
    "ax.scatter(X[:,0], X[:,1], c=assign, s=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8af526b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**General dimension** Let's see what happens in higher dimension. We repeat our experiment with $d=1000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0fe0a0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "outputs": [],
   "source": [
    "d, n, w = 1000, 100, 3.\n",
    "X = two_mixed_clusters(d, n, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dbb4c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Again, we observe two clearly delineated clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8a484b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111,aspect='equal')\n",
    "ax.scatter(X[:,0], X[:,1], s=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4847ed7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "This dataset is in $1000$ dimensions, but we've plotted the data in only the first two dimensions. If we plot in any two dimensions not including the first one instead, we see only one cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ea2f5e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111,aspect='equal')\n",
    "ax.scatter(X[:,1], X[:,2], s=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fc3e08",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Let's see how $k$-means fares on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fdfb4b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "assign = mmids.kmeans(X, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569eda20",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Our attempt at clustering does not appear to have been successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df591af",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111,aspect='equal')\n",
    "ax.scatter(X[:,0], X[:,1], c=assign, s=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b0558e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Our attempt at clustering does not appear to have been successful. What happened? While these clusters are easy to tease apart *if we know to look at the first coordinate only*, in the full space the within-cluster and between-cluster distances become harder to distinguish: the noise overwhelms the signal. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea651b51",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The function below plots the histograms of within-cluster and between-cluster distances for a sample of size $n$ in $d$ dimensions with a given offset. As $d$ increases, the two distributions become increasingly indistinguishable. Later in the course, we will develop dimension-reduction techniques that help deal with this issue. This time, we need labeled data points, i.e., we need to know the component from which each data point comes from. For this purpose, we implement a variant of our previous mixture simulator. This one generates -- separately -- each Gaussian and concatenates the data. So the first `n` samples are from one component and the next `n` samples are from the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3d3099",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def one_cluster(d, n, w):\n",
    "    X = np.stack(\n",
    "        [np.concatenate(([w], np.zeros(d-1))) + rng.normal(0,1,d) for _ in range(n)]\n",
    "    )\n",
    "    return X\n",
    "\n",
    "def two_clusters(d, n, w):\n",
    "    X1 = one_cluster(d, n, -w)\n",
    "    X2 = one_cluster(d, n, w)\n",
    "    return X1, X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43929470",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def highdim_2clusters(d, n, w):\n",
    "    # generate datasets\n",
    "    X1, X2 = two_clusters(d, n, w)\n",
    "    \n",
    "    # within-cluster distances for X1\n",
    "    intra = np.stack(\n",
    "        [LA.norm(X1[i,:] - X1[j,:]) for i in range(n) for j in range(n) if j>i]\n",
    "    )\n",
    "    plt.hist(intra, density=True, label='within-cluster')\n",
    "    plt.title(f'dim={d}')\n",
    " \n",
    "    # between-cluster distances\n",
    "    inter = np.stack(\n",
    "        [LA.norm(X1[i,:] - X2[j,:]) for i in range(n) for j in range(n)]\n",
    "    )\n",
    "    plt.hist(inter, density=True, alpha=0.75, label='between-cluster')\n",
    "\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(f'dim={d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efe3310",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Next we plot the results for dimensions $d=2, 100, 1000$. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb7e9f1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "highdim_2clusters(2, 100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb8c50e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "highdim_2clusters(100, 100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb595d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "highdim_2clusters(1000, 100, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54f7b92",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "As the dimension increases, the distributions of intra-cluster and inter-cluster distances overlap significantly and become more or less indistinguishable. That provides some insights into why clustering may fail here. Note that we used the same offset for all simulations. On the other hand, if the separation between the clusters is sufficiently large, one would expect clustering to work even in high dimension. \n",
    "\n",
    "**TRY IT!** What precedes (and what follows in the next subsection) is not a formal proof that $k$-means clustering will be unsuccessful here. The behavior of the algorithm is quite complex and depends, in particular, on the initialization and the density of points. Here, increasing the number of data points eventually leads to a much better performance. Explore this behavior on your own by modifying the code. (For some theoretical justifications (beyond this course), see [here](https://arxiv.org/pdf/0912.0086.pdf) and [here](http://www.stat.yale.edu/~pollard/Papers/Pollard81AS.pdf).)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7827339d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We can check the theorem in a simulation. Here we pick $n$ points uniformly at random in the $d$-cube $\\mathcal{C}$, for a range of dimensions up to `dmax`. We then plot the frequency of landing in the inscribed $d$-ball $\\mathcal{B}$ and see that it rapidly converges to $0$. Alternatively, we could just plot the formula for the volume of $\\mathcal{B}$. But knowing how to do simulations is useful in situations where explicit formulas are unavailable or intractable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3679abf2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def highdim_cube(dmax, n):\n",
    "    \n",
    "    in_ball = np.zeros(dmax)\n",
    "    for d in range(dmax):\n",
    "        # recall that d starts at 0 so we add 1 below\n",
    "        in_ball[d] = np.mean(\n",
    "            [(LA.norm(rng.random(d+1) - 1/2) < 1/2) for _ in range(n)]\n",
    "        )\n",
    "    \n",
    "    plt.plot(np.arange(1,dmax+1), in_ball) \n",
    "    plt.xlabel('dim')\n",
    "    plt.ylabel('in-ball freq')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfef0f4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We plot the result up to dimension $10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ec0235",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "highdim_cube(10, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb6a4d2",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2076902",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We check our claim in a simulation. We generate standard Normal $d$-vectors using the `rng.normal(0,1,d)` function and plot the histogram of their $2$-norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e7eff3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def normal_shell(d, n):\n",
    "    one_sample_norm = [LA.norm(rng.normal(0,1,d)) for _ in range(n)]\n",
    "    plt.hist(one_sample_norm, bins=20)\n",
    "    plt.xlim=(0,np.stack(one_sample_norm).max())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aded8ff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We first plot it in one dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affaedbb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "normal_shell(1, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d9c5dc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "In higher dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3bb165",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "normal_shell(100, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f1ce99",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
