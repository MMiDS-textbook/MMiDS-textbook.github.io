{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07c0d1ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell",
     "colab-keep"
    ]
   },
   "source": [
    "***\n",
    "\n",
    "*Course:* [Math 535](https://people.math.wisc.edu/~roch/mmids/) - Mathematical Methods in Data Science (MMiDS)  \n",
    "*Chapter:* 3-Singular value decomposition   \n",
    "*Author:* [Sebastien Roch](https://people.math.wisc.edu/~roch/), Department of Mathematics, University of Wisconsin-Madison  \n",
    "*Updated:* Jan 5, 2024   \n",
    "*Copyright:* &copy; 2024 Sebastien Roch\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d743652d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# IF RUNNING ON GOOGLE COLAB, UNCOMMENT THE FOLLOWING CODE CELL\n",
    "# When prompted, upload: \n",
    "#     * mmids.py\n",
    "#     * h3n2-snp.csv\n",
    "#     * h3n2-other.csv \n",
    "#     * advertising.csv \n",
    "# from your local file system\n",
    "# Files at: https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/tree/main/utils\n",
    "# Alternative instructions: https://colab.research.google.com/notebooks/io.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a49cdd5",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell",
     "colab-uncomment"
    ]
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7f2afd",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# PYTHON 3\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "from numpy.random import default_rng\n",
    "rng = default_rng(535)\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import mmids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd43655",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Motivating example: vizualizing genetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7772ea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We consider an application of dimensionality reduction in biology. We will look at SNP data from viruses. A little background first. From [Wikipedia](https://en.wikipedia.org/wiki/Single-nucleotide_polymorphism):\n",
    "\n",
    "> A single-nucleotide polymorphism (SNP; /snɪp/; plural /snɪps/) is a substitution of a single nucleotide that occurs at a specific position in the genome, where each variation is present at a level of more than 1% in the population. For example, at a specific base position in the human genome, the C nucleotide may appear in most individuals, but in a minority of individuals, the position is occupied by an A. This means that there is a SNP at this specific position, and the two possible nucleotide variations – C or A – are said to be the alleles for this specific position."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5017390a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Quoting [Jombart et al., BMC Genetics (2010)](https://bmcgenet.biomedcentral.com/articles/10.1186/1471-2156-11-94), we analyze:\n",
    "\n",
    "> the population structure of seasonal influenza A/H3N2 viruses using hemagglutinin (HA) sequences. Changes in the HA gene are largely responsible for immune escape of the virus (antigenic shift), and allow seasonal influenza to persist by mounting yearly epidemics peaking in winter. These genetic changes also force influenza vaccines to be updated on a yearly basis. [...] Assessing the genetic evolution of a pathogen through successive epidemics is of considerable epidemiological interest. In the case of seasonal influenza, we would like to ascertain how genetic changes accumulate among strains from one winter epidemic to the next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f5fe7b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Some details about the Jombart et al. dataset:\n",
    "\n",
    "> For this purpose, we retrieved all sequences of H3N2 hemagglutinin (HA) collected between 2001 and 2007 available from Genbank. Only sequences for which a location (country) and a date (year and month) were available were retained, which allowed us to classify strains into yearly winter epidemics. Because of the temporal lag between influenza epidemics in the two hemispheres, and given the fact that most available sequences were sampled in the northern hemisphere, we restricted our analysis to strains from the northern hemisphere (latitudes above 23.4°north). The final dataset included 1903 strains characterized by 125 SNPs which resulted in a total of 334 alleles. All strains from 2001 to 2007 were classified into six winter epidemics (2001-2006). This was done by assigning all strains from the second half of the year with those from the first half of the following year. For example, the 2005 winter epidemic comprises all strains collected between the 1st of July 2005 and the 30th of June 2006."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d86f93e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We load a dataset, which contains a subset of strains from the dataset mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcc4a02",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('h3n2-snp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3debd764",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The first five rows are the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2701a1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a146e6a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Overall it contains $1642$ strains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7d0290",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ab629d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The data lives in a $318$-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0ef3fa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d74b32d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Obviously, vizualizing this data is not straighforward. How can we make sense of it? More specifically, how can we explore any underlying structure it might have. Quoting [Wikipedia](https://en.wikipedia.org/wiki/Exploratory_data_analysis):\n",
    "\n",
    "> In statistics, exploratory data analysis (EDA) is an approach of analyzing data sets to summarize their main characteristics, often using statistical graphics and other data visualization methods. [...] Exploratory data analysis has been promoted by John Tukey since 1970 to encourage statisticians to explore the data, and possibly formulate hypotheses that could lead to new data collection and experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826f5feb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "In this chapter we will encounter an importatn mathematical technique for dimension reduction, which allow us to explore this data -- and find interesting structure -- in $2$ (rather than $318$!) dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb79382b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Background: review of spectral decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5285b59a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** In Python, the eigenvalues and eigenvectors of a matrix can be computed using [`numpy.linalg.eig`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed52d975",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "A = np.array([[2.5, -0.5], [-0.5, 2.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0467c7",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "w, v = LA.eig(A)\n",
    "print(w)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc3f4e6",
   "metadata": {
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ceb8c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Approximating subspaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03120b2d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** In Numpy, the outer product is computed using [`numpy.outer`](https://numpy.org/doc/stable/reference/generated/numpy.outer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da763706",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "u = np.array([0., 2., -1.])\n",
    "v = np.array([3., -2.])\n",
    "Z = np.outer(u, v)\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239b4a0f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print(LA.matrix_rank(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f358d0a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16206de6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Power iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06691f3c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We implement the algorithm suggested by the *Power Iteration Lemma*. That is, we compute $B^{k} \\mathbf{x}$, then normalize it. To obtain the corresponding singular value and left singular vector, we use that $\\sigma_1 = \\|A \\mathbf{v}_1\\|$ and $\\mathbf{u}_1 = A \\mathbf{v}_1/\\sigma_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148959b1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def topsing(A, maxiter=10):\n",
    "    x = rng.normal(0,1,np.shape(A)[1])\n",
    "    B = A.T @ A\n",
    "    for _ in range(maxiter):\n",
    "        x = B @ x\n",
    "    v = x / LA.norm(x)\n",
    "    s = LA.norm(A @ v)\n",
    "    u = A @ v / s\n",
    "    return u, s, v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506e0f63",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We will apply it to our previous two-cluster example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378ecb10",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "d, n, w = 10, 100, 3.\n",
    "X1, X2 = mmids.two_clusters(d, n, w)\n",
    "X = np.concatenate((X1, X2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1d8f37",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111,aspect='equal')\n",
    "ax.scatter(X[:,0], X[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20c9018",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Let's compute the top singular vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd370b66",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "u, s, v = topsing(X)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfd8c2d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "This is approximately $-\\mathbf{e}_1$. We get roughly the same answer (possibly up to sign) from Python's [`numpy.linalg.svd`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcfd904",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "u, s, vh = LA.svd(X)\n",
    "print(vh.T[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3cd7d4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Recall that, when we applied $k$-means clustering to this example with $d=1000$ dimension, we obtained a very poor clustering. Let's try again after projecting onto the top singular vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48de6203",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "d, n, w = 1000, 100, 3.\n",
    "X1, X2 = mmids.two_clusters(d, n, w)\n",
    "X = np.concatenate((X1, X2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb42196",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111,aspect='equal')\n",
    "ax.scatter(X[:,0], X[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22140e63",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "assign = mmids.kmeans(X, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8306ba6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111,aspect='equal')\n",
    "ax.scatter(X[:,0], X[:,1], c=assign)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97aef92c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Let's try again, but after projecting on the top singular vector. Recall that this corresponds to finding the best one-dimensional approximating subspace. The projection can be computed using the truncated SVD $Z= U_{(1)} \\Sigma_{(1)} V_{(1)}^T$. We can interpret the rows of $U_{(1)} \\Sigma_{(1)}$ as the coefficients of each data point in the basis $\\mathbf{v}_1$. We will work in that basis. We need one small hack: because our implementation of $k$-means clustering expects data points in at least $2$ dimension, we add a column of $0$'s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c019803",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "u, s, v = topsing(X)\n",
    "Xproj = np.stack((u*s, np.zeros(np.shape(X)[0])), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f726158a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111,aspect='equal')\n",
    "ax.scatter(Xproj[:,0], Xproj[:,1])\n",
    "plt.ylim([-3,3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6821e0c5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "There is a small - yet noticeable - gap around 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33f7680",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "A histogram of the first component of `Xproj` gives a better sense of the density of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37811df6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.hist(Xproj[:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56d9e8f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We run $k$-means clustering on the projected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dcb16a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "assign = mmids.kmeans(Xproj, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6be7e23",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111,aspect='equal')\n",
    "ax.scatter(X[:,0], X[:,1], c=assign)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5efe73",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Much better. We will give an explanation of this outcome in an upcoming (optional) subsection. In essence, quoting [BHK, Section 7.5.1]:\n",
    "\n",
    "> [...] let's understand the central advantage of doing the projection to [the top $k$ right singular vectors]. It is simply that for any reasonable (unknown) clustering of data points, the projection brings data points closer to their cluster centers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a58b8c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Finally, looking at the top right singular vector (or its first ten entries for lack of space), we see that it does align quite well (but not perfectly) with the first dimension. In the next (optional) section, we try again with the top two singular vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844682b2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(v[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92d7c6a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd87e93",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We implement this last algorithm. We will need our previous implementation of *Gram-Schimdt*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37bcfe7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def svd(A, l, maxiter=100):\n",
    "    V = rng.normal(0,1,(np.size(A,1),l))\n",
    "    for _ in range(maxiter):\n",
    "        W = A @ V\n",
    "        Z = A.T @ W\n",
    "        V, R = mmids.gramschmidt(Z)\n",
    "    W = A @ V\n",
    "    S = [LA.norm(W[:, i]) for i in range(np.size(W,1))]\n",
    "    U = np.stack([W[:,i]/S[i] for i in range(np.size(W,1))],axis=-1)\n",
    "    return U, S, V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f46dfae",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Note that above we avoided forming the matrix $A^T A$. With a small number of iterations, that approach potentially requires fewer arithmetic operations overall and it allows to take advantage of the possible sparsity of $A$ (i.e. the fact that it may have many zeros)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b01adff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We apply it again to our two-cluster example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b57e6f1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "d, n, w = 1000, 100, 3.\n",
    "X1, X2 = mmids.two_clusters(d, n, w)\n",
    "X = np.concatenate((X1, X2), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc236483",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Let's try again, but after projecting on the top two singular vectors. Recall that this corresponds to finding the best two-dimensional approximating subspace. The projection can be computed using the truncated SVD $Z= U_{(2)} \\Sigma_{(2)} V_{(2)}^T$. We can interpret the rows of $U_{(2)} \\Sigma_{(2)}$ as the coefficients of each data point in the basis $\\mathbf{v}_1,\\mathbf{v}_2$. We will work in that basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049e9292",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "U, S, V = svd(X, 2)\n",
    "Xproj = np.stack((U[:,0]*S[0], U[:,1]*S[1]), axis=-1)\n",
    "assign = mmids.kmeans(Xproj, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d0900d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111,aspect='equal')\n",
    "ax.scatter(X[:,0], X[:,1], c=assign)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67250af2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Finally, looking at the first two right singular vectors, we see that the first one does align quite well with the first dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2645f1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(np.stack((V[:,0], V[:,1]), axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842c455a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1fab65",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We load the dataset again and examine its first rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc7b3e7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('h3n2-snp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64e42ac",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ddf021",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Recall that it contains $1642$ strains and lives in a $318$-dimensional space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b66c633",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd599e3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Our goal is to find a \"good\" low-dimensional representation of the data. Two dimensions will do here. We use the SVD. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f3a382",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Specifically, we extract a data matrix, run our SVD algorithm with $k=2$, and plot the data in the projected subspace of the first two singular vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a431a33b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A = df[[df.columns[i] for i in range(1,len(df.columns))]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9654c7fc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "U, S, V = svd(A, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb627d1e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hid-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.scatter(U[:,0]*S[0], U[:,1]*S[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6fe3ec",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "There seems to be some reasonably well-defined clusters in this projection. To further reveal the structure, we color the data points by year. That information is in a separate file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd5213a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dfoth = pd.read_csv('h3n2-other.csv')\n",
    "dfoth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a818b6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "year = dfoth['year'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9593ec21",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We color the points on the scatterplot by year. (We use [`legend_elements()`](https://matplotlib.org/stable/api/collections_api.html#matplotlib.collections.PathCollection.legend_elements) for automatic legend creation.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f0645a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "scatter = plt.scatter(U[:,0]*S[0], U[:,1]*S[1], c=year, label=year)\n",
    "plt.legend(*scatter.legend_elements())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7eed36",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "To some extent, one can \"see\" the virus evolving from year to year. The $y$-axis in particular seems to correlate strongly with the year. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2025c1e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "## Further applications of the SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb43dc38",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** In Numpy, the Frobenius norm of a matrix can be computed using the default of the function `numpy.linalg.norm` while the induced norm can be computed using the same function with [`ord` parameter set to `2`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add58978",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "A = np.array([[1., 0.],[0., 1.],[0., 0.]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55fab63",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "LA.norm(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94d7ac7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "LA.norm(A, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dde0ab4",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b276aadd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** In Numpy, the pseudoinverse of a matrix can be computed using the function [`numpy.linalg.pinv`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.pinv.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fea678a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "M = np.array([[1.5, 1.3], [1.2, 1.9], [2.1, 0.8]])\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec5a9e7",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "Mp = LA.pinv(M)\n",
    "print(Mp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54be12b8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "Mp @ M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6d5a8e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Let's try our previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb92262",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A = np.array([[1., 0.], [-1., 0.]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4458c05",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Ap = LA.pinv(A)\n",
    "print(Ap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0017d05",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfd9b83",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** In Numpy, the condition number of a matrix can be computed using the function [`numpy.linalg.cond`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.cond.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b426e5b6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "For example, orthogonal matrices have condition number $1$, the lowest possible value for it (Why?). That indicates that orthogonal matrices have good numerical properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d4e4da",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "q = 1/np.sqrt(2)\n",
    "Q = np.array([[q, q], [q, -q]])\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e4eaf9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "LA.cond(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e85ab4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "In contrast, matrices with nearly linearly dependent columns have large condition numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac28086",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "eps = 1e-6\n",
    "A = np.array([[q, q], [q, q+eps]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a32248",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "LA.cond(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e835db52",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "u, s, vh = LA.svd(A)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b60bdcc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We compute the solution to $A \\mathbf{x} = \\mathbf{b}$ when $\\mathbf{b}$ is the left singular vector of $A$ corresponding to the largest singular value. Recall that in the proof of the *Conditioning of Matrix-Vector Multiplication Theorem*, we showed that the worst case bound is achieved when $\\mathbf{z} = \\mathbf{b}$ is right singular vector of $M= A^{-1}$ corresponding to the lowest singular value. In a previous example, given a matrix $A = \\sum_{j=1}^n \\sigma_j \\mathbf{u}_j \\mathbf{v}_j^T$ in compact SVD form, we derived a compact SVD for the inverse as\n",
    "\n",
    "$$\n",
    "A^{-1} = \\sigma_n^{-1} \\mathbf{v}_n \\mathbf{u}_n^T + \\sigma_{n-1}^{-1} \\mathbf{v}_{n-1} \\mathbf{u}_{n-1}^T + \\cdots + \\sigma_1^{-1} \\mathbf{v}_1 \\mathbf{u}_1^T.\n",
    "$$\n",
    "\n",
    "Here, compared to the SVD of $A$, the order of the singular values is reversed and the roles of the left and right singular vectors are exchanged. So we take $\\mathbf{b}$ to be the top left singular vector of $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9985122a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "b = u[:,0]\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eed2861",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = LA.solve(A,b)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf83381c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We make a small perturbation in the direction of the second right singular vector. Recall that in the proof of the *Conditioning of Matrix-Vector Multiplication Theorem*, we showed that the worst case is achieved when $\\mathbf{d} = \\delta\\mathbf{b}$ is top right singular vector of $M = A^{-1}$. By the argument above, that is the left singular vector of $A$ corresponding to the lowest singular value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f288a9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "delta = 1e-6\n",
    "bp = b + delta*u[:,1]\n",
    "print(bp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a216d1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The relative change in solution is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323050ff",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "xp = LA.solve(A,bp)\n",
    "print(xp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3e2daf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "(LA.norm(x-xp)/LA.norm(x))/(LA.norm(b-bp)/LA.norm(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045d52e8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "Note that this is exactly the condition number of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027fc89c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468a2a41",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** We give a quick example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c46e9eb",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "A = np.array([[1., 101.],[1., 102.],[1., 103.],[1., 104.],[1., 105]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5f4d1b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "LA.cond(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcd18a4",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "LA.cond(A.T @ A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df0b2bb",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "This observation -- and the resulting increased numerical instability -- is one of the reasons we previously developed an alternative approach to the least-squares problem. Quoting [Sol, Section 5.1]:\n",
    "\n",
    "> Intuitively, a primary reason that $\\mathrm{cond}(A^T A)$ can be large is that columns of $A$ might\n",
    "look “similar” [...] If two columns $\\mathbf{a}_i$ and $\\mathbf{a}_j$ satisfy $\\mathbf{a}_i \\approx \\mathbf{a}_j$, then the least-squares residual length $\\|\\mathbf{b} − A \\mathbf{x}\\|_2$ will not suffer much if we replace multiples of $\\mathbf{a}_i$ with multiples of $\\mathbf{a}_j$ or vice versa. This wide range of nearly—but not completely—equivalent solutions yields poor conditioning. [...] To solve such poorly conditioned problems, we will employ an alternative technique with closer attention to the column space of $A$ rather than employing row operations as in Gaussian elimination. This strategy identifies and deals with such near-dependencies explicitly, bringing about greater numerical stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee5045c",
   "metadata": {
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8331fea",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "**NUMERICAL CORNER:** Here is a numerical example taken from [[TB](https://books.google.com/books/about/Numerical_Linear_Algebra.html?id=JaPtxOytY7kC), Lecture 19]. We will approximate the following function with a polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f03244",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "n = 100 \n",
    "t = np.arange(n)/(n-1)\n",
    "b = np.exp(np.sin(4 * t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71de798e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.plot(t, b)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be26aa5",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We use a [Vandermonde matrix](https://en.wikipedia.org/wiki/Vandermonde_matrix), which can be constructed using [`numpy.vander`](https://numpy.org/doc/stable/reference/generated/numpy.vander.html), to perform polynomial regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17af610a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "m = 17\n",
    "A = np.vander(t, m, increasing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dffca3",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "The condition numbers of $A$ and $A^T A$ are both high in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ac6ef5",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print(LA.cond(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad8807f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print(LA.cond(A.T @ A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1854d30f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We first use the normal equations and plot the residual vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889e36cf",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "xNE = LA.solve(A.T @ A, A.T @ b)\n",
    "print(LA.norm(b - A@xNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8878b5a6",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.plot(t, b - A@xNE)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71587111",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "We then use `numpy.linalg.qr` to compute the QR solution instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0201d838",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "Q, R = LA.qr(A)\n",
    "xQR = mmids.backsubs(R, Q.T @ b)\n",
    "print(LA.norm(b - A@xQR)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c03edf5",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.plot(t, b - A@xNE)\n",
    "plt.plot(t, b - A@xQR)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc54ddad",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "colab-keep"
    ]
   },
   "source": [
    "$\\unlhd$"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
