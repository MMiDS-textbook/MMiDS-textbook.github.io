

<!DOCTYPE html>


<html data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>1.4. Some observations about high-dimensional data &#8212; MMiDS Online Book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chap01_intro/04_highdim/roch-mmids-intro-4highdim';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="1.5. Advanced material" href="../05_adv/roch-mmids-intro-5adv.html" />
    <link rel="prev" title="1.3. Clustering: an objective, an algorithm and a guarantee" href="../03_clustering/roch-mmids-intro-3clustering.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="None"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/mmids-cover.png" class="logo__image only-light" alt="MMiDS Online Book - Home"/>
    <script>document.write(`<img src="../../_static/mmids-cover.png" class="logo__image only-dark" alt="MMiDS Online Book - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">MMiDS</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../00_intro/roch-mmids-intro-0intro.html">1. Introduction</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../01_motiv/roch-mmids-intro-1motiv.html">1.1. Motivating example: species delimitation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02_review/roch-mmids-intro-2review.html">1.2. Background: review of basic linear algebra, calculus, and probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03_clustering/roch-mmids-intro-3clustering.html">1.3. Clustering: an objective, an algorithm and a guarantee</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">1.4. Some observations about high-dimensional data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../05_adv/roch-mmids-intro-5adv.html">1.5. Advanced material</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../chap02_ls/00_intro/roch-mmids-ls-intro.html">2. Least squares</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/01_motiv/roch-mmids-ls-1motiv.html">2.1. Motivating example: predicting sales</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/02_subspaces/roch-mmids-ls-2spaces.html">2.2. Background: linear spaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/03_orthog/roch-mmids-ls-3orthog.html">2.3. A key concept: orthogonality</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/04_overdetermined/roch-mmids-ls-4overdetermined.html">2.4. Overdetermined linear systems and regression analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/05_qr/roch-mmids-ls-5qr.html">2.5. QR decomposition and Householder transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chap02_ls/06_adv/roch-mmids-ls-6adv.html">2.6. Advanced material</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Some observations about high-dimensional data</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clustering-in-high-dimension">1.4.1. Clustering in high dimension</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#surprising-phenomena-in-high-dimension">1.4.2. Surprising phenomena in high dimension</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section id="some-observations-about-high-dimensional-data">
<h1><span class="section-number">1.4. </span>Some observations about high-dimensional data<a class="headerlink" href="#some-observations-about-high-dimensional-data" title="Permalink to this headline">#</a></h1>
<p>In this section, we first apply <span class="math notranslate nohighlight">\(k\)</span>-means clustering to a high-dimensional example to illustrate the issues that arise in that context. We then discuss some surprising phenomena in high dimensions.</p>
<section id="clustering-in-high-dimension">
<h2><span class="section-number">1.4.1. </span>Clustering in high dimension<a class="headerlink" href="#clustering-in-high-dimension" title="Permalink to this headline">#</a></h2>
<p>In this section, we test our implementation of <span class="math notranslate nohighlight">\(k\)</span>-means on a simple simulated dataset in high dimension.</p>
<p>The following function generates <span class="math notranslate nohighlight">\(n\)</span> data points from two spherical <span class="math notranslate nohighlight">\(d\)</span>-dimensional Gaussians with variance <span class="math notranslate nohighlight">\(1\)</span>, one with mean <span class="math notranslate nohighlight">\(-w\mathbf{e}_1\)</span> and one with mean <span class="math notranslate nohighlight">\(w \mathbf{e}_1\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">one_cluster</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span>
        <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(([</span><span class="n">w</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">d</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span> <span class="o">+</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">d</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">X</span>

<span class="k">def</span> <span class="nf">two_clusters</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="n">X1</span> <span class="o">=</span> <span class="n">one_cluster</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="o">-</span><span class="n">w</span><span class="p">)</span>
    <span class="n">X2</span> <span class="o">=</span> <span class="n">one_cluster</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">X1</span><span class="p">,</span> <span class="n">X2</span>
</pre></div>
</div>
</div>
</div>
<p>We will mix these two datasets to form an interesting case for clustering.</p>
<p><strong>Two dimensions</strong> We start with <span class="math notranslate nohighlight">\(d=2\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mf">3.</span>
<span class="n">X1</span><span class="p">,</span> <span class="n">X2</span> <span class="o">=</span> <span class="n">two_clusters</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We use a scatterplot to vizualize the data. Each dot corresponds to one data point. Observe the two clearly delineated clusters.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span><span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/f368c7651171a09109739e2b800b86fb4b39119d37c4f15429c284bd6f82fe9e.png" src="../../_images/f368c7651171a09109739e2b800b86fb4b39119d37c4f15429c284bd6f82fe9e.png" />
</div>
</div>
<p>Let’s run <span class="math notranslate nohighlight">\(k\)</span>-means on this dataset using <span class="math notranslate nohighlight">\(k=2\)</span>. We use <code class="docutils literal notranslate"><span class="pre">kmeans()</span></code> from the <code class="docutils literal notranslate"><span class="pre">mmids.py</span></code> file.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">assign</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">kmeans</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2011.9444469919526
465.1926461875746
411.86063821540495
411.86063821540495
411.86063821540495
411.86063821540495
411.86063821540495
411.86063821540495
411.86063821540495
411.86063821540495
</pre></div>
</div>
</div>
</div>
<p>Our default of <span class="math notranslate nohighlight">\(10\)</span> iterations seem to have been enough for the algorithm to converge. We can visualize the result by <a class="reference external" href="https://matplotlib.org/stable/api/_as_gen/matplotlib.lines.Line2D.html">coloring</a> the points according to the assignment.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span><span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">assign</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/62061615634bc8fa98088999880acea8a86b3193ad8f40bb17aeeec0e2fadad8.png" src="../../_images/62061615634bc8fa98088999880acea8a86b3193ad8f40bb17aeeec0e2fadad8.png" />
</div>
</div>
<p><strong>General dimension</strong> Let’s see what happens in higher dimension. We repeat our experiment with <span class="math notranslate nohighlight">\(d=1000\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mf">3.</span>
<span class="n">X1</span><span class="p">,</span> <span class="n">X2</span> <span class="o">=</span> <span class="n">two_clusters</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Again, we observe two clearly delineated clusters.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span><span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/abae9142ea496c0260824af51a2f04943ec540a1d6b01bc5345a6d6655b91168.png" src="../../_images/abae9142ea496c0260824af51a2f04943ec540a1d6b01bc5345a6d6655b91168.png" />
</div>
</div>
<p>This dataset is in <span class="math notranslate nohighlight">\(1000\)</span> dimensions, but we’ve plotted the data in only the first two dimensions. If we plot in any two dimensions not including the first one instead, we see only one cluster.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span><span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">2</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/3f05bcfcabc5a08049fb4e3a51df039c83463dab22c0dcc153596de267a52e4b.png" src="../../_images/3f05bcfcabc5a08049fb4e3a51df039c83463dab22c0dcc153596de267a52e4b.png" />
</div>
</div>
<p>Let’s see how <span class="math notranslate nohighlight">\(k\)</span>-means fares on this dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">assign</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">kmeans</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>200632.90971929248
200587.26732022577
200587.26732022577
200587.26732022577
200587.26732022577
200587.26732022577
200587.26732022577
200587.26732022577
200587.26732022577
200587.26732022577
</pre></div>
</div>
</div>
</div>
<p>Our attempt at clustering does not appear to have been successful.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span><span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">assign</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/4830598e4516341c536e2be654555a3ce184e153e5333074531bfb59b20de863.png" src="../../_images/4830598e4516341c536e2be654555a3ce184e153e5333074531bfb59b20de863.png" />
</div>
</div>
<p>Our attempt at clustering does not appear to have been successful. What happened? While these clusters are easy to tease apart <em>if we know to look at the first coordinate only</em>, in the full space the within-cluster and between-cluster distances become harder to distinguish: the noise overwhelms the signal.</p>
<p>The function below plots the histograms of within-cluster and between-cluster distances for a sample of size <span class="math notranslate nohighlight">\(n\)</span> in <span class="math notranslate nohighlight">\(d\)</span> dimensions with a given offset. As <span class="math notranslate nohighlight">\(d\)</span> increases, the two distributions become increasingly indistinguishable. Later in the course, we will develop dimension-reduction techniques that help deal with this issue.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">highdim_2clusters</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="c1"># generate datasets</span>
    <span class="n">X1</span><span class="p">,</span> <span class="n">X2</span> <span class="o">=</span> <span class="n">two_clusters</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
    
    <span class="c1"># within-cluster distances for X1</span>
    <span class="n">intra</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span>
        <span class="p">[</span><span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">X1</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span> <span class="o">-</span> <span class="n">X1</span><span class="p">[</span><span class="n">j</span><span class="p">,:])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="k">if</span> <span class="n">j</span><span class="o">&gt;</span><span class="n">i</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">intra</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;within-cluster&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;dim=</span><span class="si">{</span><span class="n">d</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
 
    <span class="c1"># between-cluster distances</span>
    <span class="n">inter</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span>
        <span class="p">[</span><span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">X1</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span> <span class="o">-</span> <span class="n">X2</span><span class="p">[</span><span class="n">j</span><span class="p">,:])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span>
    <span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">inter</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;between-cluster&#39;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;dim=</span><span class="si">{</span><span class="n">d</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Next we plot the results for dimensions <span class="math notranslate nohighlight">\(d=2, 100, 1000\)</span>. What do you observe?</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">highdim_2clusters</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/fbe7f14054db9458b8734e65a5219ef53d8c504c60aa1d73f317c9f65d422b50.png" src="../../_images/fbe7f14054db9458b8734e65a5219ef53d8c504c60aa1d73f317c9f65d422b50.png" />
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">highdim_2clusters</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/1abe96fdffd4c9e839dfc7bff0c7331651e6b307a6bbd442bec9b26e60134595.png" src="../../_images/1abe96fdffd4c9e839dfc7bff0c7331651e6b307a6bbd442bec9b26e60134595.png" />
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">highdim_2clusters</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/6aeb79fa8a831fa4a00d2d46fb7e0df1a19797788f15d6a5b7dae3ecf198d38b.png" src="../../_images/6aeb79fa8a831fa4a00d2d46fb7e0df1a19797788f15d6a5b7dae3ecf198d38b.png" />
</div>
</div>
<p>As the dimension increases, the distributions of intra-cluster and inter-cluster distances overlap significantly and become more or less indistinguishable. That provides some insights into why clustering may fail here. Note that we used the same offset for all simulations. On the other hand, if the separation between the clusters is sufficiently large, one would expect clustering to work even in high dimension.</p>
<p><strong>TRY IT!</strong> What precedes (and what follows in the next subsection) is not a formal proof that <span class="math notranslate nohighlight">\(k\)</span>-means clustering will be unsuccessful here. The behavior of the algorithm is quite complex and depends, in particular, on the initialization and the density of points. Here, increasing the number of data points eventually leads to a much better performance. Explore this behavior on your own by modifying the code. (For some theoretical justifications (beyond this course), see <a class="reference external" href="https://arxiv.org/pdf/0912.0086.pdf">here</a> and <a class="reference external" href="http://www.stat.yale.edu/~pollard/Papers/Pollard81AS.pdf">here</a>.)</p>
</section>
<section id="surprising-phenomena-in-high-dimension">
<h2><span class="section-number">1.4.2. </span>Surprising phenomena in high dimension<a class="headerlink" href="#surprising-phenomena-in-high-dimension" title="Permalink to this headline">#</a></h2>
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">a high-dimensional space is a lonely place</p>&mdash; Bernhard Schölkopf (@bschoelkopf) <a href="https://twitter.com/bschoelkopf/status/503554842829549568?ref_src=twsrc%5Etfw">August 24, 2014</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> <p>In the previous section, we saw how the contribution from a large number of “noisy dimensions” can overwhelm the “signal” in the context of clustering. In this section we discuss further properties of high-dimensional space that are relevant to data science problems.</p>
<p>Applying <em>Chebyshev’s inequality</em> to sums of independent random variables has useful statistical implications: it shows that, with a large enough number of samples <span class="math notranslate nohighlight">\(n\)</span>, the sample mean is close to the population mean. Hence it allows us to infer properties of a population from samples. Interestingly, one can apply a similar argument to a different asymptotic regime: the limit of large dimension <span class="math notranslate nohighlight">\(d\)</span>. But as we will see in this section, the statistical implications are quite different.</p>
<p><strong>High-dimensional cube</strong> To start explaining the quote above, we consider a simple experiment. Let <span class="math notranslate nohighlight">\(\mathcal{C} = [-1/2,1/2]^d\)</span> be the <span class="math notranslate nohighlight">\(d\)</span>-cube with side lengths <span class="math notranslate nohighlight">\(1\)</span> centered at the origin and let <span class="math notranslate nohighlight">\(\mathcal{B} = \{\mathbf{x} \in \mathbb{R}^d : \|\mathbf{x}\|\leq 1/2\}\)</span> be the inscribed <span class="math notranslate nohighlight">\(d\)</span>-ball.</p>
<p>Now pick a point <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> uniformly at random in <span class="math notranslate nohighlight">\(\mathcal{C}\)</span>. What is the probability that it falls in <span class="math notranslate nohighlight">\(\mathcal{B}\)</span>?</p>
<p>To generate <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>, we pick <span class="math notranslate nohighlight">\(d\)</span> independent random variables <span class="math notranslate nohighlight">\(X_1, \ldots, X_d \sim \mathrm{U}[-1/2, 1/2]\)</span>, and form the vector <span class="math notranslate nohighlight">\(\mathbf{X} = (X_1, \ldots, X_d)\)</span>. Indeed, the PDF of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is then <span class="math notranslate nohighlight">\(f_{\mathbf{X}}(\mathbf{x})= 1^d = 1\)</span> if <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathcal{C}\)</span> and <span class="math notranslate nohighlight">\(0\)</span> otherwise.</p>
<p>The event we are interested in is <span class="math notranslate nohighlight">\(A = \left\{\|\mathbf{X}\| \leq 1/2\right\}\)</span>. The uniform distribution over the set <span class="math notranslate nohighlight">\(\mathcal{C}\)</span> has the property that <span class="math notranslate nohighlight">\(\mathbb{P}[A]\)</span> is the volume of <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> divided by the volume of <span class="math notranslate nohighlight">\(\mathcal{C}\)</span>. In this case, the volume of <span class="math notranslate nohighlight">\(\mathcal{C}\)</span> is <span class="math notranslate nohighlight">\(1^d = 1\)</span> and the volume of <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> has an <a class="reference external" href="https://en.wikipedia.org/wiki/Volume_of_an_n-ball">explicit formula</a>.</p>
<p>This leads to the following surprising fact:</p>
<p><strong>THEOREM</strong> <strong>(High-dimensional Cube)</strong> Let
<span class="math notranslate nohighlight">\(\mathcal{B} = \{\mathbf{x} \in \mathbb{R}^d \,:\, \|\mathbf{x}\|\leq 1/2\}\)</span> and
<span class="math notranslate nohighlight">\(\mathcal{C} = [-1/2,1/2]^d\)</span>. Pick <span class="math notranslate nohighlight">\(\mathbf{X} \sim \mathrm{U}[\mathcal{C}]\)</span>. Then, as <span class="math notranslate nohighlight">\(d \to +\infty\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}[\mathbf{X} \in \mathcal{B}]
\to 0.
\]</div>
<p><span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p>In words, in high dimension if one picks a point at random from the cube, it is unlikely to be close to the origin. Instead it is likely to be in the corners. A geometric interpretation is that a high-dimensional cube is a bit like a “spiky ball.”</p>
<p><img alt="Vizualization of a high-dimensional cube" src="../../_images/ball_with_spikes.png" /></p>
<p><strong>Figure:</strong> Vizualization of a high-dimensional cube as a spiky ball (<em>Credit:</em> Made with <a class="reference external" href="https://www.midjourney.com/">Midjourney</a>)</p>
<p>We give a proof based on <em>Chebyshev’s inequality</em>. It has the advantage of providing some insight into this counter-intuitive phenomenon by linking it to the concentration of sums of independent random variables, in this case the squared norm of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>.</p>
<p><em>Proof idea:</em> We think of <span class="math notranslate nohighlight">\(\|\mathbf{X}\|^2\)</span> as a sum of independent random variables and apply <em>Chebyshev’s inequality</em>. It implies that the norm of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is concentrated around its mean, which grows like <span class="math notranslate nohighlight">\(\sqrt{d}\)</span>. The latter is larger than <span class="math notranslate nohighlight">\(1/2\)</span> for <span class="math notranslate nohighlight">\(d\)</span> large.</p>
<p><em>Proof:</em> To see the relevance of <em>Chebyshev’s inequality</em>, we compute the mean and standard deviation of the norm of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>. In fact, because of the square root in <span class="math notranslate nohighlight">\(\|\mathbf{X}\|\)</span>, computing its expectation is difficult. Instead we work with the squared norm</p>
<div class="math notranslate nohighlight">
\[
\|\mathbf{X}\|^2 = X_1^2 + X_2^2 + \cdots + X_d^2,
\]</div>
<p>which has the advantage of being a sum of independent random variables - for which the expectation is much easier to compute. Observe further that the probability of the event of interest <span class="math notranslate nohighlight">\(\{\|\mathbf{X}\| \leq 1/2\}\)</span> can be re-written in terms of <span class="math notranslate nohighlight">\(\|\mathbf{X}\|^2\)</span> as follows</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb{P}
\left[
\|\mathbf{X}\| \leq 1/2
\right]
&amp;= 
\mathbb{P}
\left[
\|\mathbf{X}\|^2 \leq 1/4
\right].
\end{align*}\]</div>
<p>To simplify the notation, we use <span class="math notranslate nohighlight">\(\tilde\mu = \mathbb{E}[X_1^2]\)</span> and <span class="math notranslate nohighlight">\(\tilde\sigma = \sqrt{\mathrm{Var}[X_1^2]}\)</span> for the mean and standard deviation of <span class="math notranslate nohighlight">\(X_1^2\)</span> respectively. Using linearity of expectation and the fact that the <span class="math notranslate nohighlight">\(X_i\)</span>’s are independent, we get</p>
<div class="math notranslate nohighlight">
\[
\mu_{\|\mathbf{X}\|^2}
= \mathbb{E}\left[
\|\mathbf{X}\|^2
\right]
= \sum_{i=1}^d \mathbb{E}[X_i^2]
= d \,\mathbb{E}[X_1^2]
= \tilde\mu \, d,
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\mathrm{Var}\left[
\|\mathbf{X}\|^2
\right]
= \sum_{i=1}^d \mathrm{Var}[X_i^2]
= d \,\mathrm{Var}[X_1^2].
\]</div>
<p>Taking a square root, we get an expression for the standard deviation of our quantity of interest <span class="math notranslate nohighlight">\(\|\mathbf{X}\|^2\)</span> in terms of the standard deviation of <span class="math notranslate nohighlight">\(X_1^2\)</span></p>
<div class="math notranslate nohighlight">
\[
\sigma_{\|\mathbf{X}\|^2}
= \tilde\sigma \, \sqrt{d}.
\]</div>
<p>(Note that we could compute <span class="math notranslate nohighlight">\(\tilde\mu\)</span> and <span class="math notranslate nohighlight">\(\tilde\sigma\)</span> explicitly, but it will not be necessary here.)</p>
<p>We use <em>Chebyshev’s inequality</em> to show that <span class="math notranslate nohighlight">\(\|\mathbf{X}\|^2\)</span> is highly likely to be close to its mean <span class="math notranslate nohighlight">\(\tilde\mu \, d\)</span>, which is much larger than <span class="math notranslate nohighlight">\(1/4\)</span> when <span class="math notranslate nohighlight">\(d\)</span> is large. And that therefore <span class="math notranslate nohighlight">\(\|\mathbf{X}\|^2\)</span> is highly unlikely to be smaller than <span class="math notranslate nohighlight">\(1/4\)</span>. We give the details next.</p>
<p>By the one-sided version of <em>Chebyshev’s inequality</em> in terms of the standard deviation, we have</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}\left[
\|\mathbf{X}\|^2 - \mu_{\|\mathbf{X}\|^2} \leq - \alpha
\right]
\leq 
\left(\frac{\sigma_{\|\mathbf{X}\|^2}}{\alpha}\right)^2.
\]</div>
<p>That is, using the formulas above and rearranging slightly,</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}\left[
\|\mathbf{X}\|^2 \leq \tilde\mu \, d - \alpha
\right]
\leq 
\left(\frac{\tilde\sigma \, \sqrt{d}}{\alpha}\right)^2.
\]</div>
<p>How do we relate this to the probability of interest
<span class="math notranslate nohighlight">\(\mathbb{P}\left[\|\mathbf{X}\|^2 \leq 1/4\right]\)</span>? Recall that we are free to choose <span class="math notranslate nohighlight">\(\alpha\)</span> in this inequality. So simply take <span class="math notranslate nohighlight">\(\alpha\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
\tilde\mu \,d - \alpha = \frac{1}{4},
\]</div>
<p>that is, <span class="math notranslate nohighlight">\(\alpha = \tilde\mu \,d - 1/4\)</span>. Observe that, once <span class="math notranslate nohighlight">\(d\)</span> is large enough, it holds that <span class="math notranslate nohighlight">\(\alpha &gt; 0\)</span>.</p>
<p>Finally, replacing this choice of <span class="math notranslate nohighlight">\(\alpha\)</span> in the inequality above gives</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb{P}
\left[
\|\mathbf{X}\| \leq 1/2
\right]
&amp;= \mathbb{P}\left[\|\mathbf{X}\|^2 \leq 1/4\right]\\
&amp;=
\mathbb{P}\left[
\|\mathbf{X}\|^2 \leq \tilde\mu \, d - \alpha
\right]\\
&amp;\leq 
\left(\frac{\tilde\sigma \, \sqrt{d}}{\alpha}\right)^2\\
&amp;\leq 
\left(\frac{\tilde\sigma \, \sqrt{d}}{\tilde\mu \,d - 1/4}\right)^2.
\end{align*}\]</div>
<p>Critically, <span class="math notranslate nohighlight">\(\tilde\mu\)</span> and <span class="math notranslate nohighlight">\(\tilde\sigma\)</span> do not depend on <span class="math notranslate nohighlight">\(d\)</span>. So the right-hand side goes to <span class="math notranslate nohighlight">\(0\)</span> as <span class="math notranslate nohighlight">\(d \to +\infty\)</span>. Indeed, <span class="math notranslate nohighlight">\(d\)</span> is much larger than <span class="math notranslate nohighlight">\(\sqrt{d}\)</span> when <span class="math notranslate nohighlight">\(d\)</span> is large. That proves the claim.<span class="math notranslate nohighlight">\(\square\)</span></p>
<p>We will see later in the course that this high-dimensional phenomenon has implications for data science problems. It is behind what is referred to as the <a class="reference external" href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">Curse of Dimensionality</a>.</p>
<p>While <em>Chebyshev’s inequality</em> correctly implies that <span class="math notranslate nohighlight">\(\mathbb{P}[\mathbf{X} \in \mathcal{B}]\)</span> goes to <span class="math notranslate nohighlight">\(0\)</span>, it does not give the correct rate of convergence. In reality, that probability goes to <span class="math notranslate nohighlight">\(0\)</span> at a much faster rate than <span class="math notranslate nohighlight">\(1/d\)</span>. Specifically, <a class="reference external" href="https://en.wikipedia.org/wiki/Volume_of_an_n-ball#High_dimensions">it can be shown</a> that <span class="math notranslate nohighlight">\(\mathbb{P}[\mathbf{X} \in \mathcal{B}]\)</span> goes to <span class="math notranslate nohighlight">\(0\)</span> roughly as <span class="math notranslate nohighlight">\(d^{-d/2}\)</span>. We will not need or derive this fact here.</p>
<p><strong>NUMERICAL CORNER:</strong> We can check the theorem in a simulation. Here we pick <span class="math notranslate nohighlight">\(n\)</span> points uniformly at random in the <span class="math notranslate nohighlight">\(d\)</span>-cube <span class="math notranslate nohighlight">\(\mathcal{C}\)</span>, for a range of dimensions up to <code class="docutils literal notranslate"><span class="pre">dmax</span></code>. We then plot the frequency of landing in the inscribed <span class="math notranslate nohighlight">\(d\)</span>-ball <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> and see that it rapidly converges to <span class="math notranslate nohighlight">\(0\)</span>. Alternatively, we could just plot the formula for the volume of <span class="math notranslate nohighlight">\(\mathcal{B}\)</span>. But knowing how to do simulations is useful in situations where explicit formulas are unavailable or intractable.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">highdim_cube</span><span class="p">(</span><span class="n">dmax</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    
    <span class="n">in_ball</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">dmax</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dmax</span><span class="p">):</span>
        <span class="c1"># recall that d starts at 0 so we add 1 below</span>
        <span class="n">in_ball</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
            <span class="p">[(</span><span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">d</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span>
        <span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">dmax</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">in_ball</span><span class="p">)</span> 
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;dim&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;in-ball freq&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>We plot the result up to dimension <span class="math notranslate nohighlight">\(10\)</span>.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">highdim_cube</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/4c6033e4a2444d842530db5ad904ecc115617a67b26396deeba55907bda8cef9.png" src="../../_images/4c6033e4a2444d842530db5ad904ecc115617a67b26396deeba55907bda8cef9.png" />
</div>
</div>
<p><strong>Gaussians in high dimension</strong> In this section, we turn our attention to the <a class="reference external" href="https://en.wikipedia.org/wiki/Normal_distribution">Gaussian (or Normal) distribution</a> and its behavior in high dimension.
Using <em>Chebyshev’s inequality</em>, we show that a standard Normal vector has the following counter-intuititve property in high dimension: a typical draw has <span class="math notranslate nohighlight">\(2\)</span>-norm that is highly likely to be around <span class="math notranslate nohighlight">\(\sqrt{d}\)</span>. Visually, when <span class="math notranslate nohighlight">\(d\)</span> is large, the joint PDF looks something like this:</p>
<p><img alt="A spherical shell" src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/07/Kugelschale.svg/640px-Kugelschale.svg.png" /></p>
<p><strong>Figure:</strong> A spherical shell (<a class="reference external" href="https://commons.wikimedia.org/wiki/File:Kugelschale.svg">Source</a>)</p>
<!--TEX
![A spherical shell ([Source](https://commons.wikimedia.org/wiki/File:Kugelschale.svg))](./figs/2560px-Kugelschale.svg.png)
--><p>This is unexpected because the joint PDF is maximized at <span class="math notranslate nohighlight">\(\mathbf{x} = \mathbf{0}\)</span> for all <span class="math notranslate nohighlight">\(d\)</span> (including <span class="math notranslate nohighlight">\(d=1\)</span> as can be seen in the figure above). But the rough intuition is the following: (1) there is only “one way” to obtain <span class="math notranslate nohighlight">\(\|\mathbf{X}\|^2 = 0\)</span> – every coordinate must be <span class="math notranslate nohighlight">\(0\)</span> by the point-separating property of the <span class="math notranslate nohighlight">\(2\)</span>-norm; (2) on the other hand, there are “many ways” to obtain <span class="math notranslate nohighlight">\(\|\mathbf{X}\|^2 = \sqrt{d}\)</span> – and that compensates for the lower density.</p>
<p><strong>THEOREM</strong> <strong>(High-dimensional Gaussians)</strong> Let <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> be a standard Normal <span class="math notranslate nohighlight">\(d\)</span>-vector. Then, for any <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}
\left[\,
\|\mathbf{X}\| \notin (\sqrt{d(1-\varepsilon)}, \sqrt{d(1+\varepsilon)})
\,\right]
\to 0
\]</div>
<p>as <span class="math notranslate nohighlight">\(d \to +\infty\)</span>. <span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p><em>Proof idea:</em> We apply <em>Chebyshev’s inequality</em> to the squared norm, which is a sum of independent random variables.</p>
<p><em>Proof:</em> Let <span class="math notranslate nohighlight">\(Z = \|\mathbf{X}\|^2 = \sum_{i=1}^d X_i^2\)</span> and notice that, by definition, it is a sum of independent random variables. Appealing to the expectation and variance formulas from the previous sections:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[\|\mathbf{X}\|^2] = d \,\mathbb{E}[X_1^2] = d \,\mathrm{Var}[X_1] = d
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\mathrm{Var}[\|\mathbf{X}\|^2] = d \,\mathrm{Var}[X_1^2]
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathrm{Var}[X_1^2]\)</span> does not depend on <span class="math notranslate nohighlight">\(d\)</span>. By <em>Chebyshev’s inequality</em></p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}\left[
\|\mathbf{X}\|^2 \notin 
\left(
d(1-\varepsilon),d(1+\varepsilon)
\right)
\right]
=
\mathbb{P}[|\|\mathbf{X}\|^2 - d| \geq \varepsilon d]
\leq \frac{d \,\mathrm{Var}[X_1^2]}{\varepsilon^2 d^2}
= \frac{\mathrm{Var}[X_1^2]}{d \varepsilon^2}.
\]</div>
<p>Taking a square root inside the probability on the leftmost side and taking a limit as <span class="math notranslate nohighlight">\(d \to +\infty\)</span> on the rightmost side gives the claim.<span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>NUMERICAL CORNER:</strong> We check our claim in a simulation. We generate standard Normal <span class="math notranslate nohighlight">\(d\)</span>-vectors using the <code class="docutils literal notranslate"><span class="pre">rng.normal(0,1,d)</span></code> function and plot the histogram of their <span class="math notranslate nohighlight">\(2\)</span>-norm.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">normal_shell</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="n">one_sample_norm</span> <span class="o">=</span> <span class="p">[</span><span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">d</span><span class="p">))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">one_sample_norm</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">one_sample_norm</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>We first plot it in one dimensions.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">normal_shell</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10000</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/d0eb28f83069f2fc205a85ff84316ad2b35e4318e16e76fcc3772ffeff53a64d.png" src="../../_images/d0eb28f83069f2fc205a85ff84316ad2b35e4318e16e76fcc3772ffeff53a64d.png" />
</div>
</div>
<p>In higher dimension:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">normal_shell</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">10000</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/dc06c5772bdbffced52590fa60925b3a07f5e4f2a30295f87b9e71c7dfd9beeb.png" src="../../_images/dc06c5772bdbffced52590fa60925b3a07f5e4f2a30295f87b9e71c7dfd9beeb.png" />
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chap01_intro/04_highdim"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../03_clustering/roch-mmids-intro-3clustering.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">1.3. </span>Clustering: an objective, an algorithm and a guarantee</p>
      </div>
    </a>
    <a class="right-next"
       href="../05_adv/roch-mmids-intro-5adv.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">1.5. </span>Advanced material</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clustering-in-high-dimension">1.4.1. Clustering in high dimension</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#surprising-phenomena-in-high-dimension">1.4.2. Surprising phenomena in high dimension</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Sebastien Roch
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>